[{"categories":["k8s 学习"],"content":"1 概述 Kubernetes 中，组织对象的方式，就是按照 Group、Version、Resource 三个层级。 Group 用以来对 API 进行分组（分类）； Version 用以对相同的 API 进行版本控制； Resource 代表着一个具体的资源对象的 API； etcd 中如何组织对象 这不是在 etcd 中组织资源对象的方式，etcd 中还是按照资源类型，以及命名来进行分类存储 $ alias edctl=\"etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key $edctl get /registry --prefix --keys-only /registry/pingcap.com/tidbinitializers/mycluster/mycluster-init /registry/pingcap.com/tidbmonitors/mycluster/mycluster /registry/pods/kube-system/coredns-7c7788d75c-cggn5 … 这三个层次，也会体现在资源定义的 yaml 文件中： apiVersion:batch/v2alpha1 # Group/Versionkind:CronJob # Resource# … 而 Kubernetes 就是通过比较 Group Version Resource，再加上资源对象的 name 来寻找一个资源。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"2 版本控制 Kubernetes 对于版本的控制体现在 URL 中，每个 API 对应的 URL 基于 /apis/\u003cgroup\u003e/\u003cversion\u003e/namespaces/\u003cns\u003e/\u003cresource\u003e 的格式构建，所以不同的版本有着不同的 URL。 对于一个特定的版本，例如 v1，包含三种级别的版本： Alpha ：预览版本 version 以 \u003cv\u003ealpha\u003cnr\u003e 格式，例如 v1alpha1。 表明软件是不稳定，软件可能会有 Bug，某些新增特性支持可能随时被删除，某些新增 API 可能会出现不兼容性更改。 因此仅仅使用于测试集群，不适合生产环境。 Beta ：测试版本 version 以 \u003cv\u003ebeta\u003cnr\u003e 格式，例如 v2beta3。 表明软件经过测试，并且特性都是安全的。如果特性与 API 发生兼容性更改，那么会提供迁移的说明。 GA ：稳定版本 verison 以 \u003cv\u003e 格式，例如 v1。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"3 API Group 对于比较核心的 API 对象，其 URL 路径前缀为 /api/v1。其 Group 为空，因此使用 spec.apiVersion : v1。 对于其他的 API 对象，URL 路径前缀为 /apis/\u003cGroup\u003e/\u003cVersion\u003e，使用 spec.apiVersion: \u003cGroup\u003e/\u003cVersion\u003e。 Kubernetes 内置的所有 API Group 可以见 API 参考文档。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"3.1 启用或禁用 API Group 所有 API Group 默认情况都是可用的。可以通过 API Server 启动配置开启用或禁用某个 API 组。 例如 --runtime-config=batch/v1=false 表明禁用 batch/v1 下的所有 API。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"4 对象 Kubernetes API 所有资源类型都是以 对象 方式看待，并且以 REST 方式提供 HTTP API 接口。 每个对象的 API 以以下格式表示： non-namespace 资源： GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/\u003cResource\u003e ：返回资源集合 GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/\u003cResource\u003e/\u003cName ：操作指定 name 的资源对象 namespace 资源： GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/\u003cResource\u003e ：返回所有 namespace 下资源对象 GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/namespaces/\u003cNamespace\u003e/\u003cResource\u003e ：返回指定 namespace 下的所有资源对象 GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/namespaces/\u003cNamespace\u003e/\u003cResource\u003e/\u003cName\u003e ：返回 namespace 下指定 name 的资源对象 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"4.1 watch 对象 基于 etcd 的机制，每个 Kubernetes 对象都有一个 resourceVersion 字段，代表着资源在 etcd 中存储的版本。 因此，在 client 的 watch 连接断开，重新连接后，可以通过指定 resourceVersion 来得到断连期间其对象所有的更改。 GET /api/v1/namespaces/test/pods?watch=1\u0026resourceVersion=10245 --- 200 OK Transfer-Encoding: chunked Content-Type: application/json { \"type\": \"ADDED\", \"object\": {\"kind\": \"Pod\", \"apiVersion\": \"v1\", \"metadata\": {\"resourceVersion\": \"10596\", ...}, ...} } { \"type\": \"MODIFIED\", \"object\": {\"kind\": \"Pod\", \"apiVersion\": \"v1\", \"metadata\": {\"resourceVersion\": \"11020\", ...}, ...} } ... 当然，etcd 只会保存一定时间内的资源变更历史（默认 5min）。如果请求的历史版本不存在，那么会返回 HTTP Code 410 Gone。 所以 client 必须能够处理 410 错误，清理本地对象缓存，调用 list 操作，重新建立 watch。 Reflector 官方提供了封装好这些功能的代码组件，例如 Go 中 Reflector 组件已经实现了这些逻辑 为了解决历史窗口太短的问题，为 event 引入了 bookmark event 的概念。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:4:1","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"4.2 查询对象 一般情况下，查询对象只需要使用 HTTP GET URL 就可以对象集合或者指定对象。 不过有时候，当你查询大量的对象时，可能返回的数据很大，使得浪费网络资源。从 1.9 开始，Kubernetes 支持分段查询对象。 在进行查询请求时，可以指定 limit 和 continue 参数。 查询 500 个 Pod，指定 “limit=500”。 GET /api/v1/pods?limit=500 --- 200 OK Content-Type: application/json { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\":\"10245\", \"continue\": \"ENCODED_CONTINUE_TOKEN\", ... }, \"items\": [...] // returns pods 1-500 } 继续前面调用，查询剩余的 Pod，指定 “continue=ENCODED_CONTINUE_TOKEN” 表明继续上一次查询。 GET /api/v1/pods?limit=500\u0026continue=ENCODED_CONTINUE_TOKEN --- 200 OK Content-Type: application/json { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\":\"10245\", \"continue\": \"\", // continue token is empty because we have reached the end of the list ... }, \"items\": [...] // returns pods 1001-1253 } 注意，当你使用 continue 继续查询时，查询的永远是同一个 resourceVersion 的对象。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:4:2","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"4.3 对象表现形式 默认 Kubernetes 都是通过 JSON 格式来进行数据的传输。通过下面方式，允许 client 使用 protobuf 形式进行数据传输。 请求数据时，添加头部 Accept: application/vnd.kubernetes.protobuf 表明期望返回 protobuf 类型数据； 发送数据时，通过指定 Content-Type: application/vnd.kubernetes.protobuf 表明传输的是一个 protobuf 类型数据； 不过，部分 CRD 或者 API 扩展加入的资源不支持 Protobuf，可以使用 Accept 头部指定多种类型来允许回退。 Accept: application/vnd.kubernetes.protobuf, application/json ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:4:3","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"4.4 对象的删除 资源对象删除经过两个阶段： finalization 终止 资源的 metadata.deletionTimestamp 被设置，然后随机顺序执行 Finalizers。 delete 删除 当所有 Finalizers 执行结束后，资源才从 etcd 中被真正删除。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:4:4","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"5 访问控制 Kubernetes 对 API 有着三种访问控制方式： 认证Authentication 授权Authorization 准入控制Admission Control 这里仅仅会介绍三种方式的概念，更多的细节后面才深入去学习。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"5.1 Authentication Authentication 用于验证请求者是否是合法的，也就是检查身份。 每次收到请求时，APIServer 都会通过令排链进行认证，某一个认证成功即可： x509 处理程序 ：认证 HTTP 请求的证书是否有效； bearer token 处理程序 ：验证 token 是否有效； 基本认证处理程序 ：确保 HTTP 请求的基本认证凭证与本地的状态匹配； ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:5:1","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"5.2 Authorization Authorization 用于验证请求者是否由对应操作的权限，也就是检查权限。 APIServer 会组合一系列授权方法，只要有一个授权者批准了操作，那么请求就可以继续。 使用哪些授权方法，可以在 APIServer 启动时通过 --authorization_mode 参数设置 目前支持的几种授权方法： webhook ：允许集群外的 HTTP(s) 服务进行授权； ABAC ：执行静态文件中定义的策略； RBAC ：通过 RBAC 机制进行授权，这允许管理员进行动态的配置； Node ：确保 kubelet 只能访问自己节点上资源； ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:5:2","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"5.3 Admission Control Admission Control 是作为对授权模块的补充，用于拦截请求以确保其符合集群的期望与规则。 准入控制仅仅对于对象的创建、删除、更新、连接请求其作用，对于对象读取操作不起作用。 只有所有的准入控制器都通过，请求才能继续。如果任一准入控制器检查不通过，请求就会被立即拒绝。 准入控制是最后的关卡，一旦通过后，资源对象就会写入 etcd 中。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:5:3","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"参考 Blog：Kubernetes API 的版本控制，分组，对象，访问控制 **官方文档：Kubernetes API Concepts **Blog: what-happens-when-k8s ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/:6:0","tags":["k8s","云计算"],"title":"K8s 学习 - API 概念","uri":"/posts/cloud_computing/k8s_learning/api-%E6%A6%82%E5%BF%B5/"},{"categories":["k8s 学习"],"content":"1 概述 无论是基本的副本控制器，还是自定义资源，其控制的底层 Pod 的调度都是都通过 Scheduler 完成的。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"2 Schedule ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"2.1 nodeSelector Pod 的 spec.nodeSelector 可以用于控制 Pod 能被调度到哪些节点上。其内容是一组 kv 键值对，只有节点 label 包含所有设定的 kv，才可以被调度 Pod。 apiVersion:v1kind:Podmetadata:name:nginxlabels:env:testspec:containers:- name:nginximage:nginximagePullPolicy:IfNotPresentnodeSelector:disktype:ssd # 只有 label 包含 disktype:ssd 的节点才能被调度 除了你手动为节点添加 label 外，每个节点会默认添加上一些 label： kubernetes.io/hostname failure-domain.beta.kubernetes.io/zone failure-domain.beta.kubernetes.io/region topology.kubernetes.io/zone topology.kubernetes.io/region beta.kubernetes.io/instance-type node.kubernetes.io/instance-type kubernetes.io/os kubernetes.io/arch ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"2.2 nodeName spec.nodeName 是最简单的选择节点方法，指定 Pod 只能在一个指定节点上运行。 apiVersion:v1kind:Podmetadata:name:nginxspec:containers:- name:nginximage:nginxnodeName:kube-01 # 指定调度到节点 kube-01 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"2.3 affinity 2.3.1 nodeAffinity spec.affinity.nodeAffinity 与 nodeSelector 类似，可以根据节点的 label 来控制 Pod 调度到哪些节点。 目前包含两种类型的节点亲和性： requiredDuringSchedulingIgnoredDuringExecution ：指定调度到的节点必须满足的条件，与 nodeSelector 一样但是表达性更高； preferredDuringSchedulingIgnoredDuringExecution ：指定调度到节点的偏好条件，也就是优先调度到满足条件的节点； 只影响调度 目前两种类型节点亲和性都仅仅影响调度时的选择，而不会驱逐已经运行的 Pod。 apiVersion:v1kind:Podmetadata:name:with-node-affinityspec:affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 必须满足条件nodeSelectorTerms:- matchExpressions:- key:kubernetes.io/e2e-az-nameoperator:Invalues:- e2e-az1- e2e-az2preferredDuringSchedulingIgnoredDuringExecution:# 优先级条件- weight:1preference:matchExpressions:- key:another-node-label-keyoperator:Invalues:- another-node-label-valuecontainers:- name:with-node-affinityimage:k8s.gcr.io/pause:2.0 nodeSelectorTerms 下的数组之间是 “或” 关系，也就是满足其中一个条件就可以被调度。 matchExpressions 下的数组之间是 “与” 关系，需要满足所有条件才可以被调度。 weight 字段范围 1-100，如果满足其指定的条件，那么节点优选算分时就会加上 weight 的值。 2.3.2 Pod 亲和性与反亲和性 spec.affinity.podAffinity 亲和性允许根据节点上已经运行的 Pod 的 label 来控制是否调度到该节点。 Pod 亲和性也包含两种类型： requiredDuringSchedulingIgnoredDuringExecution ：必须满足的条件 preferredDuringSchedulingIgnoredDuringExecution ：优选的条件 spec.affinity.podAntiAffinity 与亲和性相反，表明将 Pod 尽量与其他 Pod 分开部署。 对于 Pod 亲和性与反亲和性，判断范围都是针对拓扑域来说的。通过 topologyKey 指定判断拓扑域的 label，具有相同 : 的节点会认为属于用一个拓扑域下： topologyKey:topology.kubernetes.io/zone # 如果两个节点具有相同的 topology.kubernetes.io/zone:\u003cval\u003e 的 label，那么它们属于同一个拓扑域。 所以，节点亲和性的规则为：对将被调度的节点，如果其相同拓扑域下的某个节点运行着满足条件的 Pod，那么就可以调度到该节点。 对应的，节点反亲和性的规则为：对将被调度的节点，如果其相同拓扑域下的某个节点运行着满足条件的 Pod，那么就尽量不要调度到该节点。 apiVersion:apps/v1kind:Deploymentmetadata:name:web-serverspec:selector:matchLabels:app:web-storereplicas:3template:metadata:labels:app:web-storespec:affinity:podAntiAffinity:requiredDuringSchedulingIgnoredDuringExecution:- labelSelector:matchExpressions:- key:appoperator:Invalues:- web-storetopologyKey:\"kubernetes.io/hostname\"# 拓扑域为节点podAffinity:requiredDuringSchedulingIgnoredDuringExecution:- labelSelector:matchExpressions:- key:appoperator:Invalues:- storetopologyKey:\"kubernetes.io/hostname\"containers:- name:web-appimage:nginx:1.16-alpine 上面例子中，podAntiAffinity 表明不要调度到同节点已经运行着 app:web-store 的 Pod 的节点上，podAffinity 表明调度到同节点运行着 app:store 的 Pod 的节点上。通俗点说，该 Pod 不能重复部署在同一个节点，并且每次部署要与 app:store 的 Pod 绑定。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"2.4 taint 与 tolerations 与 affinity 相反，taint 使节点排斥一类特定的 Pod。 为了能使 taint 节点能够被调度到一些特殊的 Pod，可以设置 Pod 的 toleration，表明不在意某些节点的 taint 。 2.4.1 taint 通过 kubectl taint 为节点增加一个 taint： $ kubectl taint nodes node1 key1=value1:NoSchedule 为 node1 添加 key1:value1 的 traint，其触发的效果是不能被调度（NoSchedule） 当然，你也可以为删除某个节点的 taint： kubectl taint nodes node1 key1=value1:NoSchedule- 结尾的 - 号表示是删除一个 taint； 设置的 kv 对用于来判断 toleration 是否匹配 taint。 目前包含几种类型的 effect ： NoSchedule ：不将 Pod 调度到该节点，但是不影响已经运行的 Pod； PreferNoSchedule ：尽量不降 Pod 分配到该节点，是个软性条件； NoExecute ：不将 Pod 调度到该节点，并且会驱逐已经运行并且不能容忍污点的 Pod； Kubernetes 会在一些条件下，自动会节点添加一些内置的污点： node.kubernetes.io/not-ready + NoExecute ：节点为准备好，Ready 为 false； node.kubernetes.io/unreachable + NoExecute ：节点不可达，Ready 为 unknown； node.kubernetes.io/memory-pressure + ：节点存在内存压力； node.kubernetes.io/disk-pressure + NoSchedule ：节点存在磁盘压力； node.kubernetes.io/pid-pressure + NoSchedule ：节点 PID 压力； node.kubernetes.io/network-unavailable + NoSchedule ：节点网络不可用； node.kubernetes.io/unschedulable + NoSchedule ：节点不可调度； node.cloudprovider.kubernetes.io/uninitialized + NoSchedule ：节点未被云平台初始化； DaemonSet 创建的 Pod DaemonSet 创建的 Pod 会自动加上上面两个 NoExecute 的 taint，使得其 Pod 不会被驱逐。 2.4.2 tolerations 当 Pod 设置的 spec.tolerations 能够 “匹配” 节点某个 taint 时，就可以认为该 taint 不存在。 “匹配” 有两个含义： 如果 operator 是 Exist，那么相同的 key 即可。如果 operator 为 Equal，那么 key val 都要相同 effect 相同 apiVersion:v1kind:Podmetadata:name:nginxlabels:env:testspec:containers:- name:nginximage:nginximagePullPolicy:IfNotPresenttolerations:- key:\"example-key\"operator:\"Exists\"effect:\"NoSchedule\" 可以容忍 key 为 “example-key”，effect 为 “NoSchedule” 的 taint； 通过 spec.tolerations.tolerationSeconds 可以指定匹配到容忍的污点后，能够持续容忍的时间。 tolerations:- key:\"key1\"operator:\"Equal\"value:\"value1\"effect:\"NoExecute\"tolerationSeconds:3600# 匹配到 taint 后，3600 内不会被驱逐 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:2:4","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"3 Eviction ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"3.1 节点压力驱逐 kubelet 会监控 CPU、Mem、磁盘空间、文件系统 inode 数量等资源，一旦某个资源消耗达到一个阈值，kubelet 会主动驱逐节点上的一个或多个 Pod，以回收资源。 驱逐时，kubelet 会将 Pod 设置为 Failed 状态，并停止 Pod，而上层的副本控制器可能会在其他地方创建 Pod 来替代。 驱逐的阈值分为： soft eviction thresholds ：达到软阈值后并持续了一段时间没有恢复，就会通过 graceful 的方式驱逐一些 Pod； hard eviction thresholds ：一旦触发阈值，立即通过 force 方式进行驱逐； 3.1.2 配置驱逐参数 驱逐相关的配置参数都需要配置 kubelet 的启动参数来进行配置。 3.1.3 驱逐策略 kubelet 会按照下面参数来决定驱逐 pod 的顺序： Pod 资源使用量是否超过 spec.request； Pod 优先级； Pod 相对于 spec.request 的资源使用情况； 因此，kubelet 会按照下面顺序进行驱逐：。 如果 BestEffort 或者 Burstable Pod 资源使用量超过 request。超出的越多的 Pod 优先被驱逐； 如果都是 Guaranteed 和 Burstable Pod 并小于 request，那么基于 Pod Priority 驱逐。 BestEffort Burstable Guaranteed 是 QosClass。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"3.2 API 驱逐 与节点压力驱逐的不同，API 驱逐是指通过 Eviction API 来进行主动的驱逐，并且停止 Pod 是 graceful。 kubectl drain 就是通过 API 进行驱逐，停止某个节点上的所有 Pod。 API 驱逐会受到 PodDisruptionBudgets 和 terminationGracePeriodSeconds 的控制。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:3:2","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"3.3 污点驱逐 在第 2 部分看到，自定义的污点也会导致 Pod 的驱逐，不能容忍 NoExecute 污点的 Pod 都会被驱逐。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:3:3","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"4 Preemption Pods 可以被提供一个优先级。高优先级的 Pod 会被优先调度，scheduler 甚至会尝试抢占低优先级的 Pod，来让高优先级的 Pod 先运行。 要使用优先级与抢占功能： 创建 PriorityClass； Pod 或者 Pod template 定义中指定 spec.priorityClassName 为一个特定的 PriorityClasses。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"4.1 PriorityClass PriorityClass 是一个 non-namespaced 资源，其包含一个 value 来描述优先级。 Kubernetes 内置两个 PriorityClass ：system-cluster-critical system-node-critical，表明是系统关键的组件、 一个基本的 PriorityClass 优先级如下： apiVersion:scheduling.k8s.io/v1kind:PriorityClassmetadata:name:high-priorityvalue:1000000globalDefault:falsedescription:\"This priority class should be used for XYZ service pods only.\" value ：优先级值，32 位整型，越大表示优先级越高。 globalDefault ：是否是系统默认优先级，没有指定 PriorityClass 的 Pod 使用默认优先级； 如果系统没有设置 globalDefault，那么默认优先级是 0。 description ：文本描述； ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:4:1","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"4.2 Pod 优先级 创建 Pod 时通过指定 spec.priorityClassName 来指定一个特定的 PriorityClass。 apiVersion:v1kind:Podmetadata:name:nginxlabels:env:testspec:containers:- name:nginximage:nginximagePullPolicy:IfNotPresentpriorityClassName:high-priority 当 Pod 优先级设置后，scheduler 会按照优先级对 pending Pods 进行排序，高优先级的 pending Pod 优先于低优先级的进行处理。 如果高优先级的 Pod 无法被调度到节点，那么 scheduler 才会继续调度到低优先级 Pod。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:4:2","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"4.3 抢占 当 scheduler 发现一个 Pod 无法被调度到任何节点时，就会触发抢占的逻辑。scheduler 会尝试计算：是否移除某个节点的一个或多个低优先级的 Pod，使得节点能够满足被调度的条件。 如果能够找到该节点，新 Pod 状态信息中的 nominatedNodeName 为被设置为节点名，使得用户可以看到抢占信息。 之后，节点上被低优先级的 Pod 会被驱逐（graceful stop 30s）。因此，这里会导致新 Pod 调度到该节点之间有一个需要等待的时间差。 所以，Nominated Node 这不代表新 Pod 必定会调度到该节点，也许驱逐期间出现别的节点满足调度条件，那么就会被调度。 如果新 Pod 与将被驱逐的 Pod 之间有 pod affinity 关系，那么抢占后亲和性关系就不再会被满足，因此 scheduler 不会选择这样的节点来进行抢占。同样，推荐在同优先级或者高优先级的 Pod 间设置 pod affinity。 同样，针对拓扑域下的 pod affinity，也会有上述的问题，因此 scheduler 不会进行跨节点抢占。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:4:3","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"参考 Blog：k8s 节点资源预留与 pod 驱逐 官方文档：Scheduling, Preemption and Eviction ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/schedule-preemption-eviction/"},{"categories":["k8s 学习"],"content":"1 概述 Kubernetes 不仅仅是一个编排框架，更是提供了极大的扩展性，可以看做一个资源框架。你可以基于 k8s 提供的种种功能，来满足你应用的需要。 其中 CRD 就是允许你来自定义 Resource，可以不修改 Kubernetes 代码来实现类似于 NetworkPolicy 这样的资源。 Note 可以理解 Kubernetes 提供了基础的框架，Pod Service Volume 等都是最基础的组件，而我们可以在这些组件之上进行再次抽象与组合，将其赋予一个特定的概念 CustomResource 以贵司 TiDB Operator 来举个例子，Operator 用于管理 TiDB 的集群，其核心组件包括 PD、TiDB、TiKV 程序。这些核心组件在最底层都是以 Pod 方式运行的，并且是多副本形式。而我们将所有的 Pod + Service 等组合在一起，构成了 TiDBCluster 这个 CustomResource。 所以，当你想部署 TiDB 集群时，只需要修改 TiDBCluster 这个资源的配置，然后提交到 Kubernetes 中。TiDB Operator（CRD Controller）就会根据 TiDBCluster 来操作，使得集群按照期望状态部署。 为什么能够做到？ Kubernetes 底层架构被尽可能的进行拆分与解耦，这样使得上层可以有很高的扩展性。 对于使用自定义资源，最基本要涉及到： CustomResourceDefinition ：一种 Resource 类型，为了让 k8s 能够认识到你的自定义资源，需要先提交 CRD； CustomResource Controller ：管理 CR 的程序，以 Pod 方式运行，并通过 k8s API 来管理 CR 以及执行操作； CustomResource ：你的自定义资源，就像 Pod Service 这种资源一样使用； 比喻 我习惯以编程的方式来看到这 3 个： CRD 是类定义，告诉编译器类型； CR 基于类创建的对象； CR Controller 是代码逻辑； ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:1:0","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"2 CRD CRD 是使用的 CustomResource 的基础，能够让 k8s 认识到你的自定义的资源。 基本的 CRD 定义如下（来自官网）： apiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:crontabs.stable.example.comspec:group:stable.example.comversions:- name:v1beta1# Each version can be enabled/disabled by Served flag.served:true# One and only one version must be marked as the storage version.storage:true- name:v1served:truestorage:falsescope:Namespacednames:# plural name to be used in the URL: /apis/\u003cgroup\u003e/\u003cversion\u003e/\u003cplural\u003eplural:crontabssingular:crontabkind:CronTabshortNames:- ctadditionalPrinterColumns:- name:Spectype:stringdescription:The cron spec defining the interval a CronJob is runjsonPath:.spec.cronSpec metadata.name ：命名，必须是 \u003cplural\u003e.\u003cgroup\u003e； spec group ：组织，用以 REST API 注册（/apis/\u003cgroup\u003e/\u003cversion\u003e），基本以 URL 方式； versions ：版本号，用以 REST API 注册（/apis/\u003cgroup\u003e/\u003cversion\u003e）； scope ：资源的范围，Namespaced 或者 Cluster； names plural ：CRD 的复数命名； singular ：CRD 的单数命名； kind ：CR 的名字，定义 CR 时使用； shortNames ：简短的名称，用以 kubectl 时简写； additionalPrinterColumns ：kubectl 打印出的额外信息，从 CR 的 spec 中获取相关的信息； ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:2:0","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"2.1 URL 创建 CRD 后，会自动在 API Server 建立其对应的 Endpoint URL，使得可以通过 HTTP 方式来查询与操作该 CR。 其 URL 为 /apis/\u003cgroup\u003e/\u003cversion\u003e/namespaces/\u003cns\u003e/\u003cplural\u003e/…。 例如，示例中的 API endpoint 为 /apis/stable.example.com/v1/namespaces/*/crontabs/… ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:2:1","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"2.2 Validation spec.validation 用以用户在提交 CR 时，对其定义进行合法性检查。 该功能需要配置 kube-apiserver 的 –feature-gates=CustomResourceValidation=true。 我们对上面的示例对其增加两个检查： spec.cronSpec 必须是匹配正则表达式的字符串 spec.replicas 必须是从 1 到 10 的整数 apiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:crontabs.stable.example.comspec:group:stable.example.comversion:v1scope:Namespacednames:plural:crontabssingular:crontabkind:CronTabshortNames:- ctvalidation:# openAPIV3Schema is the schema for validating custom objects.openAPIV3Schema:properties:spec:properties:cronSpec:type:stringpattern:'^(\\d+|\\*)(/\\d+)?(\\s+(\\d+|\\*)(/\\d+)?){4}$'replicas:type:integerminimum:1maximum:10 更多的 openAPIV3Schema 检查语法见 OpenAPI v3 schemas。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:2:2","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"2.3 Defaulting 与 Nullable 在 OpenAPI v3 validation schema 下，允许你为某些项指定默认值。 apiVersion 必须是 apiextensions.k8s.io/v1 下面示例为 spec.cronSpec 指定了一个默认值，而 spec.image 默认为 null 值： apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:crontabs.stable.example.comspec:group:stable.example.comversions:- name:v1served:truestorage:trueschema:# openAPIV3Schema is the schema for validating custom objects.openAPIV3Schema:type:objectproperties:spec:type:objectproperties:cronSpec:type:stringpattern:'^(\\d+|\\*)(/\\d+)?(\\s+(\\d+|\\*)(/\\d+)?){4}$'default:\"5 0 * * *\"image:type:stringnullable:truereplicas:type:integerminimum:1maximum:10default:1scope:Namespacednames:plural:crontabssingular:crontabkind:CronTabshortNames:- ct ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:2:3","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"2.4 Subresources TODO ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:2:4","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"2.5 Categories spec.names.categories 可以将资源分组，通过使用 kubectl get \u003ccategories\u003e 来获取一组资源。 apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:crontabs.stable.example.comspec:# … names:# categories 是定制资源所归属的分类资源列表categories:- all 上面例子将 CR 分类到 “all” 类别中，使得 kubectl get all 可以获取到该类别下的所有已经创建的资源。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:2:5","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"3 CR 在提交 CRD 后，我们就可以创建 CR 来提交了。当然，CR 中的 spec 相关的属性 k8s 是不会使用的，而是在 Controller 中解析并使用。 基于上面示例，创建一个 CronTab 资源： apiVersion:\"stable.example.com/v1\"kind:CronTabmetadata:name:my-new-cron-objectspec:cronSpec:\"* * * * /5\"image:my-awesome-cron-image 当我们提交资源后，其资源就保存在了 Kubernetes 中，可以查看与删除。但是，我们并没有注册 CronTab 的 Controller，所以没有基于该资源进行系统上的管理。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:3:0","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"4 CR Controller 上面看到，CR 与 CRD 都是静态的资源，而 CR Controller 就是基于已经创建的 CR 来进行实际的系统操作，使得整个系统是向期望的状态发展。 Controller 需要我们自行编写其逻辑，然后注册到 Kubernetes 中。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:4:0","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"4.1 Controller 模式 不论是内置的 Controller，还是你需要编写的自定义的 Controller，都需要遵循 控制器模式Controller Pattern 的方式实现。 我理解的控制器模式是：Controller 不断永久循环执行着 Controll Loop，而每一个 Loop 都是基于当前实际的资源状态（status），进行系统操作，向期望的资源状态（spec）靠拢。 所以整个逻辑是基于状态的，而不是基于事件的，这也就是 声明式 APIDeclarative API 与 命令式 APIImperative API 的区别。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:4:1","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"4.2 工作原理 Kubernetes 提供了资源状态的存储功能，而 Controller 需要实现的就是基于对资源的更变的控制逻辑。 一个 Controller 的工作原理，可以使用以下流程图表示： 整体上看，Controller 实现分为三个组件： Informer ：提供特定对象的存储，事件触发功能，即 List 与 Watch 功能； WorkQueue ：对象变更事件通知队列，通过队列来防止阻塞 Informer； Control Loop ：控制循环，基于事件来执行对应的操作； 4.2.1 Informer Informer 是 Kubernetes 提供的代码模块，针对于特定的 API 对象，包含几个职责： 同步 etcd 中对应 API 对象，将所有对象缓存到本地； 某个 API 对象的任何变更，都可以触发对象变更事件； 触发/检测到对象变更事件时，触发注册的 ResourceEventHandler 回调，即 AddFunc UpdateFunc DeleteFunc 回调； 针对于 etcd 的 List 与 Watch 机制，Informer 的同步机制也包含两种： 使用 Watch 的事件增量同步：当 APIServer 有对象的创建、删除或者更新，Informer.Reflector 模块会收到对应事件，解析后放入 Delta FIFO Queue； 使用 List 的定期周期全量同步：经过一定周期，Informer 会通过 List 来进行本地对象的强制更新。 该更新操作会强制触发“更新事件”，从而调用 UpdateFunc 回调 主动判断 ResourceVersion 所以 UpdateFunc 要通过对象的 ResourceVersion 来判断，是否对象有着真正的更新。 同时，Informer 中异步的不断读取 Delta FIFO Queue 中事件，并触发其注册的回调函数（AddFunc UpdateFunc DeleteFunc）。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:4:2","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"5 自定义资源示例 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:5:0","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"5.1 使用 kubebuilder 整体过程来自于官方文档 QuickStart 安装 kubebuilder 进行初始化 $ kubebuilder init --domain shiori.me --repo shiori.me/crd-practice --skip-go-version-check … Next: define a resource with: $ kubebuilder create api 创建一个 API $ kubebuilder create api --group shiori --version v1 --kind Echo Create Resource [y/n] y Create Controller [y/n] y Resource 会让其生成资源的定义文件 api/v1/echo_types.go Controller 会让其生成 Controller 逻辑框架 controllers/echo_controller.go 我们看一下基本的目录结构： TODO ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/crd/:5:1","tags":["k8s","云计算"],"title":"[WIP] K8s 学习 - CRD","uri":"/posts/cloud_computing/k8s_learning/crd/"},{"categories":["k8s 学习"],"content":"1 概述 Pod 在设计的理念上是无状态的，Pod 可以在任意时刻被销毁，可以在任意时刻被在别的节点创建相同的副本。 Deployment 与 RelicSets 就是基于这个理念设计的，它们仅仅保证 Pod 的副本数量，而不关心其他。 对于大多数程序，其都是需要 “状态” 的，也就是重启能够恢复之前的数据。这个状态可能包括： 存储 网络 启停顺序 ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"1.1 “状态” 是什么 对于存储很好理解，大多数程序会将数据保存到磁盘或者云盘，而重启后重新读取数据，来恢复状态。所以，Pod 重启前后读取到的存储要是一样的。 对于网络，基于 Service 的存在，一般的后端服务可以不用关系其所处于的网络环境。但是对于分布式程序或者 p2p 程序，每个程序是需要知道其他程序的网络地址的。所以，Pod 重启前后的网络地址要是一样的。 还有一点是基于 Pod 之间的关系，有时候多个 Pod 实例之间的启停顺序也应该是一样的。 ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:1:1","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"1.1 如何保存 “状态” 对于存储，PVC 是与 Pod 生命周期不耦合的，所以我们让 Pod 重启前后都使用同一个 PVC，那么其数据就是相同的。所以问题变为了：建立 Pod 与 PVC 的映射关系。 对于网络，Pod 的 IP 不是恒定的，这个是 Pod 的基本概念，无能更改。所以问题变为了：要找到一个可以代表这个 Pod 的恒定网络地址。 对于启动顺序，要保证 Pod 之间的运行顺序，那么需要认得每一个 Pod，也就是 Pod 的命名必须是有规范与固定的。 所有的问题都由固定的 Pod 命名来解决，Pod 会按照 0-N 的方式来进行命名，而这样 Pod0 可以固定到 Pod0 PVC，固定到 Pod0 DNS 域名，启动顺序按照 Pod0 Pod1 … 来启动。 这就是 StatefulSet 做的事情，总结一下： 固定的持久化存储，通过 PVC； 固定的网络标识，通过 Headless Service 使得 \u003cpodname\u003e.\u003cservice\u003e.\u003cnamespace\u003e 与固定命名的 Pod 绑定； 按照编号进行有序的启动与停止； ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:1:2","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"StatefulSet 基础 ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"2.1 定义 StatefuleSet 基本定义如下： apiVersion:apps/v1kind:StatefulSetmetadata:name:webspec:serviceName:\"nginx\"replicas:2selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:k8s.gcr.io/nginx-slim:0.8ports:- containerPort:80name:webvolumeMounts:- name:wwwmountPath:/usr/share/nginx/htmlvolumeClaimTemplates:- metadata:name:wwwspec:accessModes:[\"ReadWriteOnce\"]resources:requests:storage:1GistorageClassName:shared-ssd-storage serviceName ：使用的 Headless Service 命名； replicas ：副本数量； selector ：Pod selector，决定要管理哪些 Pod； template ：Pod template； volumeClaimTemplates ：创建的 PVC 模板； metadata.name ：创建的 PVC 基础命名，命名以 \u003cname\u003e-\u003cpodname\u003e 格式； spec ：与创建一个 PVC 的 spec 相同； ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"2.2 使用 注意事项： PV 或者 StorageClass 需要预先创建，不会自动创建与删除； 虽然 PVC 是由 StatefulSet 自动创建的，但是不会进行自动删除，这是为了保留数据； Headless Service 需要预先创建，不会自动创建与删除； 删除 StatefulSet 不会使其清理所有的 Pod，应该通过将 .spec.replicas 设置为 0 来让其清理所有的 Pod； ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"2.3 Pod 管理策略 通过 spec.podManagementPolicy 可以设置管理的 Pod 的策略： OrderedReady ：默认策略，按照 Pod 的顺序依次创建或停止，前一个 Pod 完成后才会进行下一个 Pod 的操作； Parallel ：所有 Pod 创建或停止都是并行的，也就是说你不需要启停顺序的特性； ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"2.4 Pod 升级策略 通过 spec.updateStrategy 可以设置 Pod 的升级策略，也就是当你更新 spec.template 的镜像版本时，触发的操作。 # …spec:updateStrategy:rollingUpdate:partition:3type:RollingUpdate type 指定升级策略 updateStrategy ：指定 RollingUpdate 时控制滚动升级行为 目前支持两种策略： OnDelete （默认）：需要升级时，并不会自动删除旧版本 Pod，需要用户手动停止旧版本的 Pod，才会创建对应新版本 Pod。 RollingUpdate ：自动删除旧版本的 Pod，并创建新版本 Pod。管理方式依赖于 Pod 管理策略。 2.4.1 Partitions 设置 spec.updateStrategy.rollingUpdate.partition 可以对 Pod 进行分区管理。只有序号大于或等于的 Pod 才会进行滚动升级，其他 Pod 保持不变（即使被删除后重新创建）。 # 设置 partition 为 1 $ kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":3}}}}' statefulset \"web\" patched # 更新 StatefulSet $ kubectl patch statefulset web --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/image\",\"value\":\"gcr.io/google_containers/nginx-slim:0.7\"}]' statefulset \"web\" patched # 验证更新，可以看到从 web-1 web-2 都升级了，而 web-0 没有变化 kubectl get pods -n mytest NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 3m2s web-1 1/1 Running 0 30s web-2 1/1 Running 0 50s ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:2:4","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"总结 StatefulSet 在原理上设计的让人感觉很优雅，仅仅从固定的 Pod 出发来实现 “有状态” 这个复杂的事情。 不过也许还有好多场景 StatefulSet 也无法满足，可能需要更多的开发。 需要弄清楚以下几点： 为了需要有状态应用 如何为 Pod 实现有状态 StatefulSet 基本定义与使用 StatefulSet 的 Pod 管理策略 StatefulSet 的升级策略 ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"参考 《Kubernetes 指南》 ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/statefulset/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/statefulset/"},{"categories":["k8s 学习"],"content":"1 概述 首先我们需要明确 Service 出现的背景：因为 Pod 设计上是无状态的，可以说没有固定的 IP，所以在其他组件访问某一个网络服务时，需要一个 “固定” 的地址。 这个固定的地址，就需要由 Service 来提供。同样，因为访问的地址固定了，Service 也可以提供负载均衡这样的功能。 最简单的例子，有一个 Web 后端服务，有着 3 个 Pod 运行。而前端想访问该后端服务，想要的只是一个固定的域名或者地址，而不关系后端服务的 Pod 会怎样被调度。 为什么这样设计 可以感觉，这就是分层。上下层之间只通过固定的地址通信，而不关心层内部是怎样运行的。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"2 Service 基础 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"2.1 定义 Service 基本的定义如下： apiVersion:v1kind:Servicemetadata:name:stringnamespace:stringlabels:- name:stringannotations:- name:stringspec:selector:[]type:stringclusterIP:stringsessionAffinity:stringports:- name:stringprotocol:stringport:inttargetPort:intnodePort:intstatus:loadBalancer:ingress:ip:stringhostname:stringtopologyKeys:- \"key\"externalName:string selector ：用于 Service 选择被代理的 Pod； type ： Service 类型，见 Service 的类型； clusterIP ：固定的地址，为空那么随机提供； sessionAffinity ： 设置负载均衡策略； ports ：提供需要代理的协议，源端口，目的端口，宿主机端口（NodePort 类型）； status ：使用 LoadBalancer 类型下，提供相关参数； topologyKeys ：控制流量转发的拓扑控制，优先将流量转发到相同 key 的 Node 上的 Pod； externalName ：ExternalName 类型 service 代理的集群外的服务域名； ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"2.2 Service 的 DNS 所有 Service 都会自动对应一个 DNS 域名，其命令方式为 \u003cservice\u003e.\u003cnamespace\u003e.svc.cluster.local。 当执行 nslookup \u003cservice\u003e 时，自动在当前的 namespace 下访问。 通过 nslookup \u003cservice\u003e.\u003cnamespace\u003e 也可以跨 namespace 进行 DNS 解析。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"2.3 Service 相关的环境变量 如果 Pod 在 Service 之后创建，那么集群中同 namespace Service 地址信息会通过 ENV 传递给容器（不包括 Headless Service）。 相关的环境变量包括： \u003cservice\u003e_PORT=\u003cproto\u003e://\u003cclusterip\u003e:\u003cport\u003e \u003cservice\u003e_PORT_\u003cport\u003e_\u003cproto\u003e=\u003cproto\u003e://\u003cclusterip\u003e:\u003cport\u003e \u003cservice\u003e_PORT_\u003cport\u003e_\u003cproto\u003e_ADDR=\u003cclusterip\u003e \u003cservice\u003e_PORT_\u003cport\u003e_\u003cproto\u003e_PORT=\u003cport\u003e \u003cservice\u003e_PORT_\u003cport\u003e_\u003cproto\u003e_PROTO=\u003cproto\u003e \u003cservice\u003e_SERVICE_HOST=\u003cclusterip\u003e \u003cservice\u003e_SERVICE_PORT=\u003cport\u003e \u003cservice\u003eSERVICE_PORT\u003cNAME\u003e=\u003cport\u003e 默认相关的环境变量使用的都是 service.spec.port[0] 信息。如果使用的多 port 代理，需要使用 xxx_PORT_xxx 相关环境变量。 环境变量默认还会提供 kubernetes 的 service 地址，用于 Pod 来访问 APIServer。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"2.4 负载均衡策略 k8s 默认提供两种负载均衡策略： RoundRobin ：轮询模式，将请求轮询到后端各个 Pod。默认模式 SessionAffinity ：基于客户端 IP 地址进行会话保持的模式。 即第一次将某个客户端发起请求到后端某个 Pod，之后相同客户端发起请求都会被转发到对应 Pod。 将 service.spec.sessionAffinity 指定为 “ClientIP”，就表明了开启 SessionAffinity 策略。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:2:4","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"3 Service 类型 Service 核心的功能是：代理。代理涉及到两个点：访问代理的前端，被代理的后端。 针对这个前后端的不同，Service 分为了 4 种类型： ClusterIP（默认）：代理一组后端 Pod，提供一个固定的内部地址 ClusterIP + Port，也称为 VIP； NodePort ：NodePort 在 ClusterIP 基础上，会将 ClusterIP + Port 映射到 Node 上的某个端口，使得集群外部可以访问内部服务； LoadBalancer ：LoadBalancer 在 NodePort 上，将一部分 Node 的端口映射到一个 IP 上，使得外部网络可以访问 Node； ExternalName ：代理集群外部服务的 DNS 域名，而不是通过 selector 来选择集群内部后端 Pod； 可以看到，ClusterIP NodePort LoadBalancer 是针对访问代理的前端做了区分，而 ExternalName 是在被代理后端的不同。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"3.1 ClusterIP ClusterIP 是最基本的 Service，通过一个 VIP + Port 来在集群内部代理了一组 Pod 的 Port。始终要记得，VIP 是在集群内部才能使用。 apiVersion:v1kind:Servicemetadata:name:service-pythonspec:ports:- port:3000# 入口端口protocol:TCP # 代理协议targetPort:443# 目标端口selector:run:pod-python # 仅代理匹配的 Podtype:ClusterIP 上述定义创建了一个随机选择 IP，代理 TCP 3000 -\u003e 443 端口的 Service。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"3.2 NodePort NodePort 是对 ClusterIP 的增强，增加一个宿主机上端口到代理源端口的转发，使得集群外部也可以访问集群内部的服务。 port-forward 当你执行 kubectl port-forward svc/xxx 命令时，其实也是增加了一个宿主端口到 Service 源端口转发，和 NodePort 一样。 通过 service.spec.ports[].nodePort 指定映射的宿主机端口： apiVersion:v1kind:Servicemetadata:name:service-pythonspec:ports:- port:3000protocol:TCPtargetPort:443nodePort:30080selector:run:pod-pythontype:NodePort 上述定义在 ClusterIP 基础上，增加了宿主机 30080 的端口转发。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:3:2","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"3.3 LoadBalancer 在云厂商环境下，Node 都是在云的托管集群中的，所以外网访问 k8s 集群内的路径为：“外网 -\u003e 云集群 -\u003e k8s 集群”。而 NodePort 仅仅解决了 “云集群 -\u003e k8s 集群” 这个问题。 因此，LoadBalancer Service 在 NodePort 基础上，提供了云厂商需要的负载均衡信息，而云厂商根据该信息设置好 “外网 -\u003e 云集群” 的转发路径。 apiVersion:v1kind:Servicemetadata:name:service-pythonspec:ports:- port:3000protocol:TCPtargetPort:443nodePort:30080selector:run:pod-pythontype:LoadBalancer spec 中定义仅仅是约定的规范，不同厂商所需要的更加细节的 LoadBalancer 的参数，大多数是通过 service.metadata.annotations 来提供： metadata:name:my-serviceannotations:service.beta.kubernetes.io/aws-load-balancer-access-log-enabled:\"true\"# Specifies whether access logs are enabled for the load balancerservice.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval:\"60\"# The interval for publishing the access logs. You can specify an interval of either 5 or 60 (minutes).service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name:\"my-bucket\"# The name of the Amazon S3 bucket where the access logs are storedservice.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix:\"my-bucket-prefix/prod\"# The logical hierarchy you created for your Amazon S3 bucket, for example `my-bucket-prefix/prod` 对于 LoadBalancer，更多的信息见官方文档：LoadBalancer。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:3:3","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"3.4 ExternalName 前面三种 Service 代理的后端都是集群内部的 Pod，而 ExternalName 不再是代理 Pod，而是将请求域名重定向另一个域名。 因此，ExternalName Service 不再提供代理的功能，而是提供了域名重定向的功能。 kind:ServiceapiVersion:v1metadata:name:service-pythonspec:ports:- port:3000protocol:TCPtargetPort:443type:ExternalNameexternalName:remote.server.url.com 通过 serivce.spec.externalName 指定被代理的域名，而集群内部的 Pod 就可以通过该 Service 访问外部服务了。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:3:4","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"4 Endpoint 当创建一个 Service 后，会根据 service.spec.selector 自动来匹配作为后端的 Pod。实际上，会对应一个 Endpoints 对象来代表其匹配到的代理目标，而其每一个被代理的 Pod 称之为 Endpoint。 $ kubectl get endpoints -A NAMESPACE NAME ENDPOINTS AGE default kubernetes 172.16.4.169:6443 3d1h kube-system kube-dns 10.36.0.4:53,10.36.0.5:53,10.36.0.4:53 + 3 more... 3d1h 当然，你也可以不提供 service.spec.selector，也不使用 ExternalName Service，而指定创建一个 Endpoints 对象。这样可以实现，Service 只代理指定的地址： apiVersion:v1kind:Servicemetadata:name:plat-devspec:ports:- port:3306protocol:TCPtargetPort:3306---apiVersion:v1kind:Endpointsmetadata:name:plat-devsubsets:- addresses:- ip:\"10.5.10.109\"ports:- port:3306 同名的 Endpoint 与 Service 自动被认为是相绑定的。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"5 Headless Service Service 会自动发现一组 Pod，并提供代理服务与负载均衡。不过有时候，Pod 中程序并不想使用 Service 的代理功能，而是仅仅想让 Service 作为一个服务发现的作用，例如，peer2peer 程序想要知道有哪些对端的程序。 通过 Service 的定义看，这种情况也就是不需要 service.spec.clusterIP，但是需要 service.spec.selector。这种特殊的 Service 被称为 Headless Service。 apiVersion:v1kind:Servicemetadata:name:mysql-balance-svcnamespace:mysql-spacelabels:name:mysql-balance-svcspec:ports:- port:3308protocol:TCPtargetPort:3306clusterIP:None # clusterIP 指定为 None 表明不需要selector:name:mysql-balance-pod 上述定义将 service.spec.clusterIP 定义为了 “None”，表明创建的是一个 Headless Service。但是 service.spec.selector 还是能够让 Service 选择到 Pod，并创建对应的 Endpoints。也就是说，我们可以通过 Endpoints 来知道哪些对应服务的 Pod 正在运行。 最最重要的，每个 Endpoint 对应的 Pod 是存在一个对应的 DNS 记录：\u003cpodname\u003e.\u003cservice\u003e.\u003cnamespace\u003e.svc.cluster.local。 这使得 Headless Service 在 StatefulSet 中有很好的应用，因为 StatefulSet 中每个 Pod 的命名是固定的，所以也就是其域名 \u003cpodname\u003e.\u003cservice\u003e.\u003cnamespace\u003e.svc.cluster.local 也固定了，那么 Pod 之间通过域名访问就不需要关心 Pod IP 的变化了。 Note StatefulSet 必须使用 Headless Service 来为每个 Pod 提供固定的网络地址标识。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"6 Ingress Service 提供了基于 4 层的代理，也就是基于 IP + Port 的代理。而 Ingress 出现就是为了支持 7 层的代理，典型的就是支持 HTTP/HTTPS 协议的代理。 Ingress 类似于 nginx 的配置，提供应用层的路由，将流量路由给基于传输层的 Service，而由 Service 将流量路由给底层的 Pod。 不过 Ingress 类似于 Service，仅仅是一个规则的定义，其代理的实现依赖于 Ingress Controller。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:6:0","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"6.1 Ingress Controller Ingress Controller 基于定义好的 Ingress 来实现实际的路由，可以理解为就是实际上的 nginx。 Ingress Controller 是不包含在 controller manager 默认启动的 controller 的，需要手动进行 controller。 当然，目前有着许多种的 Ingress Controller 的实现，具体见 Ingress Controller。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:6:1","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"6.2 Ingress 定义 Ingress 的配置和配置 nginx 类似，基于 HTTP path 路径进行路由的配置。 看一个示例定义： apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-resource-backendspec:defaultBackend:resource:apiGroup:k8s.example.comkind:StorageBucketname:static-assetsrules:- http:paths:- path:/iconspathType:ImplementationSpecificbackend:resource:apiGroup:k8s.example.comkind:StorageBucketname:icon-assets defaultBackend ： 用于配置默认的后端，当 rules 中所有都不满足时，就会使用 default 路由； rules ：定义路由策略 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:6:2","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"参考 《Kubernetes in Action》 Blog: 详解 k8s 4 种类型 Service ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/service/:7:0","tags":["k8s","云计算"],"title":"K8s 学习 - Service","uri":"/posts/cloud_computing/k8s_learning/service/"},{"categories":["k8s 学习"],"content":"1 PV PVPersistent Volume 代表一个实际可用的后端存储（也可能不是后端，而是 Local PV）。大多数情况下，PV 是一个网络文件系统，或者分布式存储，或者云厂商的云盘。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"1.1 PV 的定义 PV 的定义仅仅描述了存储的属性： apiVersion:v1kind:PersistentVolumemetadata:name:pv0003spec:capacity:storage:5GivolumeMode:FilesystemaccessModes:- ReadWriteOncepersistentVolumeReclaimPolicy:RecyclestorageClassName:slowmountOptions:- hard- nfsvers=4.1nfs:path:/tmpserver:172.17.0.2 capacity：描述存储的大小； volumeMode：存储的使用类型，包括： Filesystem 文件系统：使用时会通过 Mount 挂载到 Pod 某个目录。也支持自动为 device 创建文件系统 Block 块设备：通过 device 方式提供给 Pod accessModes ：设备在实际使用时会先 mount 到宿主机上，accessMode 决定了是否允许多节点复用，并其读写权限，包括： ReadWriteOnce：只允许一个节点挂载使用，并提供读写； ReadOnlyMany：允许多个节点挂载使用，提供读权限； ReadWriteMany：允许多个节点挂载使用，提供读写权限； 不同的 PV 类型对其 accessMode 支持程度不同，具体见 AccessMode。 storageClassName ：指定其所属的 StorageClass。 也可以不指定 StorageClass，那么只有不指定 class 的 PVC 才可以绑定该 PV。 persistentVolumeReclaimPolicy ：ReclaimPolicy 指定了当其绑定的 PVC 删除时，该如何处理 PV。 目前的回收策略包括： Retain ：保留 PV，PV 会变为 Released 状态； Recycle ：自动删除 PV 数据； Delete ：同时删除后端磁盘（AWS EBS、GCE PD 等云盘也会被删除）； 目前，NFS 和 HostPath 类型的 PV 仅仅支持 Recycle，云厂商磁盘可以支持 Delete。 mountOptions ：当 PV 挂载到节点上时，添加的附加选项。 针对不同类型的 PV，这个选项是不同的，具体见 Mount Points。 nodeAffinity ：PV 可以设定节点亲和性，来限制只有特定的节点可以使用其 PV。 Note nodeAffinity 在使用 Local PV 时必定要使用。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:1:1","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"1.2 PV 生命周期 PV 生命周期包括 5 个阶段： Provisioning ：即 PV 的创建，可以直接创建 PV（静态方式），也可以使用 StorageClass 动态创建； Binding ：将 PV 分配给 PVC； Using ：Pod 通过 PVC 使用该 Volume，并可以通过准入控制 StorageObjectInUseProtection 阻止删除正在使用的 PVC； Releasing ：Pod 释放 Volume 并删除 PVC； Reclaiming ：回收 PV，可以保留 PV 以便下次使用，也可以直接从云存储中删除； Deleting ：删除 PV 并从云存储中删除后段存储； 对应 5 个阶段，一个 PV 可能处于以下状态： Available：PV 已经创建，并且没有 PVC 绑定； Bound：PVC 绑定了该 PV； Released：PVC 已经被删除，而该 PV 还没有被回收； Failed：PV 自动回收失败； ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:1:2","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"2 PVC PVCPersistent Volume Claim 描述一个 Pod 对 PV 的需求，是基于 namespace 下的。 为什么需要 PV PVC？ 感觉还是为了解耦，解耦使得 Pod 与存储形成了多对多的关系。 最直观的好处，一个 PV 可以通过多个 PVC 进行绑定，使得数据可以被多个 Pod 共享使用。也可以预定义多个 PV，而通过 PVC 可以直接由调度去选择合适的 PV 进行绑定。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"2.1 PVC 定义 PVC 定义中主要是描述了对 PV 的需求，也就是告诉调度如何选择一个合适的 PV。 apiVersion:v1kind:PersistentVolumeClaimmetadata:name:myclaimspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:slow # 指定绑定的 StorageClassselector:matchLabels:release:\"stable\"matchExpressions:- {key: environment, operator: In, values:[dev]} Access Modes：访问模式，与 PV 的访问模式要匹配； Volume Modes：使用模式，与 PV 的访问模式要匹配； Resources：Pod 所需求的资源； Selector：进一步来过滤选择的 PV，只有满足匹配条件的 PV 才可以被绑定； StorageClass：指定所属的 StorageClass，只有相同 StorageClass 的 PV PVC 才可以绑定； 如果没有指定 StorageClass，如果系统设置了 Default StorageClass 则使用，否则只能绑定没有设置 StorageClass 的 PV。 不指定 StorageClass 在没有指定 storageClassName 下，行为是有多种情况的，见 Class。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"2.2 PVC 的使用 在 Pod 定义或者 Pod template 中，使用 pvc 类型 volume 来指定使用的 PVC。 apiVersion:v1kind:Podmetadata:name:mypodspec:containers:- name:myfrontendimage:nginxvolumeMounts:# 添加 volume 挂载- mountPath:\"/var/www/html\"name:mypdvolumes:- name:mypd # volume 类型为 PVCpersistentVolumeClaim:claimName:myclaim # 指定使用的 PVC ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"3 StorageClass PV 的创建方法有两种： 静态创建Static Provision，由管理员手动创建所有支持的 PV； 动态创建Dynamic Provision，定义 StorageClass，按 PVC 来创建合适的 PV； 所以明确，StorageClass 是根据 PVC，来创建/回收 PV 的。 而创建与回收的 Driver 就称为 Provisioner，不同类型的 PV 的创建与回收方式都是不同的，所以有着许多的 Provisioner 实现。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"3.1 StorageClass 定义 StorageClass 中大多数属性是其创建出 PV 的模板。 apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:standardprovisioner:kubernetes.io/aws-ebs # 指定 Provisionerparameters:type:gp2reclaimPolicy:RetainallowVolumeExpansion:truemountOptions:- debugvolumeBindingMode:Immediate provisioner ：指定 Provisioner 的实现，这也就决定了其创建 PV 的类型。 其内置支持的 Provisioner 见 provisioner。 当然也可以使用自定义的 Provisioner。 reclaimPolicy ：针对 PV 的回收策略，这也就决定了其创建 PV 的回收策略。默认为 Delete。 allowVolumeExpansion ：是否允许通过修改 PVC 对象的属性，来对使用的 PV 进行扩容。 大多数云存储都支持该选项，具体见 allow-volume-expansion。 mountOptions ：指定其创建的 PV 的 Mount Options。 volumeBindingMode ：决定了何时进行将 PV mount 到节点上。 Immediate 表明一旦创建 PVC 后，就需要绑定 PV。 WaitForFirstConsumer 将 PV 与 PVC 的绑定延后，等到 Pod 创建并使用 PVC 时，才会去绑定对应的 PV。 Local PV 必须使用 WaitForFirstConsumer 因为 Local PV 只有等到 Pod 创建并调度时，才能决定使用本地的 PV，所以必须使用 WaitForFirstConsumer。 allowedTopologies ：在使用云服务的存储时，其 PV 是有 “访问位置”（拓扑域） 的限制的，而就需要满足节点上的 Pod 仅仅使用特定位置的 PV。 所以在 WaitForFirstConsumer 模式下，通过 allowedTopologies 使得仅仅在特定的位置来创建 PV，从而节点的 Pod 可以正常的访问 PV。 parameters ：提供给 Provisioner 创建 PV 的参数，最多支持 512 个参数，长度不能超过 256KiB。 不同 Provisioner 支持不同参数，见 parameters。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"4 PV PVC StorageClass 之间的关系 下面一张图描述了 PV PVC StorageClass 之间的关系： ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"5 Local PV ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"5.1 手动创建使用 LocalPV 在自己测试环境下，往往没有一个云存储可以作为 PV 使用，而使用 Local PV，使得 Pod 使用的 PV 来自于本地的目录或者块设备。 先明确 Local PV 如何定义，即 PV 类型为 hostPath，表明实际存储设备来自于宿主机的一个目录。 同时，只有调度到该节点的 Pod 才能使用这个 PV，所以要设定 PV 的 spec.nodeAffinity，仅仅允许该节点使用该 PV： apiVersion:v1kind:PersistentVolumemetadata:name:example-pvspec:capacity:storage:5GivolumeMode:FilesystemaccessModes:- ReadWriteOncepersistentVolumeReclaimPolicy:DeletestorageClassName:local-storage # 指定 storageClassNamelocal:# local 类型表明使用 Local PVpath:/mnt/disks/vol1nodeAffinity:# 设定节点亲和性，使得只能本地节点使用 PVrequired:nodeSelectorTerms:- matchExpressions:- key:kubernetes.io/hostnameoperator:Invalues:- node-1 # 仅仅允许 node-1 使用该 PV 而为了让 PVC 与 PV 绑定推迟到 Pod 创建后才决定绑定的 PV，我们需要一个 StorageClass，即使其并不能够支持动态创建 PV： kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:local-storageprovisioner:kubernetes.io/no-provisioner # no-provisioner 不支持动态创建 PVvolumeBindingMode:WaitForFirstConsumer # 让 PVC 绑定推迟 而在使用时候，我们只需要一个普通的 PVC，指定好 spec.storageClassName 后，就可以使用 LocalPV 了。 kind:PersistentVolumeClaimapiVersion:v1metadata:name:example-local-claimspec:accessModes:- ReadWriteOnceresources:requests:storage:5GistorageClassName:local-storage # 指定 StorageClass，来绑定 PV ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:5:1","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["k8s 学习"],"content":"5.2 local pv static provisioner local pv static provisioner 将上面的步骤进行了简化，其启动了一个 DaemonSet，根据配置好的目录，自动生成对应目录的 Local PV。 所以带来的好处就是，我们不需要为一个个节点去创建对应的 PV 了，有 Pod 自动负责该事。 5.2.1 原理 local pv static provisioner 为创建一个 DamonSet，在每个节点创建 Daemon Pod。 每个节点上的 Daemon Pod 会根据其 ConfigMap 配置好的 discovery dir，在目下查找可以作为 PV 的子目录。 在 discovery dir 下挑选目录有两个条件： 目录必须是一个挂载点，也就是 mountinfo 中可以看到的； 为什么必须是挂载点？ 我猜测这是为了通过挂载点得到目录容量大小，从而填充 PV 容量相关属性。 满足其 ConfigMap 中指定的目录匹配方式 namePattern。 5.2.2 示例 而为了使用 LocalPV，我们也需要定义一个 StorageClass，让创建出的 LocalPV 能够属于正确的 StorageClass。 所以使用的步骤为： 每个节点上准备作为 LocalPV 的目录，该目录必须是一个挂载点，且位于 discovery dir 下； 创建 StorageClass，作为后续创建出的 Local PV 的 StorageClass； 定义并创建 ConfigMap，为了将相关的信息传递给 provisioner 的 DaemonSet Pod； 定义并创建 ServiceAccount，因为后续的 Daemon Pod 需要创建 Local PV 结构，所以需要相关的权限； 定义并创建 DaemonSet，根据 config map 信息，其 Pod 里程序会自动创建 Local PV，其定义就与上述的 PV 一致； 看一下其 StorageClass、ConfigMap 和 DaemonSet 定义： apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:fast-disks # name 要与下面 ConfigMap 相同provisioner:kubernetes.io/no-provisionervolumeBindingMode:WaitForFirstConsumerreclaimPolicy:Delete # 支持 Delete Retain---apiVersion:v1kind:ConfigMapmetadata:name:local-provisioner-config namespace:default data:storageClassMap:| fast-disks:# 上面创建的 StorageClass 名字hostDir:/mnt/fast-disks # 宿主机 discovery dirmountDir:/mnt/fast-disks # Daemon Pod 中查找的 discover dirblockCleanerCommand:- \"/scripts/shred.sh\"- \"2\"volumeMode:FilesystemfsType:ext4namePattern:\"*\"# 查找子目录的匹配方式---apiVersion:apps/v1kind:DaemonSetmetadata:name:local-volume-provisionernamespace:defaultlabels:app:local-volume-provisionerspec:selector:matchLabels:app:local-volume-provisioner template:metadata:labels:app:local-volume-provisionerspec:serviceAccountName:local-storage-admincontainers:- image:\"k8s.gcr.io/sig-storage/local-volume-provisioner:v2.4.0\"imagePullPolicy:\"Always\"name:provisioner securityContext:privileged:trueenv:- name:MY_NODE_NAMEvalueFrom:fieldRef:fieldPath:spec.nodeNamevolumeMounts:- mountPath:/etc/provisioner/config name:provisioner-configreadOnly:true- mountPath:/mnt/fast-disks name:fast-disksmountPropagation:\"HostToContainer\"volumes:- name:provisioner-config # 能够访问到 ConfigMapconfigMap:name:local-provisioner-config - name:fast-disks # 能够查看到 discovery dirhostPath:path:/mnt/fast-disks 等到自动创建出 Local PV 后，后续的流程其实就是使用一个 Local PV 的流程了。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/:5:2","tags":["k8s","云计算"],"title":"K8s 学习 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/pv-pvc-storageclass/"},{"categories":["网络"],"content":"SSL/TLS 总结","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 之前一直对 TLS 与证书理解的不清晰，这里尝试进行总结一下。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:0:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"1 密码套件 SSL 在握手的第一步就是确认所使用的密码套件Cipher Suite，使得首先 client 与 server 使用相同的算法进行通信。其中包含三个组件： 非对称加密算法：表明传输秘钥使用的非对称加密算法，例如 RSA、DHE_RSA 等； 对称加密算法：加密握手后，传输数据使用的加密算法，例如 AES_128_CBC 等； 数据摘要算法：数据校验使用的算法，例如 SHA256 等； 例如，Server 端传输 TLS_RSA_WITH_AES_256_CBC_SHA256 算法套件，表明其询问使用 RSA + AES_256_CBC + SHA256 三个算法组件。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:1:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"1.1 对称加密算法 对称加密算法的作用很简单：使用同一个秘钥，发送端进行数据加密，接收端进行数据解密。 相比于非对称加密，加解密速度快，所以会用于传输数据的加解密。 但是，对称加密无法解决秘钥传输问题，一旦双方交换秘钥时泄露，那么数据的安全性就无法保障了。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:1:1","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"1.2 非对称加密算法 这里不会说明非对称加密算法的算法原理，而是想弄清楚为什么使用非对称加密算法。 首先，我们明确非对称加密算法的作用。非对称加密算法包含两个密码： 公钥：可以公开的秘钥； 私钥：不可公开，自身持有的秘钥； 作用很简单，对于非对称加密，用公钥加密的数据只能用私钥解开，用私钥加密的数据只能用公钥解开。 这样的作用在哪？因为对于对称加密，交换的秘钥一旦泄露，那么加密的数据就会被破解。而对于非对称加密，需要交换的往往只有公钥，而公钥被泄露后，那么使用公钥加密的数据还是无法被破解的，那么还是能保证了单向的数据安全性。 但是，非对称加密算法加解密速度比较慢，所以在 TLS 中仅仅用于交换秘钥。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:1:2","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"1.3 摘要算法 摘要算法不是用于数据加密的，而是用于数据的校验。无论多大的数据，经过摘要算法最后得到固定长度的数据。而不同的数据得到的摘要结果是不同的。 因此，我们比较两个数据的摘要结果，就可以确定两个数据是否相同，也就是数据检验。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:1:3","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2 SSH 在 TLS 之前，先说一下 SSH，github 与 ssh 登录都使用了 SSH 协议。 SSHSecure Shell 是建立在应用层上基础上的网络安全协议（注意，其没有使用 TLS 协议）。 SSH 使用非对称加密技术 RSA 加密了所有传输的数据。使用了两种验证方式： 基于口令的安全验证：只要知道需要登录的机器的账号与密码，就可以登录。所有传输的数据都会被加密，但是无法防止“中间人”攻击。 基于秘钥的安全验证：提前创建公钥与私钥，把公钥放在服务器上。在客户端要登录服务器时，会发送其本地保存的公钥。服务端在指定目录查找对应公钥是否匹配。 可以看到，服务器使用公钥来进行客户端的验证，因此 Github 与 SSH 登录时都需要将本地生成的公钥先提前配置。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:2:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2 SSL/TLS ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:3:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2.1 SSL 与 TLS TLSTransport Layer Security 是 SSLSecure Sockets Layer 的升级版，下面是其发展的历史： 协议 创建时间 RFC SSL 1.0 - SSL 2.0 1995 SSL 3.0 1996 RFC 6101 TLS 1.0 1999 RFC 2246 TLS 1.1 2006 RFC 4346 TLS 1.2 2008 RFC 5246 TLS 1.3 2019 RFC 8446 而 HTTPS 就是在 TCP 建立连接后，先进行 TLS 协议握手，然后使用加密传输。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:3:1","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2.2 TLS 握手过程 看一下具体的握手过程： ClientHello：客户端发起建立 TLS 连接请求（TCP 连接已经建立） client 发送的消息中，会表明其自身支持的 TLS 以及密码套件，让 server 可以选择合适的算法。 即包括几个重要信息： 支持的 TLS 最高版本； 支持的密码套件，按照优先级会进行排序； 支持的压缩算法； （可选）session id，通过 session 重用可以跳过一些握手过程； 随机串，后序生成密码使用，使得每次握手得到的密码都是不一样的； ServerHello：服务端选出合适的密码套件 server 根据 client 的 hello 消息，在自己的证书中查找合适的证书（证书里包含了非对称加密算法和摘要算法），并挑选出合适的对称加密算法，也就得到了密码套件。同时，也会返回一个随机串。 如果 TLS 版本，或者密码套件无法匹配，那么直接会连接失败。 Certificate：服务端发送自己的证书，让 client 检查 ServerKeyExchange（可选）：发送非对接加密 premaster 生成所需的参数（如果算法需要的话） 对于一些非对称加密算法（DHE_RSA），需要 server 传递一些特殊的参数，用以生成【准密码（premaster）】，因此需要传递一些特殊参数。 CertificateRequest（可选）：如果服务端开启双向认证，那么就发送请求，要求 client 提供证书 在内部服务的 HTTPS 中，有时候 server 需要去验证 client 是否是内部的，也就是需要进行双向认证，这就需要 server 去检查 client 的证书。 当然，大部分 Web 服务不需要，而 client 认证是通过账号密码来决定的。 ServerHelloDone：服务端表明结束 Certificate（可选）：client 发送自己的证书 如果第 5 步发生，那么 client 需要发送自己的证书。就算 client 没有证书，也要发送空的消息，让 server 决定是否继续。 ClientKeyExchange：验证完毕证书后，生成 premaster，通过证书中公钥加密后发送 client 验证完毕 server 证书后，生成非对称加密算法的 premaster，然后通过证书的公钥发送。 如果算法需要，会使用第 4 步接收的相关参数进行生成。 CertificateVerify（可选）：发送 client 校验码 + 私钥加密校验码的数据，让 server 尝试解密并验证，来验证 client 证书确实是属性 client 的 如果第 7 步执行，那么为了防止 client 发送一个不属于自身的证书，所以需要进行一次非对称加密通信，使得 server 确保证书是属于 client 的（client 拥有着私钥）。 Finished ：根据密码套件，双方算出 master 密码，并且使用对称加密算法进行一次通信，来验证密码无误。 根据加密套件 + premaster，并且使用 client 随机串 + server 随机串，client server 各独立生成 master 密码，并且是一样的。 接着 client 与 server 都会使用缓存的校验码，使用 master 密码进行对称加密，然后发送给对方，让对方使用 master 密码进行解密。这样来验证 master 密码双方使用的是一样的。 TLS 完成后，client server 就得到了相同的 master 密码，作为后续对称加密通信的密码。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:3:2","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2.3 为何安全 在上面的流程中可以看到，首先 client server 使用了非对称加密算法来进行 premaster 的交换（第 8 步使用公钥加密后发送），而最终的 master 密码生成需要使用 premaster 作为一个参数。 可以简单地理解，双方生成密码的一个重要参数使用了非对称加密算法进行传输，保证了数据的安全，这样最后各自独立生成的 master 密码就不会泄露。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:3:3","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"3 证书 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:4:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"3.1 原理 我们从申请证书的过程看： 生成 申请（签名）文件，申请文件中包含了要使用的公钥，以及一些申请者的信息：地点、组织等。 将申请文件通过安全的方式（私下）发送给 CA。 CA 就是一个用于分发证书的权威机构。 CA 使用自己的私钥，对申请文件进行签名（加密），得到了证书。 可以看到，最后得到的证书中包含了： 加密后的公钥 申请者的信息 CA 信息 证书年限 等 在浏览器与服务器中，会内置世界上所有 CA 的根证书，也就是包含了 CA 的公钥。通过证书公钥对 server 证书进行解密，并且判断其中的信息，就可以判断出该证书是否是 CA 签署的。 Note xx.crt 是证书文件，xx.crs 是申请文件 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:4:1","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"3.2 自签名 当我们内部服务，仅仅需要自签名时，其实就是自己生成了称为了一个 CA，得到了根证书与私钥。 而后使用私钥给 server 签证书，将根证书内置给 client 使用，就可以进行自签名的 TLS 通信。 下面进行一次自签名的过程： 生成根证书与私钥 openssl genrsa -out root.key 2048 openssl req -new -x509 -days 3650 -key root.key -out ca.crt 生成 server 证书与私钥，并使用根证书私钥签名 openssl req -newkey rsa:2048 -nodes -keyout server.key -out server.csr openssl x509 -req -extfile \u003c(printf \"subjectAltName=DNS:caliper.xycloud.com\") -days 3650 -in server.csr -CA root.crt -CAkey root.key -CAcreateserial -out server.crt （可选）如果需要双向认证，那么要生成 client 的证书，并使用相同的根证书签名。 openssl req -newkey rsa:2048 -nodes -keyout client.key -out client.csr openssl x509 -req -days 3650 -in client.csr -CA root.crt -CAkey root.key -CAcreateserial -out client.crt ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:4:2","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"3.2 总结 换个角度看，为了证书使用需要的条件： server 使用 CA 私钥加密后的证书； client 内置了 CA 根证书（公钥）； 而验证就是 client 使用公钥正常解密数据，然后检查下数据。 所以证书，其实就是给 server “盖了一个章”，表明其是权威机构认证的。而 client 内置的根证书使得 client 可以认到这个权威机构。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:4:3","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"参考 Blog：SSL/TLS 及证书概述 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:5:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Linux 存储架构中的 VFS 层的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:0:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"1 VFS 整体模型 虚拟文件系统Virtual File System 是在用户进程与文件系统之间的一个抽象层，使得用户进程无需关系文件系统的实现，而通过相同的 IO 接口就可以进行数据的读写。 当用户程序对一个 fd 调用 IO 系统调用时，VFS 根据其对应的文件系统，调用其各个函数的实现。 可以将其想象问，你定义了一个 Golang interface VFS，而不同的文件系统实现了其 interface 的操作。上层只需要调用 VFS 的各个接口即可。 Tip “一切的抽象层都是为了解耦” ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:1:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2 VFS 的通用文件模型 为了管理不同的文件系统，VFS 需要一个通用的文件模型来抽象整个文件系统的模型，而不同的文件系统需要通过自身的实现来满足这个模型。 VFS 的通用文件模型主要包含 4 个对象： superblock：保存文件系统的基本元数据，一个文件系统有着一个 superblock 对象，包含文件系统类型、大小、状态等信息； inode：保存一个文件或目录相关的元信息，代表着一个文件或目录，包含文件访问权限、文件类型、大小等（不包括文件名）； file：进程打开文件后，文件的表示，进程所看见的文件； dentry：保存文件名（目录名）与其 inode 的对应关系，用于加速文件的查询； ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.1 inode inode 是针对文件系统维度而言的，是每个文件或目录在文件系统的唯一标识，并包含其相关的元信息。可以想到，inode 需要持久化在具体的存储设备上，用以机器重启后恢复文件信息。 看一下 inode 的结构，可以知晓其对应的包含的信息： // fs.h struct inode { umode_t i_mode; kuid_t i_uid; kgid_t i_gid; unsigned int i_flags; const struct inode_operations *i_op; struct super_block *i_sb; struct address_space *i_mapping; #ifdef CONFIG_SECURITY void *i_security; #endif /* Stat data, not accessed from path walking */ unsigned long i_ino; /* * Filesystems may only read i_nlink directly. They shall use the * following functions for modification: * * (set|clear|inc|drop)_nlink * inode_(inc|dec)_link_count */ union { const unsigned int i_nlink; unsigned int __i_nlink; }; dev_t i_rdev; loff_t i_size; struct timespec64 i_atime; struct timespec64 i_mtime; struct timespec64 i_ctime; spinlock_t i_lock; /* i_blocks, i_bytes, maybe i_size */ unsigned short i_bytes; u8 i_blkbits; u8 i_write_hint; blkcnt_t i_blocks; #ifdef __NEED_I_SIZE_ORDERED seqcount_t i_size_seqcount; #endif /* Misc */ unsigned long i_state; struct rw_semaphore i_rwsem; unsigned long dirtied_when; /* jiffies of first dirtying */ unsigned long dirtied_time_when; struct hlist_node i_hash; struct list_head i_io_list; /* backing dev IO list */ #ifdef CONFIG_CGROUP_WRITEBACK struct bdi_writeback *i_wb; /* the associated cgroup wb */ /* foreign inode detection, see wbc_detach_inode() */ int i_wb_frn_winner; u16 i_wb_frn_avg_time; u16 i_wb_frn_history; #endif struct list_head i_lru; /* inode LRU list */ struct list_head i_sb_list; struct list_head i_wb_list; /* backing dev writeback list */ union { struct hlist_head i_dentry; struct rcu_head i_rcu; }; atomic64_t i_version; atomic64_t i_sequence; /* see futex */ atomic_t i_count; atomic_t i_dio_count; atomic_t i_writecount; #if defined(CONFIG_IMA) || defined(CONFIG_FILE_LOCKING) atomic_t i_readcount; /* struct files open RO */ #endif union { const struct file_operations *i_fop; /* former -\u003ei_op-\u003edefault_file_ops */ void (*free_inode)(struct inode *); }; struct file_lock_context *i_flctx; struct address_space i_data; struct list_head i_devices; union { struct pipe_inode_info *i_pipe; struct block_device *i_bdev; struct cdev *i_cdev; char *i_link; unsigned i_dir_seq; }; __u32 i_generation; void *i_private; /* fs or device private pointer */ } __randomize_layout; i_mode：文件类型，文件、目录等，也包括一些权限信息； i_uid、i_gid：文件所属的 UID 与 GID； i_flags：文件状态标记； i_op：inode 操作集合，不同的文件系统注册不同的操作； i_sb：指向所属文件系统的 superblock 结构； i_mapping：inode 对应的地址空间（address_space），对于 mmap 很重要； i_ino：inode 索引号； i_nlink：inode 引用计数； i_rdev：inode 表示设备文件时，为设备的 dev number（major+minor）； i_size：inode 对应的文件大小； i_atime、i_mtime、i_ctime：文件最后访问时间、文件内容最后修改时间、inode 最后变化时间（权限、所有者等） i_bytes：文件占用存储设备的字节数（考虑稀疏文件）； i_blocks：文件占用存储设备的块数（考虑稀疏文件）； i_hash：inode 所存在的 hash 表； i_lru：inode 构成的 LRU list； i_fop：文件操作函数集合，不同文件系统不同实现； 从上面可以看到，任何文件系统上的每一个单元（普通文件、目录、设备、管道等等），在 VFS 都由一个 inode 来抽象。 因此，对于每个进程，所有的操作都是相同的，即基于文件系统某个“文件”的增删改查等等，接口是统一的。而各个文件系统就要实现自身的 i_op、i_fop 操作。 例如，对于网络 IO，也是通过 VFS 层操作的，而需要网络子系统实现一个走网络的 i_fop 操作。 Note “一切皆文件” 2.1.1 文件系统对 inode 操作 看一下文件系统对 inode 的操作 inode_operations： // fs.h struct inode_operations { struct dentry * (*lookup) (struct inode *,struct dentry *, unsigned int); const char * (*get_link) (struct dentry *, struct inode *, struct delayed_call *); int (*permission) (struct inode *, int); struct posix_acl * (*get_acl)(struct inode *, int); int (*readlink) (struct dentry *, char __user *,int); int (*create) (struct inode *,struct dentry *, umode_t, bool); int (*link) (struct dentry *,struct inode *,struct dentry *); int (*unlink) (struct inode *,struct dentry *); int (*symlink) (struct inode *,struct dentry *,const char *); int (*mkdir) (struct inode *,struct dentry *,umode_t); int (*rmdir) (struct inode *,struct dentry *); int (*mknod) (struct inode *,struct dentry *,umode_t,dev_t); int (*rename) (struct inode *, struct dentry *, struct inode *, struct dentry *, unsigned int); int (*setattr) (struct dentry *, struct iattr *); int (*getattr) (const struct path *, struct kstat *, u32, uns","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:1","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.2 superblock superblock 包含一个具体文件系统的全局信息。 struct super_block { struct list_head s_list; /* Keep this first */ dev_t s_dev; /* search index; _not_ kdev_t */ unsigned char s_blocksize_bits; unsigned long s_blocksize; loff_t s_maxbytes; /* Max file size */ struct file_system_type *s_type; const struct super_operations *s_op; const struct dquot_operations *dq_op; const struct quotactl_ops *s_qcop; const struct export_operations *s_export_op; unsigned long s_flags; unsigned long s_iflags; /* internal SB_I_* flags */ unsigned long s_magic; struct dentry *s_root; struct rw_semaphore s_umount; int s_count; atomic_t s_active; char s_id[32]; /* Informational name */ uuid_t s_uuid; /* UUID */ unsigned int s_max_links; fmode_t s_mode; const struct dentry_operations *s_d_op; /* default d_op for dentries */ /* s_inode_list_lock protects s_inodes */ spinlock_t s_inode_list_lock ____cacheline_aligned_in_smp; struct list_head s_inodes; /* all inodes */ spinlock_t s_inode_wblist_lock; struct list_head s_inodes_wb; /* writeback inodes */ } __randomize_layout; s_list ：所有存在的文件系统以 list 方式组织； s_dev ：文件系统对应的设备的 dev 编号； s_blocksize_bits、s_blocksize ： s_maxbytes ：可以创建的最大文件大小； s_type ：文件系统类型； s_op ：文件系统操作，包含 mount、umount 等操作的实现； dq_op ：文件系统 quota 功能的实现； s_magic ：文件系统 magic number，每类文件系统有着唯一的 magic number； s_root ：root 目录，如果是存储文件系统的话； s_uuid ：文件系统 UUID，每个文件系统有着单机唯一的 UUID； s_inodes ：所有 inode 的组成的链表； s_inodes_wb ：所有等待 writeback 的 inode 组成的链表； ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:2","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.3 file file 是文件在进程中的表示，当进程通过 open() 打开一个文件得到文件描述符后，内核就会有一个 file 对象来表示这个“打开”。 如下图所示，每个进程所打开的每个 fd，就对应这一个 file 结构对象。 看下具体的 file 对象： // fs.h struct file { union { struct llist_node fu_llist; struct rcu_head fu_rcuhead; } f_u; struct path f_path; struct inode *f_inode; /* cached value */ const struct file_operations *f_op; spinlock_t f_lock; enum rw_hint f_write_hint; atomic_long_t f_count; unsigned int f_flags; fmode_t f_mode; struct mutex f_pos_lock; loff_t f_pos; struct fown_struct f_owner; const struct cred *f_cred; struct file_ra_state f_ra; u64 f_version; #ifdef CONFIG_SECURITY void *f_security; #endif /* needed for tty driver, and maybe others */ void *private_data; #ifdef CONFIG_EPOLL /* Used by fs/eventpoll.c to link all the hooks to this file */ struct list_head f_ep_links; struct list_head f_tfile_llink; #endif /* #ifdef CONFIG_EPOLL */ struct address_space *f_mapping; … } __randomize_layout __attribute__((aligned(4))); /* lest something weird decides that 2 is OK */ f_owner 包含处理该文件的进程有关信息，例如进程的 pid； f_ra 表示预读相关的操作，包含预读文件数据的大小、预读的方式等； f_mode 为打开文件时传递的方式，例如 readonly rw 等模式； f_flags 为 open 系统调用时传递的额外的 flag 标志； f_path 用于维护两个信息： 文件名与 inode 之间的关联（dentry）； 文件所在文件系统的信息（vfsmount）； f_op 为对应 inode 的文件操作函数； f_mapping ：指向文件对应的 inode 的地址空间映射，即 inode -\u003e i_mapping； file.f_path 属性包含文件对应的 dentry 对象，dentry 中包含着对应的 inode 指针，因此完成了 file -\u003e inode 的映射。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:3","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.4 dentry dentry 表示的是一个文件名与其对应的 inode 的关系，dentry 组成的树型结构构建起了某个文件系统的目录树。 每个文件系统有着一个没有父 dentry 的根目录，被文件系统对应的 superblock 所引用。 必须要清楚，磁盘等持久化存储设备上并没有存储 dentry。dentry 树型结构的构建是在 VFS 读取目录、文件的数据后，对应创建并构建的。也就是说，dentry 是一个缓存结构，用于运行时构建出树型组织结构，来加速文件、目录的查找。 看下其主要的数据结构： // dcache.h struct dentry { /* RCU lookup touched fields */ unsigned int d_flags; /* protected by d_lock */ seqcount_spinlock_t d_seq; /* per dentry seqlock */ struct hlist_bl_node d_hash; /* lookup hash list */ struct dentry *d_parent; /* parent directory */ struct qstr d_name; struct inode *d_inode; /* Where the name belongs to - NULL is * negative */ unsigned char d_iname[DNAME_INLINE_LEN]; /* small names */ /* Ref lookup also touches following */ struct lockref d_lockref; /* per-dentry lock and refcount */ const struct dentry_operations *d_op; struct super_block *d_sb; /* The root of the dentry tree */ unsigned long d_time; /* used by d_revalidate */ void *d_fsdata; /* fs-specific data */ union { struct list_head d_lru; /* LRU list */ wait_queue_head_t *d_wait; /* in-lookup ones only */ }; struct list_head d_child; /* child of parent list */ struct list_head d_subdirs; /* our children */ /* * d_alias and d_rcu can share memory */ union { struct hlist_node d_alias; /* inode alias list */ struct hlist_bl_node d_in_lookup_hash; /* only for in-lookup ones */ struct rcu_head d_rcu; } d_u; } __randomize_layout; d_parent：父目录的 dentry，根目录 dentry 指向自身； d_name：文件名/目录名； d_inode：文件名对应文件/目录的 inode； d_op：文件系统 dentry 操作的实现； d_sb：dentry 所属的文件系统的 superblock 结构； d_lockref：dentry 引用计数； d_alias：链表元素，不同 dentry 表示相同文件时，会通过链表链接所有 dentry。例如，使用硬链接将两个不同名称对应同一个文件时，会发生这种情况； 2.4.1 dentry 操作 dentry_operations 结构保存了指向特定文件系统对 dentry 需要实现的操作。 // dcache.h struct dentry_operations { int (*d_revalidate)(struct dentry *, unsigned int); int (*d_weak_revalidate)(struct dentry *, unsigned int); int (*d_hash)(const struct dentry *, struct qstr *); int (*d_compare)(const struct dentry *, unsigned int, const char *, const struct qstr *); int (*d_delete)(const struct dentry *); int (*d_init)(struct dentry *); void (*d_release)(struct dentry *); void (*d_prune)(struct dentry *); void (*d_iput)(struct dentry *, struct inode *); char *(*d_dname)(struct dentry *, char *, int); struct vfsmount *(*d_automount)(struct path *); int (*d_manage)(const struct path *, bool); struct dentry *(*d_real)(struct dentry *, const struct inode *); } ____cacheline_aligned; d_revalidate：检查内存中各个 dentry 对象构建的结构，是否符合当前文件系统的情况。这对于网络文件系统至关重要。 因为网络文件系统不直接关联到内核与 VFS，所有的数据都要通过网络 IO 来收集。该函数用于保证 dentry 结构与实际情况的一致性。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:4","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.5 模型总结 首先从内核角度看，一个已经挂载的文件系统就是由 superblock + inode 组成的。文件系统信息保存在 superblock 中，每个文件由 inode 表示。 对于一个文件的读写，就是找到对应文件的 inode，然后调用底层文件系统的 read/write 接口。一个目录 inode 对应的数据，就是其子文件或者目录的命名。 因为文件的读写增删是非常频繁的，为了在目录树中更快速的查找 inode，内核维护了 dentry 结构树，dentry 包含了文件名与 inode 的映射关系，使得不用依靠读写磁盘来查找文件对应的 inode。 从进程角度看，每个进程可以打开多文件，因此使用 file 结构代表了每个进程打开的文件，从 file -\u003e dentry -\u003e inode 来找到其对应的文件。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:5","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3 文件操作实现 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:3:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.1 挂载 文件系统的挂载有 mount 系统调用触发，当文件系统挂载到一个目录时，挂载点的内容就被替换为了被挂载的文件系统中的根目录的内容。前一个目录消失，无法访问，当挂载点被取消挂载后又会重新出现，并且数据不会改变。 每个挂载的文件系统都对应内核中的 mount 结构对象： struct vfsmount { struct dentry *mnt_root; /* root of the mounted tree */ struct super_block *mnt_sb; /* pointer to superblock */ int mnt_flags; } __randomize_layout; struct mount { struct hlist_node mnt_hash; struct mount *mnt_parent; struct dentry *mnt_mountpoint; struct vfsmount mnt; union { struct rcu_head mnt_rcu; struct llist_node mnt_llist; }; #ifdef CONFIG_SMP struct mnt_pcp __percpu *mnt_pcp; #else int mnt_count; int mnt_writers; #endif struct list_head mnt_mounts; /* list of children, anchored here */ struct list_head mnt_child; /* and going through their mnt_child */ struct list_head mnt_instance; /* mount instance on sb-\u003es_mounts */ const char *mnt_devname; /* Name of device e.g. /dev/dsk/hda1 */ struct list_head mnt_list; struct list_head mnt_expire; /* link in fs-specific expiry list */ struct list_head mnt_share; /* circular list of shared mounts */ struct list_head mnt_slave_list;/* list of slave mounts */ struct list_head mnt_slave; /* slave list entry */ struct mount *mnt_master; /* slave is on master-\u003emnt_slave_list */ struct mnt_namespace *mnt_ns; /* containing namespace */ struct mountpoint *mnt_mp; /* where is it mounted */ union { struct hlist_node mnt_mp_list; /* list mounts with the same mountpoint */ struct hlist_node mnt_umount; }; struct list_head mnt_umounting; /* list entry for umount propagation */ #ifdef CONFIG_FSNOTIFY struct fsnotify_mark_connector __rcu *mnt_fsnotify_marks; __u32 mnt_fsnotify_mask; #endif int mnt_id; /* mount identifier */ int mnt_group_id; /* peer group identifier */ int mnt_expiry_mark; /* true if marked for expiry */ struct hlist_head mnt_pins; struct hlist_head mnt_stuck_children; } __randomize_layout; mnt_parent：挂载点位于父文件系统挂载； mnt_mountpoint：挂载点，也就是父文件系统中挂载点对应的 dentry； mnt：包含代表的文件系统的挂载属性： mnt_root：文件系统根目录的 dentry； mnt_sb：文件系统对应的 superblock； mnt_flags：挂载标志； mnt_count：挂载的引用计数； mnt_ns：所属的 mount namespace； mnt_devname：挂载的设备名称，例如 /dev/sda； 挂载子树相关数据结构 …； mnt_id：该挂载的唯一 id； mnt_group_id：同组的挂载点的集合，即 peer group id； 3.1.1 共享子树 普通的挂载理解起来并不难，但是涉及到 bind mount 与 mount namespace 后，事情开始不一样了起来。想象几个场景： 如果系统新添加了磁盘，因为容器间 mount namespace 隔离了挂载信息，如果容器想用这个磁盘，是否只能手动进行挂载。 Shared subtrees 可以帮助我们解决这个问题。 共享子树Shared subtrees 就是一种控制子挂载点能否在其他地方被看到的技术，只会在 bind mount 和 mount namespace 中被使用到。 这个概念比较绕，并且也不会常常用到，可以先跳过。 首先，需要先知道两个概念： peer group：一个或多个挂载点的集合，同组的挂载之间会共享挂载信息，也就是可以看到。 目前有两种情况会让两个挂载点属于同一个 peer group： 使用 mount –bind，并且挂载的 propagation type 为 share，这样源目录和目标目录属于同一个 peer group（前提是源是一个挂载点，而不是普通文件或者普通目录）。 当创建新的 mount namespace 时，新 namespace 会拷贝一份老 namespace 的挂载点信息，于是新的和老的 namespace 里面的相同挂载点就会属于同一个 peer group。 propagation type：每个挂载点都有一个 type 标志，由它决定同一个 peer group 里的其他的挂载点下面是不是也会创建和移除相应的挂载点。 目前包含四种类型的 propagation type： MS_SHARED：挂载信息在同一个 peer group 的不同挂载点之间共享传播； MS_PRIVATE：挂载信息不共享，设置 private 的挂载点不属于任何的 peer group； MS_SLAVE：传播是单向的，同一个 peer group 中，master 的挂载点下面发生变化会共享到 slave。反之，slave 下变化不会影响 master； MS_UNBINDABLE：和 MS_PRIVATE 相同，只是这种类型挂载点不能作为 bind mount； 可以看到，peer cgroup 的概念就是 mount 结构中的 mnt_group_id，各个不同的 type 的子挂载，会被保存在不同的 mount 的链表中。 有个大神给出了清晰的示例与解释，见：Shared subtrees ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:3:1","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.2 查找 inode 查找 inode 应该是最常见的操作了，当你要打开文件或者进入目录，内核第一个任务就是要根据其路径，来查找到对应的 inode。 因为 dentry 的存在，查找自然分为了：通过 dentry 缓存查找，以及通过实际的读文件系统查找。 先看下通过 dentry 缓存查找： 解析文件路径，然后不断查找对应的 dentry。 例如 /a/b/c，先通过 root dentry 查找 a 对应的 dentry，接着通过 a dentry 查找 b，不断循环。 因为 dentry 只是缓存，所以需要使用 d_revalidate() 来检查 dentry 是否是有效地。 返回有效 dentry 的 inode。 当对应 dentry 不存在，或者无效之后，就会通过文件系统 inode 操作 lookup() 接口，通过底层的文件系统进行查找，并将其重新记录到 dentry 中。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:3:2","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"参考 Blog：linux 的 VFS 详解 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:4:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"Golang 常见调试方法归纳总结","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Golang 调试方法的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:0:0","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1 go pprof Go pprof 是 Go 性能分析工具，在程序运行的过程中，可以记录运行的：CPU、Mem、goroutine 等情况。基本定位 Go 程序的问题第一反应就是使用 go pprof。 pprof 像基本的采集一样，分为采集、上报、分析三个部分。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:1:0","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.1 采集 Go 标准库提供个各个数据的采集接口，可以主动开启 pprof 并将数据写入到 io.writer 中。 其包括： pprof.Profiles() ：返回支持的 profile 的快照。包括：goroutine、threadcreate、heap、allocs、block、mutex； trace.Start() / trace.Stop() ：开启各个事件的追踪，包括 goroutine 状态变更、GC 事件、mheap 大小变更等； runtime 包函数； 不过大多数情况下，我们通过使用 net/http/pprof 就行了，其注册了各个 HTTP 请求的处理函数，而其函数使用前面的接口进行的数据的采集。 // net/http/pprof/pprof.go func init() { http.HandleFunc(\"/debug/pprof/\", Index) http.HandleFunc(\"/debug/pprof/cmdline\", Cmdline) http.HandleFunc(\"/debug/pprof/profile\", Profile) http.HandleFunc(\"/debug/pprof/symbol\", Symbol) http.HandleFunc(\"/debug/pprof/trace\", Trace) } /debug/pprof/ ：引导界面，包含各个子类别的访问； /debug/pprof/cmdline ：输出程序启动的命令行； /debug/pprof/profile ：输出大多数 profile 结果，调用 pprof.Profiles()； /debug/pprof/trace ：一定时间内 go runtime 事件的追踪，调用 trace.Start()； ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:1:1","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.2 上报 前面看到，net/http/pprof 包就 init 注册对应的 HTTP 请求处理函数，而我们只要在程序中导入 net/http/pprof 并启动一个 HTTP Server 就可以使用其提供的上报的功能。 package main import ( \"net/http\" _ \"net/http/pprof\" ) func main() { // … // 启动对应的 HTTP Server // 这里我的程序中 main 函数后面会阻塞, 所以用协程启动 server go func() { if err := http.ListenAndServe(\"0.0.0.0:12300\", nil); err != nil { panic(err) } }() // … } 1.2.1 浏览器方式 浏览器打开指定的地址的 /debug/pprof/ 页面，可以看到引导页面： 各个支持的 profile 都有简要的解释，这里翻译一下： allocs ：过去所有的内存分配的样本； block ： cmdline ：输出程序执行的命令； goroutine ：所有 goroutine 当前执行的上下文； heap ：heap 内存使用的样本； mutex ： profile ：一定时间内的 CPU profile，可以通过参数指定采集多少时间； threadcreate ：创建新的系统线程的函数执行上下文，也许在 cgo 中很有用； trace ：一定时间内程序的执行 trace，可以通过参数指定采集时间。 当你点击其中的某一项时，就会得到一个用于 profile 的数据文件，通过 go tool profile 或者 go tool trace 等命令可以将其可视化。 Tip 也许你点击 heap 或者其他会发现浏览器打开了一个新的页面，这是浏览器直接将获取的数据展示了出来，但是其数据可读性很差。 你可以使用 curl 保存数据文件，或者直接通过 go tool profile \u003caddr\u003e/debug/profile/heap 来获取文件并解析。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:1:2","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.3 解析 通过 go tool pprof 可以解析 pprof 得到的数据文件。go tool pprof \u003cfile\u003e 执行后会进入一个交互式命令行，其中通过各个命令可以进行数据文件的解析与展示。 常用的命令包括： top \u003cN\u003e ：展示前 N 个数据最大的项； list \u003cpackge\u003e.\u003cfunc\u003e ：列出某个函数的指标； traces ：展示各个 goroutine 的调用栈，以及对应的指标； web ：通过浏览器打开解析后的图片； 1.3.1 heap 我们先来看一下 heap 项的数据文件，下载后通过 go tool pprof 的 web 命令打开： $ go tool pprof http://10.9.195.138:12300/debug/pprof/heap 弹出的浏览器的图片中，展示各个函数的申请的 heap 占用。其中，框越大就代表使用的越多，所以我们很快可以找到内存使用最多的函数。 1.3.2 alloc 对于 heap 项展示的是一瞬间的 heap 使用的快照。但是有一些情况从 heap 项的数据中看不出来，例如一直在快速申请无用内存，而导致 GC 频繁触发。这种情况虽然整体程序内存占用不大，但是是频繁 GC 之后的假象。 而 alloc 就是用于展示申请内存大小，这与 heap 展示当前占用内存大小是不一样的。 $ go tool pprof http://10.9.195.138:12300/debug/pprof/allocs 1.3.3 goroutine Go 自带内存回收，所以一般不会发生内存泄漏的情况。而大部分所说的内存泄漏，都是由于 goroutine 泄漏导致的 goroutine 结构占用内存过多。 如果直接点击浏览器的 goroutine 项中，会展示出所有 goroutine 的上下文。 但是这不利于分析整体的 goroutine 情况，还是通过 go tool profile 命令来看： $ go tool pprof http://10.9.195.138:12300/debug/pprof/goroutine 在解析后的图片中，可以看到各个函数创建的 goroutine 的数量，因此可以很快找到 goroutine 泄漏的情况（图片不是泄漏，只是展示一下）。 1.3.4 profile profile 项用于展示各个函数的 CPU 执行时间，当你发现程序 CPU 使用异常时，这项非常关键。 默认情况会程序在收到情况后，采集 30s 内的各个函数的 CPU 执行时间。你可以通过设置参数来调整该时间。 $ go tool pprof http://10.9.195.138:12300/debug/pprof/profile?seconds=30 一眼就可以看到执行时间最长的函数流： 1.3.5 trace 与前面几个不同，trace 可以用于跟踪程序的执行情况。go runtime 会上报特定的 trace 事件，而 trace 将其收集并展示出来。 $ curl http://10.9.195.138:12300/debug/pprof/trace\\?seconds\\=30 \u003e trace.out $ go tool trace trace.out 执行命令后，浏览器会弹出一个页面（必须是 Chrome），包含几个分析项，包括： 项目 作用 View trace 按照时间维度观察各个事件； Goroutine analysis 查看各个 G 的统计信息，包括执行时间，阻塞时间等； Network blocking profile 网络接口阻塞情况； Synchronization blocking profile：用户态阻塞情况，包括 select、chan 阻塞等； Syscall blocking profile 系统调用导致的阻塞情况，最常见的就是 IO 系统调用； Scheduler latency profile 各个函数的执行时间情况； User defined task User defined regions Minimum mutator utilization 展示出了 GC 外，应用能够获得的 CPU 资源的最小比例； (1) View trace 选择 “View trace”，会出现一个以时间为横轴的图片。 按住 W 放大后可以看到，其中每个 P 执行的 G 的时间段与事件（事件执行是个竖线）都会展示出来。各个 G 直接存在事件，点击 Flow events 后还会展示出事件流。 上面 G76 这个 Goroutine，经过了 P0 执行 -\u003e 系统调用阻塞 -\u003e P2 执行 点击其中一个段，可以看到该 G 的执行的统计信息。 Title：G 编号以及执行的函数 Start：启动时间； Wall Duration：执行时间； Start Stack Trace：G 开始执行的函数栈； End Stack Trace：G 切换前或者结束的函数栈； (2) Goroutine analysis 选择 “Goroutine analysis” 后，点击一个具体的 G，可以看到其执行时间的统计。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:1:3","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2 delve pprof 是程序整体的运行情况的数据，如果想对程序执行进行单步调试，就需要使用 delve。 delve 类似于 gdb，但是 gdb 是针对于线程的，无法识别 goroutine。而 delve 是专门用于 Go 程序的，可以做到针对不同 goroutine 打断点，单步执行等操作。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:2:0","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"Linux 网络收发包内核过程总结","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Linux 网络收发包一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:0:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"1 背景知识 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:1:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"1.1 中断 中断终止 CPU 执行流，立即执行必要的处理程序。分为两个类型： 同步中断和异常：由 CPU 自身产生。例如程序崩溃、缺页异常； 异步中断：由 IO 设备产生，任意时间可能发生； 两种中断发生后，CPU 会切换至内核态，并执行 中断处理程序interrupt handler。 对于异步中断，因为会停止当前执行的进程，所以内核要确保中断处理程序尽快完成，尽快返还 CPU。同样，也会阻塞 IO 设备的下一次中断，Linux 将异步中断分为了 硬件中断 与 中断下半部。 1.1.1 硬件中断 硬件中断hardware interrupt 指硬件设备发起信号中断 CPU 执行，CPU 立即进行中断的处理。 实际上，为了不长时间阻塞与中断处理，硬中断往往启动到的是一个通知的作用。例如网卡收到数据包，并通过 DMA 写入 ringbuffer 后，会通过硬中断通知 CPU 从 ringbuffer 读取并处理数据。 通过 /proc/interrupts 可以看到硬中断的触发次数： $ cat /proc/interrupts # … CPU0 100: 15 IR-PCI-MSI 35651584-edge p2p1-TxRx-0 101: 0 IR-PCI-MSI 35651585-edge p2p1-TxRx-1 102: 0 IR-PCI-MSI 35651586-edge p2p1-TxRx-2 103: 0 IR-PCI-MSI 35651587-edge p2p1-TxRx-3 104: 0 IR-PCI-MSI 35651588-edge p2p1-TxRx-4 105: 0 IR-PCI-MSI 35651589-edge p2p1-TxRx-5 106: 0 IR-PCI-MSI 35651590-edge p2p1-TxRx-6 第一行：IRQ 编号； 第二行：每个 CPU 的中断次数； 第三行：中断的类型； 第四行：？？； 第五行：中断发起的设备名； 平时可能只需要关注第五行设备名称就行，因为可能要过滤出网卡对应队列的中断，然后将其绑定触发的 CPU，见 网卡多队列。 1.1.2 中断下半部 中断下半部bottom half 包含三种处理方式： 软中断 softirq：固定的 32 个接口，只留给对时间要求最严格的下半部使用。 查看 /proc/softirqs 文件 可以看到目前支持的软中断： 命名 含义 HI TIMER 定时中断 NET_TX 网络发送 NET_RX 网络接收 BLOCK IRQ_POLL TASKLET tasklet 软中断扩展 SCHED 内核调度 HRTIMER RCU RCU 锁 tasklet：因为软中断只有固定的 32 个，为了支持扩展，tasklet 基于软中断时间，在不同处理器上运行，并且支持通过代码动态注册； 工作队列 work queue：将一个中断的部分工作推后，可以实现一些 tasklet 不能实现的工作（比如可以睡眠）。 一些内核线程会不断处理工作队列的数据，其运行在进程上下文中，并且可以睡眠以及被重新调度。目前，kworker 内核线程 负责处理这个工作。 对于软中断与 tasklet，如果大量出现时，为了不一直进行 CPU 中断，内核会唤醒 ksoftirqd 内核线程进行异步的处理。每个处理器有一个 ksoftirqd/n 线程，n 为 CPU 编号。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:1:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2 网卡层 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:2:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.1 网卡多队列 网卡与系统传输数据包通过两个环形队列：TX ring buffer、RX ring buffer，也称为 DMA 环形队列。平时所说的设置网卡多队列指的就是设置这个环形队列的数量。 当网卡收到帧时，会通过哈希来决定将帧放在哪个 ring buffer 上，然后通过硬中断通知其对应的 CPU 处理。 默认下，处理环形队列数据由 CPU0 负责，可以通过配置 中断亲和性，或者通过开启 irqbalance service 将中断均衡到各个 CPU 上。 2.1.1 配置网卡队列 通过 ethool -l/-L \u003cnic\u003e 命令查看与配置网卡的队列数，通常配置的与机器 CPU 个数一样（如果网卡支持的话）： $ ethtool -l eth0 Channel parameters for eth0: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 8 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 7 $ ethtool -L eth0 combined 8 设置后，你在 /sys/class/net/\u003cnic\u003e/queues/ 可以看到对应的收发队列目录： $ ls /sys/class/net/eth0/queues/ rx-0 rx-1 rx-2 rx-3 rx-4 rx-5 rx-6 rx-7 tx-0 tx-1 tx-2 tx-3 tx-4 tx-5 tx-6 tx-7 在 /proc/interrupts 中也可以看到各个队列对各个 CPU 的中断次数： $ cat /proc/interrupts | grep eth0 # … 这里只打印了一个 CPU 中断 CPU0 62: 0 IR-PCI-MSI 524288-edge eth0 63: 0 IR-PCI-MSI 524289-edge eth0-TxRx-0 64: 0 IR-PCI-MSI 524290-edge eth0-TxRx-1 65: 1766 IR-PCI-MSI 524291-edge eth0-TxRx-2 66: 0 IR-PCI-MSI 524292-edge eth0-TxRx-3 67: 0 IR-PCI-MSI 524293-edge eth0-TxRx-4 68: 0 IR-PCI-MSI 524294-edge eth0-TxRx-5 69: 0 IR-PCI-MSI 524295-edge eth0-TxRx-6 70: 0 IR-PCI-MSI 524296-edge eth0-TxRx-7 通过 ethtool -g/-G \u003cnic\u003e 可以查看与配置 ring buffer 的长度： $ ethtool -g eth0 Ring parameters for eth0: Pre-set maximums: RX: 4096 RX Mini: 0 RX Jumbo: 0 TX: 4096 Current hardware settings: RX: 256 RX Mini: 0 RX Jumbo: 0 TX: 256 2.1.2 配置中断亲和性 如果你发现 CPU0 的中断很高，那么就很有可能所有网卡队列的中断都打到了 CPU0 上。可以通过 /proc/irq/\u003cid\u003e/smp_affinity_list 查看指定编号的中断的对应允许的 CPU。 $ for i in {62..70}; do echo -n \"Interrupt $iis allowed on CPUs \"; cat /proc/irq/$i/smp_affinity_list; done Interrupt 62 is allowed on CPUs 4 Interrupt 63 is allowed on CPUs 8 Interrupt 64 is allowed on CPUs 13 Interrupt 65 is allowed on CPUs 21 Interrupt 66 is allowed on CPUs 6 Interrupt 67 is allowed on CPUs 31 Interrupt 68 is allowed on CPUs 5 Interrupt 69 is allowed on CPUs 2 Interrupt 70 is allowed on CPUs 31 62-70 的网卡队列中断都打到了不同的 CPU 上； 通过写入 echo \"\u003cbitmark\u003e\" \u003e /proc/irq/\u003cid\u003e/smp_affinity 可以中断绑定的 CPU，bitmark 的每个位对应一个 CPU，例如 “0x1111” 表示 CPU 0-3 都可以处理这个中断。 当然，你也可以通过 irqbalance 来均衡各个 CPU 的中断，动态的改变中断与绑定的 CPU。不过，irqbalance 不仅仅针对网卡队列中断，还会调整其他的。 如果你的网卡不支持多队列，可以尝试配置 RPS。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:2:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.2 接收数据 先来看第一个阶段，网卡接收到数据是如何处理的。 packet 进入物理网卡，物理网卡会根据目的 mac 判断是否丢弃（除非混杂模式）； 网卡通过 DMA 方式将 packet 写入到 ringbuffer ringbuffer 由网卡驱动程序分配并初始化。 网卡通过硬中断通知 CPU，有数据来了。 CPU 根据中断执行中断处理函数，该函数会调用网卡驱动函数。 驱动程序先禁用网卡中断（NAPI），表示网卡下次直接写到 ringbuffer 即可，不需要中断通知了。 这样避免 CPU 不断被中断。 驱动程序启动软中断，让 CPU 执行软中断处理函数不断从 ringbuffer 读取并处理 packet。 网卡多队列就是在这里生效，网卡会将 packet 放置到不同的 ringbuffer，不同的 ringbuffer 会中断不同的 CPU（如果设置了中断亲和性），使得各个 CPU 的队列硬中断是均衡的。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:2:2","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.3 发送数据 网络设备通过驱动函数发送数据后，就归网卡驱动管了，不同的驱动有着不同的处理方式。 大致的流程如下： 将 sk_buff 放入 TX ringbuff。 通知网卡发送数据。 网卡发送完数据后，通过中断通知 CPU。 收到中断后，进行 sk_buff 的清理工作。 当然，网卡驱动还有一些与 net_device 打交道的地方，比如网卡的队列满了，需要告诉上层不要再发了，等队列有空闲的时候，再通知上层接着发数据。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:2:3","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3 网络访问层 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.1 net_device 每个网络设备都表示为 net_device 一个实例。不同类型的网络设备都会有 net_device 表示，而其相关操作函数的实现不同。 net_device 是针对于 namespace 的，通过 sysfs 你可以看到当前命名空间下所有的 net_device。 $ ls /sys/class/net docker0 enp0s3 enp0s8 lo lxcbr0 vethea62395 net_device 包含了设备相关的所有信息，定义很长， 下面经过简化： struct net_device { char name[IFNAMSIZ]; // IO 相关字段 unsigned long mem_end; unsigned long mem_start; unsigned long base_addr; int irq; unsigned long state; struct list_head dev_list; int ifindex; const struct net_device_ops *netdev_ops; const struct ethtool_ops *ethtool_ops; unsigned int flags; unsigned int mtu; unsigned short type; unsigned short hard_header_len; unsigned char perm_addr[MAX_ADDR_LEN]; unsigned char addr_len; #if IS_ENABLED(CONFIG_VLAN_8021Q) struct vlan_info __rcu *vlan_info; #endif // … 协议相关的特殊指针 /* Interface address info used in eth_type_trans() */ unsigned char *dev_addr; struct netdev_rx_queue *_rx; unsigned int num_rx_queues; unsigned int real_num_rx_queues; struct bpf_prog __rcu *xdp_prog; unsigned long gro_flush_timeout; int napi_defer_hard_irqs; rx_handler_func_t __rcu *rx_handler; void __rcu *rx_handler_data; struct netdev_queue __rcu *ingress_queue; truct netdev_queue *_tx； unsigned int num_tx_queues; unsigned int real_num_tx_queues; struct Qdisc *qdisc; unsigned int tx_queue_len; }; name[IFNAMSIZ] ：设备命名； irq ：irq 编号； ifindex ：设备编号； mtu ：设备的最大传输单元； netdev_ops ：设备的操作接口，不同类型的接口有着不同的操作实现； dev_addr ：网卡硬件地址； _rx ：数据包接收队列（ringbuffer）； _tx ：数据包发送队列； qdisc : 设备进入的 qdisc； ingress_queue ：ingress 队列； 其他包含一些 XDP 相关，统计相关等字段； 大多数的统计信息都可以在对应设备的 sysfs 目录中找到。 网卡命名 linux 内核启动过程中，会默认给网卡以 ethx 方式命名，后面 systemd 回去 rename 网卡名称。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.2 GRO GROGeneric Receive Offloading 用于将 jumbo frame（超过 1500B）的多个分片合并，然后将给上层处理，以减少上层处理数据包的数量。 通过 ethtool -k/K \u003cnic\u003e 查和设置 GRO。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:2","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.3 RPS XPS RFS 3.3.1 RPS 网卡多队列需要硬件的支持，而 RPSReceive Packet Steering 这些则是软件实现多队列，将包让指定的 CPU 去处理。通常情况，当网卡队列数小于 CPU 个数时，可以让 RPS 进一步利用多余的 CPU，让其去处理中断。 Note RPS 的设置是针对单个 ringbuffer 的，与网卡多队列处理不是同一个阶段，也不是冲突的。 开启 RPS 后，数据会由经由被中断 CPU 转发，由其他 CPU 处理。流程如下： 当网卡收到数据存入 ring buffer 后，还是通知指定 CPUx 从 ringbuffer 取出数据； 不过接下来，CPUx 会为每个 packet 哈希放入其他 CPU 的 input_pkt_queue 中。 CPUx 通过 Inter-processor Interrupt (IPI) 中断告知其他 CPU，处理自己的 input_pkt_queue ； 其他 CPU 从各个的 input_pkt_queue 中取出数据包，并处理之后的流程； 可以看到，RPS 不是用于减少 CPU 软中断的次数，而是用于将数据包处理时间均摊到各个 CPU 上，也就是减少单个 CPU 的软中断执行时间（%soft）。 # 配置该 ringbuffer 使用 CPU0 CPU1 的队列 $ echo \"0x11\" \u003e /sys/class/net/eth0/queues/rx-0/rps_cpus # 配置每个 CPU 的 input_pkt_queue 的大小 $ sysctl -w net.core.netdev_max_backlog=1000 3.3.2 RFS RFSReceive Flow Steering 一般和 RPS 配合工作。 RPS 将受到的 packet 经过哈希发配到不同的 CPU input_pkt_queue。而 RFS 会根据 packet 的数据流，发送到对应被处理的 CPU input_pkt_queue 上，即同一个数据流的 packet 会被路由到处理当前数据流的 CPU 上，从而提高 CPU cache 的命中率。 RFS 默认是关闭的，需要通过配置生效，一般推荐的配置如下： # 配置系统期望的活跃连接数 $ sysctl -w net.core.rps_sock_flow_entries=32768 # 配置每个 rps 队列负责的 flow 最大数量，为 rps_sock_flow_entries / N $ echo 2048 \u003e /sys/class/net/eth0/queues/rx-0/rps_flow_cnt 3.3.3 XPS XPSTransmit Packet Steering 则是发送时的多队列处理。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:3","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.4 接收数据 在网卡层中，最后由异步的软中断处理函数来异步的从里 ringbuffer 的数据。而这由内核线程 ksoftirqd 来调用对应的网络软中断函数处理。 所以，在数据包到达 socket buffer 前，数据处理都是由 ksoftirqd 线程执行的，也就是算在软中断处理时间里的。 ksoftirqd 调用驱动程序的 poll 函数来一个个处理 packet。 如果没有 packet 的话，就会重新启动网卡硬中断，等待下一次重新的流程。 poll 函数将读取的每个 packet，转换为 sk_buff 数据格式，并分析其传输层协议。 （可选）如果开启了 GRO，那么进行 GRO 的处理。 如果开启了 RPS，那么进行 RPS 的处理，否则放入当前 CPU 的 input_pkt_queue。 RPS 处理的流程如下： 当前 CPU 将 sk_buff 放到其他 CPU 的 input_pkt_queue 中。如果 input_pkt_queue 满的话，packet 会被丢弃。 CPU 通过 IPI 硬中断通知其他 CPU 处理自己的 input_pkt_queue，也就是走 11 步流程。 到这里，而无论是否开启 RPS，接下来就是 CPU 从各自 input_pkt_queue 取出 sk_buff 并处理。 CPU 查看 socket 是否是 AF_PACKET 类型的。如果是的话复制一份数据处理（例如 tcpdump 抓这里的包）。 调用对应协议栈的函数，将数据包解析出网络层协议，并交给对应的协议栈处理。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:4","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.5 发送数据 接受到网络层的数据后，来到网络访问层会经过一个非常重要的系统：Traffic Controller。这是接收数据时不会经过。 根据 sk_buff 中的设备信息，获取对应 net_device.qdisc。 如果 qdisc 存在的话，走流量控制系统，可能会丢弃包： TODO 拷贝一份 sk_buff 给 “packet taps”。 调用具体驱动的发送数据的函数发送。 Note 注意，发送数据时并没有 CPU 的 “outputqueue”，因为流量控制系统已经进行了流量控制。 入口流量控制 如果想对入口流量进行流量控制，可以使用 tc ingressqdisc + 虚拟设备 IFB 进行 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:5","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4 网络层 网络访问层在接受到数据后，调用各个网络层协议的处理函数，进行 sk_buff 的处理。当然，我们下面说的是 IP 协议。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.1 sk_buff 网卡接受到的数据包，整个在内核中传递使用的都是 sk_buff 数据结构。网络的各个层都是使用的同一个 sk_buff 对象，而无需进行数据的复制，使得性能更高。 sk_buff 包含各个指针执行对应数据的内存区域，并且表示其协议的 head data 等区域。 // \u003csk_buff.h\u003e struct sk_buff { // ... union { __be16 inner_protocol; __u8 inner_ipproto; }; __u16 inner_transport_header; __u16 inner_network_header; __u16 inner_mac_header; __be16 protocol; __u16 transport_header; __u16 network_header; __u16 mac_header; /* These elements must be at the end, see alloc_skb() for details. */ sk_buff_data_t tail; sk_buff_data_t end; unsigned char *head, *data; }; head，end 为整个数据包的头尾内存地址； data，tail 为当前层对应的数据的头尾内存地址； transport_header，network_header，mac_header 传输层、网络层、链路层 header 的内存地址； 看图可能更好理解，sk_buff 通过指针将数据包各个区域表示出来了，而在各个协议层之间移动则是移动 data 与 tail 指针。 sk_buff_head 表示 sk_buff 组成的链表，这个结构就是 RPS 各个 CPU 的队列，以及 socket buffer 的实现。 // \u003csk_buff.h\u003e struct sk_buff_head { /* These two members must be first. */ struct sk_buff *next; struct sk_buff *prev; __u32 qlen; // 链表长度 spinlock_t lock; }; ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.2 netfilter netfilter 是一个在内核框架，位于网络层，可以根据动态的条件过滤或操作分组。 主要包含如下功能： filter：根据分组元信息，对不同数据流进行分组过滤； NAT：根据规则来转换 source ip 或者 destination ip； mangle: 根据特定分组拆分与修改； iptables iptables 是用于提供给用户配置防火墙、分组过滤等功能，使用的就是 netfilter 框架。 针对不同的阶段，内核代码中会存在不同的 hook 点，如下图： ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:2","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.3 接收数据 接受数据到这里，数据包的网络层协议已经解析过了，看一下网络层处理数据报的步骤： 如果其数据包的 MAC 地址不是当前网卡，那么丢弃（可能由于网卡混杂模式进来的，还是无法经过协议栈处理）。 经过 netfilter.PREROUTING 阶段回调。 进行路由判断：如果目的 IP 是本机 IP，那么接受该包。如果不是本机 IP，判断是否要路由。 经过 netfilter.LOCALIN 阶段回调。 进入传输层。 可以看到，如果仅仅是简单的接收数据包很简单，只要经过 netfilter 的回调即可。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:3","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.4 路由数据 当接受数据时发现数据包不是发往本机时，就会判断是否需要进行路由。 路由的前提是：机器开启了 ip forward 功能，否则会直接丢包。 # 开启 ip forward sysctl -w net.ipv4.ip_forward=1 看一下路由对数据包的处理： 检查是否开启 ip forward 功能，没有开启的话数据包会执行丢弃。 经过 netfilter.FORWARD 阶段回调。 走正常的发包流程发送数据包（会经过 netfilter.POSTROUTING 阶段回调） ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:4","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.5 发送数据 在 sk_buff 指向的数据区设置好 IP 报文头。 调用 netfilter.LOCALOUT 阶段回调。 调用相关协议的发送函数（IP 协议或其他），将出口网卡设备信息写入 sk_buff。 调用 netfilter.POSTOUTPUT 阶段回调。 POSTOUTPUT 可能设置了 SNAT，从而导致路由信息变化，如果发生变化重新重新回到 3。 根据目的 IP，从路由表中获取下一跳的地址，然后在 ARP 缓存表中找到下一跳的 neigh 信息。 如果没有 neigh 信息，那么会进行一个 ARP 请求，尝试得到下一跳的 mac 地址。 到这里，得到了下一跳设备的 mac 地址，将其填入 sk_buff 并调用下一层的接口。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:5","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"5 传输层 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:5:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"5.1 sock sock 是 socket 在内核的表示结构，每个 sock 对应于一个用户态使用的 socket。TCP UDP 都是基于该结构来实现的。 其中，sock 最主要的属性就是常说的 socket buffer 了。 // \u003csock.h\u003e struct sock { // … #define sk_state __sk_common.skc_state // … struct sk_buff_head sk_error_queue; struct sk_buff_head sk_receive_queue; struct sk_buff_head sk_write_queue; int sk_rcvbuf; int sk_sndbuf; u32 sk_ack_backlog; u32 sk_max_ack_backlog; // 各个回调函数 void (*sk_state_change)(struct sock *sk); void (*sk_data_ready)(struct sock *sk); void (*sk_write_space)(struct sock *sk); void (*sk_error_report)(struct sock *sk); int (*sk_backlog_rcv)(struct sock *sk, struct sk_buff *skb); // … } sk_state ：TCP 的状态； sk_receive_queue sk_write_queue ：接受/发送队列（buffer）； rcvbuf，sndbuf ：接受/发送队列的大小，单位 B； sk_ack_backlog ：经过三次握手后，等待 accept() 的全连接队列； 5.1.1 配置 socket buffer 大小 通过 sysctl 可以配置 TCP、UDP 的接受与发送缓冲区大小： sysctl -w net.ipv4.tcp_rmem=\"4096 503827 6291456\" sysctl -w net.ipv4.tcp_wmem=\"4096 503827 6291456\" sysctl -w net.ipv4.udp_mem=\"377868 503827 755736\" 每个设置包含三个值，min、default、max。内核会根据当前的可用内存动态调节队列的大小； Tip 使用 SO_SNDBUF/SO_RCVBUF 可以针对 socket 单独设置 r/w buffer。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:5:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"5.2 UDP 层 5.2.1 接收数据 先从简单的 UDP 处理开始看： 对数据包进行一致性检查。 根据目的 IP 与目的 port，在 udptable（包含机器所有的 udp sock）查找对应的 sock。没有找到则会丢弃数据包，否则继续。 检查 receive buffer 是否满，如果满了则会直接丢弃数据包。 检查数据包是否满足 BPF socket filter，如果不满足则直接丢弃数据包。 将数据包放入 receive buffer。 调用回调函数，以通知数据包已经准备好。这会将阻塞等待数据包到来的用户态程序唤醒。 UDP 的处理很简单，找到对应的 sock 结构，然后将其放入到队列中，唤醒用户态程序。 5.2.2 发送数据 发送操作与接收不是对称的，因为在发送时就要确定出口的设备，来确认包是否会被直接丢弃。 根据机器的路由表和目的 IP，决定数据包应该从哪个设备发送出去。如果根据路由表无法到达目的地址，直接丢弃包。 如果 socket 没有绑定源 IP，那么就使用设备的源 IP。 根据获取到路由信息，将 msg 构建为 sk_buff 结构体。 向 sk_buff 的数据区填充 UDP 包头，然后调用 IP 层相关函数。 Note UDP 不存在 send buffer，数据直接会发送出去，因为 UDP 没有拥塞控制。 设置 socket SO_SNDBUF 选项时，对于 UDP 这是每次发送数据的最大值，超过的话发送会直接返回 ENOBUFS。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:5:2","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"总结 整个内核网络收发是个很复杂的模块，可能好多地方的细节都没有涉及，也有可能有理解错误的地方。 所幸内核实现层次分明的比较清楚，因此可以一层层观察其对应负责的行为。 在一个普通程序员的角度下，首先需要理解网络包收发涉及到的各个层的作用：网卡层、网络访问层、网络层、传输层。 对于各个层，需要知道一些包处理的关键点的所处于的位置，包括：网卡多队列、流量控制、netfilter、r/w buffer 等； 目前整理的主要的点如下： 网卡层 网卡多队列的概念与位置； CPU 如何接受网卡数据； CPU 如何发送网卡数据； 网络访问层 网络访问层接受数据； 网络访问层发送数据； ksoftiqrd 线程的任务； RPS、XPS、RFS 的概念； TC 流量控制处理的位置； XDP 处理的位置； 网络层 sk_buff 结构的概念； 网络层接受数据； 网络层发送数据； netfilter 各个阶段回调的位置； 如何进行路由判断； 传输层 sock 结构的概念； UDP 的 recv socket buffer； TCP 的半连接队列，全连接队列，r/w socket buffer； ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:6:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"参考 Blog: Linux 网络 - 数据包的接收过程 Blog: Linux 网络 - 数据包的发送过程 《深入 Linux 内核架构》 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:7:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"Go 协程实现，GMP 模型实现，调度算法实现","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Golang 并发调度实现的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 下面代码都是基于 go 1.15.6。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:0:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1 背景知识 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:1:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.1 进程、线程与协程 看协程的实现之前，绕不开的需要知道进程与线程，以及它们之间的比较。 进程progress 就是程序执行的实例。在内核角度上看，进程是分配系统资源的实体。 而这也可以推出，所谓的进程切换的消耗，也就是切换分配的系统资源，包括虚拟内存（页表等）、寄存器的值等。 Tip 据说每次上下文切换需要几十纳秒到微秒的 CPU 时间。 线程thread 是内核调度的基本单元，在 Linux 上，线程就是 “轻量级进程”，因为它与进程在内核的看来都一样，仅仅是共享了一些资源，包括： 内存地址空间； 进程的基础信息； 打开的文件描述符； 信号处理； 等等 所以相对于进程间切换，同一个进程下的线程切换，可以省略这些共享的资源的切换。因此，线程间切换速度快于进程间切换。 Note 所谓进程切换就是不同进程间的线程切换，所以下面所说的线程间的切换、线程上下文都是指同进程下的线程。 上面所说的调度、切换都是站在内核的角度看线程。而站在线程的角度，它可以认为自己是 “完全” 占用 CPU 的。如果线程阻塞等待，就等于 CPU 在 空闲 浪费。 所以，写代码往往会使用异步 API，通过回调/通知来使得线程阻塞的更加少（典型的 epoll），这时候 CPU 原来阻塞的时间可以执行其他的代码。 但是，回调的代码不是同一个函数里线性执行的，会有一些缺点，具体例子，A 函数最初的执行流为： 执行 -\u003e 读取数据 -\u003e 执行 如果使用异步 API，其执行流程变为： A 函数：执行 -\u003e 异步 -\u003e 返回 回调函数：读取数据 -\u003e 执行。 首先，单个函数的执行流变为了两个不同函数，代码写法上串行的思维要变为分割的思维。并且，如果这个操作还是有状态的，那么还涉及到了 “A 函数将状态传递到回调函数” 等问题。 因此，协程routine 出现，上面的例子的执行流还是： 执行 -\u003e 读取数据 -\u003e 执行 但是，在读取数据这一步，协程会主动让出 CPU，等待数据到来时再次切换到该协程，继续执行。因此，写法不变，功能相同。 并且，在一些用户空间的生产消费模型实现上（channel），协程的阻塞不需要线程阻塞，而在用户空间就完成了协程的切换。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:1:1","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.2 调度器 调度器scheduler 是为了在决定在有限的 CPU 上，选择哪些任务（进程/线程/协程）执行使得系统运行更高效，所以主要有两个工作： 决定为各个任务决定其运行多长时间，以及何时切换到下个任务执行； 在切换任务 A 至任务 B 时，必须保存任务 A 的运行环境，并且任务 B 的运行环境与上次其被切换时完全相同； 第 1 个工作涉及到的就是 调度算法，如何调度使得系统运行的最高效。 第 2 个工作涉及到的就是 上下文切换，这里再说一下进程、线程、协程的上下文： 进程：虚拟内存、寄存器的值、内核对应进程信息等；（内核完成上下文切换） 线程：寄存器的值、内核对应的进程信息等；（内核完成上下文切换） 协程：寄存器的值、用户空间对应的协程信息等；（用户空闲 runtime 完成协程上下文切换） Note 上面的切换没有说栈，因为栈的切换就是寄存器的切换。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:1:2","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.3 PC 与 SP 这一块在 内存管理总结 说过，因为对协程切换很重要，这个再次复制一下。 1.3.1 PC 程序计数器 PCProgram Counter 是 CPU 中的一个寄存器，保存着下一个 CPU 执行的指令的位置。顺序执行指令时，PC = PC + 1（一个指令）。而调用函数或者条件跳转时，会将跳到的指令地址设置到 PC 中。 所以，可以想到，当需要切换执行的 goroutine，调用 JMP 指令跳转到 G 对应的代码。 1.3.2 SP 栈顶指针 SPstack pointer 是保存栈顶地址的寄存器，我们平时所说的临时变量在栈上，就是将临时变量的值写入 SP 保存的内存地址，然后 SP 保存的地址减小（栈是从高地址向低地址变化），然后临时变量销毁时，SP 地址又变为高地址。 不过，因为 goroutine 切换时，必须要保存当前 goroutine 的上下文，也就是栈里的变量。因此，goroutine 栈肯定是不能使用 Linux 进程栈了（因为进程栈有上限，也无法实现“保存”这种功能）。所以所说的协程栈，都是基于 mmap 申请内存空间（基于 Go 内存管理，内存管理基于 mmap），然后切换时修改 SP 寄存器地址实现的。 这也是为什么 goroutine 栈可以“无限大”的原因了。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:1:3","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2 GMP 模型 先从整体模型入手，整个协程实现有着三个最主要的对象： G ：表示 Goroutine，保存并发任务的状态（上下文）； M ：表示系统线程，必须在绑定 P 后，执行 G 任务； P ：处理器，作用类似于 CPU 核，控制并发执行任务数量； schedt ：全局的链表，包括全局的 G 可运行链表、空闲 M 链表、空闲 P 链表； 而大致的运行的模型如下： 每个 P 绑定一个 M，与一个正在运行的 G； 每个 P 包含自己本地的可运行的 G 的链表； 全局的 G 的可运行链表，用以提供给无 G 可以执行的 P； 一些 M 与 G 的绑定，因为系统调用阻塞而解绑了 P，M 阻塞结束后还是会进入调度循环； 一些 G 主动进入 waiting 状态，等待唤醒后重新加入某个可运行队列（典型的，读取 channel 阻塞，这是一个用户空间的阻塞，由发送 channel 时将其唤醒）； 空闲 M 链表，空闲 P 链表，空闲 G 链表等； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.1 G g 结构（runtime/runtime2.go）代表一个 G，包含了某个任务的上下文，M 切换 G 执行时，当前 G 的上下文就保存在了 g 结构中。 type g struct { // Stack parameters.  // stack describes the actual stack memory: [stack.lo, stack.hi).  // stackguard0 is the stack pointer compared in the Go stack growth prologue.  // It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.  // stackguard1 is the stack pointer compared in the C stack growth prologue.  // It is stack.lo+StackGuard on g0 and gsignal stacks.  // It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).  stack stack // offset known to runtime/cgo  stackguard0 uintptr // offset known to liblink  stackguard1 uintptr // offset known to liblink _panic *_panic // innermost panic - offset known to liblink  _defer *_defer // innermost defer  m *m // current m; offset known to arm liblink  sched gobuf atomicstatus uint32 goid int64 preempt bool // preemption signal, duplicates stackguard0 = stackpreempt  preemptStop bool // transition to _Gpreempted on preemption; otherwise, just deschedule  preemptShrink bool // shrink stack at synchronous safe point lockedm muintptr … } stack：G 对应栈空间的地址，见（内存管理模型-Goroutine 的栈）； stackguar0：扩容栈的地址，也可以用于判断 G 是否应该被抢占；当 stackguard0 == stackpreempt 就表明当前 G 被抢占了； _panic：G 下的 panic 链表； _defer：G 下的所有 defer 组成的链表； m：当前绑定的 M，为 nil 就表示当前 G 没有在执行； sched：G 的部分上下文，会提供给汇编代码； atomicstatus：G 的状态； goid：G 的唯一 ID，但是用户代码无法读取； preempt：抢占标志； lockedm：记录 G 被锁定的 M，实现 runtime.LockOSThread() 其中 g.sched 是一个非常重要的结构，需要看一下其 gobuf 的实现（runtime/runtime2.go）： type gobuf struct { // The offsets of sp, pc, and g are known to (hard-coded in) libmach.  //  // ctxt is unusual with respect to GC: it may be a  // heap-allocated funcval, so GC needs to track it, but it  // needs to be set and cleared from assembly, where it's  // difficult to have write barriers. However, ctxt is really a  // saved, live register, and we only ever exchange it between  // the real register and the gobuf. Hence, we treat it as a  // root during stack scanning, which means assembly that saves  // and restores it doesn't need write barriers. It's still  // typed as a pointer so that any other writes from Go get  // write barriers.  sp uintptr pc uintptr g guintptr ctxt unsafe.Pointer ret sys.Uintreg … } sp：当前 G 的 SP 指针地址，在创建 G 时默认设置为 goexit 函数地址； pc：当前 G 的 PC 指针地址； g：gobuf 所属的 g 结构地址； ret：系统调用的返回值； 2.1.1 G 的状态 目前 G 可能处于以下 9 种状态： 状态 值 含义 _Gidle 0 G 刚分配，并且还没有被初始化 _Grunnable 1 G 处于可运行队列中，但是并没有被执行 _Grunning 2 G 绑定了 M、P，不在可运行队列，并且可能正在执行 _Gsyscall 3 G 正在执行系统调用而阻塞，不在可运行队列上，绑定了 M，没有绑定 P _Gwaiting 4 G 因为用户空间而阻塞，不在可运行队列，等待其他 G 唤醒，没有绑定 M、P _Gdead 5 G 当前没有被使用，其 g 结构可以被复用 _Gcopystack 6 G 的栈正在被拷贝，没有执行，不再可运行队列 _Gpreempted 7 由于抢占而被阻塞，没有执行，等待唤醒 _Gscan 8 GC 正在扫描 G 的栈，可与其他状态同时存在 其状态轮转如下图所示： 2.1.2 G 的创建 在代码中调用 go 语句时，编译器会将其翻译为 newproc() 调用，这也就是创建 G 的开端： func newproc(siz int32, fn *funcval) { argp := add(unsafe.Pointer(\u0026fn), sys.PtrSize) gp := getg() pc := getcallerpc() systemstack(func() { // 创建 g 结构  newg := newproc1(fn, argp, siz, gp, pc) // 放入当前 P 或者全局可运行队列  _p_ := getg().m.p.ptr() runqput(_p_, newg, true) if mainStarted { wakep() } }) } // Create a new g in state _Grunnable, starting at fn, with narg bytes // of arguments starting at argp. callerpc is the address of the go // statement that created this. The caller is responsible for adding // the new g to the scheduler. // // This must run on the system stack because it's the continuation of // newproc, which cannot split the stack. // //go:systemstack func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g { // 调用 go 语句的 G  _g_ := getg() siz := narg siz = (siz + 7) \u0026^ 7 // 从 P.gFree 获取一个可复用的 g 对象  _p_ := _g_.m.p.ptr() newg := gfget(_p_) // 没有可复用的，重新分配一个 g 对象  if newg == nil { newg = malg(_StackMin) casgstatus(newg, _Gidle, _Gdead) allgadd(newg) // publishes with a g-\u003estatus of Gdead so GC scanner doesn't look at uninitialized stack.  } totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:1","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.2 M M 代表的就是一个操作系统线程，由 m 结构 表示（runtime/runtime2.go）： type m struct { g0 *g // goroutine with scheduling stack  gsignal *g // signal-handling g  goSigStack gsignalStack // Go-allocated signal handling stack  curg *g // current running goroutine  p puintptr // attached p for executing go code (nil if not executing go code)  nextp puintptr oldp puintptr // the p that was attached before executing a syscall  id int64 lockedg guintptr lockedExt uint32 // tracking for external LockOSThread  lockedInt uint32 // tracking for internal lockOSThread } g0 : M 私有的 g0； gsignal ：用于处理操作系统信号的 G； curg ：当前 M 正在执行的 G； p : 当前 M 绑定的 P； nextp ：暂存的 nextp； oldp ：M 陷入系统调用前绑定的 P，用于系统调用结束后尝试重新绑定； id ：m 的 ID； lockedg ：保存 M 锁定的 G，实现 runtime.LockOSThread()； 2.2.1 M 的创建 在创建 G 或者其他地方，当 G 变为 runnable 后，就会调用 wakep() 触发一次 P 执行 G 的过程。其会调用 startm() 选择/创建 一个 M 绑定 P，并执行一个 G。 // Tries to add one more P to execute G's. // Called when a G is made runnable (newproc, ready). func wakep() { if atomic.Load(\u0026sched.npidle) == 0 { return } startm(nil, true) } // Schedules some M to run the p (creates an M if necessary). // If p==nil, tries to get an idle P, if no idle P's does nothing. // May run with m.p==nil, so write barriers are not allowed. // If spinning is set, the caller has incremented nmspinning and startm will // either decrement nmspinning or set m.spinning in the newly started M. //go:nowritebarrierrec func startm(_p_ *p, spinning bool) { // 选择一个空闲的 P  lock(\u0026sched.lock) if _p_ == nil { _p_ = pidleget() if _p_ == nil { unlock(\u0026sched.lock) return } } // 选择一个空闲的 M  mp := mget() if mp == nil { // No M is available, we must drop sched.lock and call newm.  // However, we already own a P to assign to the M.  //  // Once sched.lock is released, another G (e.g., in a syscall),  // could find no idle P while checkdead finds a runnable G but  // no running M's because this new M hasn't started yet, thus  // throwing in an apparent deadlock.  //  // Avoid this situation by pre-allocating the ID for the new M,  // thus marking it as 'running' before we drop sched.lock. This  // new M will eventually run the scheduler to execute any  // queued G's.  id := mReserveID() unlock(\u0026sched.lock) var fn func() if spinning { // The caller incremented nmspinning, so set m.spinning in the new M.  fn = mspinning } // 创建一个 M  newm(fn, _p_, id) return } unlock(\u0026sched.lock) // The caller incremented nmspinning, so set m.spinning in the new M.  mp.spinning = spinning mp.nextp.set(_p_) notewakeup(\u0026mp.park) } 当存在空闲的 P，但是没有空闲的 M 时，就会调用 newm() 创建一个 M； newm() 就是创建 m 结构，以及启动系统线程的地方（runtime/proc.go）： // Create a new m. It will start off with a call to fn, or else the scheduler. // fn needs to be static and not a heap allocated closure. // May run with m.p==nil, so write barriers are not allowed. // // id is optional pre-allocated m ID. Omit by passing -1. //go:nowritebarrierrec func newm(fn func(), _p_ *p, id int64) { // 分配 m 对象（复用 sched.freem 或者新创建）  mp := allocm(_p_, fn, id) mp.nextp.set(_p_) mp.sigmask = initSigmask … newm1(mp) } func newm1(mp *m) { … execLock.rlock() // Prevent process clone.  newosproc(mp) execLock.runlock() } // May run with m.p==nil, so write barriers are not allowed. //go:nowritebarrier func newosproc(mp *m) { stk := unsafe.Pointer(mp.g0.stack.hi) // 给 thread 初始的栈来自于 g0  // Disable signals during clone, so that the new thread starts  // with signals disabled. It will enable them in minit.  var oset sigset sigprocmask(_SIG_SETMASK, \u0026sigset_all, \u0026oset) ret := clone(cloneFlags, stk, unsafe.Pointer(mp), unsafe.Pointer(mp.g0), unsafe.Pointer(funcPC(mstart))) sigprocmask(_SIG_SETMASK, \u0026oset, nil) if ret \u003c 0 { throw(\"newosproc\") } } 获取 m 对象，来自于 sched.freem 或者新创建一个； 通过 clone() 系统调用创建一个系统线程，执行的函数为 mstart()； 看一下 clone 的 flags： var ( cloneFlags = _CLONE_VM | /* share memory */ _CLONE_FS | /* share cwd, etc */ _CLONE_FILES | /* share fd table */ _CLONE_SIGHAND | /* share sig handler table */ _CLONE_SYSVSEM | /* share SysV semaphore undo lists (see issue #20763) */ _CLONE_THREAD /* revis","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:2","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.3 P P 是 M 与 G 的中间层，没有了 P，M 与 G 实际上就是一个线程池。通过 P 来分片所有可运行的 G，使得运行效率更加的高。 p 是 P 的结构体表示（runtime/runtime2.go）： type p struct { id int32 status uint32 // one of pidle/prunning/...  m muintptr // back-link to associated m (nil if idle)  mcache *mcache pcache pageCache // Queue of runnable goroutines. Accessed without lock.  runqhead uint32 runqtail uint32 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready'd by  // the current G and should be run next instead of what's in  // runq if there's time remaining in the running G's time  // slice. It will inherit the time left in the current time  // slice. If a set of goroutines is locked in a  // communicate-and-wait pattern, this schedules that set as a  // unit and eliminates the (potentially large) scheduling  // latency that otherwise arises from adding the ready'd  // goroutines to the end of the run queue.  runnext guintptr // Available G's (status == Gdead)  gFree struct { gList n int32 } sudogcache []*sudog sudogbuf [128]*sudog } status ：P 的状态； m ：当前绑定的 M； mcache ：P 唯一的 mcache，将【内存管理】； runqhead ：P 的可运行队列的 head index； runqtail ：P 的可以行队列的 tail index； runq ：P 的可运行队列，可以看到大小为 256； runnext ：下一次优先执行的 G，优先级高于 runq； gFree ：可复用的 g 结构链表； P 还包含大量与 GC 内存管理相关的字段，这里暂时省略。 2.3.1 P 的状态 状态 值 含义 _Pidle 0 P 没有任何 G 可以执行，被空闲 P 链表持有着 _Prunning 1 P 绑定了一个 M，并且正在执行 G _Psyscall 2 P 绑定了 M，但是 M 陷入系统调用阻塞，P 可以被其他 M 获取 _Pgcstop 3 P 绑定着 M，但是因为 STW 挂起 _Pdead 4 P 不在被使用，由于 GOMAXPROCS 缩小 可以看到，_Pidle 与 _Psyscall 都属于 P 可以被其他 M 绑定的状态。 2.3.2 P 的创建/销毁 前面可以看到，G 是由 go 命令创建的，而 M 是按需创建的。P 的创建不同，因为其代表的是并发个数，所以其创建是在程序启动时创建。 在执行用户 main 函数之间的 scheinit() 中进行 runtime 的初始化，其中一项就是初始化 P: // The bootstrap sequence is: // // call osinit // call schedinit // make \u0026 queue new G // call runtime·mstart // // The new G calls runtime·main. func schedinit() { // … procs := ncpu if n, ok := atoi32(gogetenv(\"GOMAXPROCS\")); ok \u0026\u0026 n \u003e 0 { procs = n } if procresize(procs) != nil { throw(\"unknown runnable goroutine during bootstrap\") } // … } procresize() 用于改变 P 的数量（runtime/proc.go）： // Change number of processors. The world is stopped, sched is locked. // gcworkbufs are not being modified by either the GC or // the write barrier code. // Returns list of Ps with local work, they need to be scheduled by the caller. func procresize(nprocs int32) *p { old := gomaxprocs // 扩大 P 的数量  // Grow allp if necessary.  if nprocs \u003e int32(len(allp)) { // Synchronize with retake, which could be running  // concurrently since it doesn't run on a P.  lock(\u0026allpLock) if nprocs \u003c= int32(cap(allp)) { allp = allp[:nprocs] } else { nallp := make([]*p, nprocs) // Copy everything up to allp's cap so we  // never lose old allocated Ps.  copy(nallp, allp[:cap(allp)]) allp = nallp } unlock(\u0026allpLock) } // 初始化新的 P  // initialize new P's  for i := old; i \u003c nprocs; i++ { pp := allp[i] if pp == nil { pp = new(p) } pp.init(i) atomicstorep(unsafe.Pointer(\u0026allp[i]), unsafe.Pointer(pp)) } // 如果当前 M 绑定的 P 是要被释放的，那么 M 新选取一个可用的 P  _g_ := getg() if _g_.m.p != 0 \u0026\u0026 _g_.m.p.ptr().id \u003c nprocs { // continue to use the current P  _g_.m.p.ptr().status = _Prunning _g_.m.p.ptr().mcache.prepareForSweep() } else { // release the current P and acquire allp[0].  //  // We must do this before destroying our current P  // because p.destroy itself has write barriers, so we  // need to do that from a valid P.  if _g_.m.p != 0 { _g_.m.p.ptr().m = 0 } _g_.m.p = 0 p := allp[0] p.m = 0 p.status = _Pidle acquirep(p) if trace.enabled { traceGoStart() } } // g.m.p is now set, so we no longer need mcache0 for bootstrapping.  mcache0 = nil // 清理不使用的 P  // release resources from unused P's  for i := nprocs; i \u003c old; i++ { p := allp[i] p.destroy() // can't free P itself because it can be referenced by an M in syscall  } // Trim allp.  if int32(len(allp)) != nprocs { lock(\u0026allpLock) allp = allp[:nprocs] unlock(\u0026allpLock) } var runnablePs *p for i := nprocs - 1; i \u003e= 0; i-- { p := allp[i] if _g_.m.p.ptr() == p { continue } p.status = _Pidle if runqempty(p) { pidleput(p) } else { p.m.set(mget())","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:3","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.4 schedt schedt 是一个单例全局变量，包含一些全局的链表（runtime/runtime2.go）： var sched schedt type schedt struct { // When increasing nmidle, nmidlelocked, nmsys, or nmfreed, be  // sure to call checkdead(). midle muintptr // idle m's waiting for work  nmidle int32 // number of idle m's waiting for work pidle puintptr // idle p's  npidle uint32 // Global runnable queue.  runq gQueue runqsize int32 // disable controls selective disabling of the scheduler.  //  // Use schedEnableUser to control this.  //  // disable is protected by sched.lock.  disable struct { // user disables scheduling of user goroutines.  user bool runnable gQueue // pending runnable Gs  n int32 // length of runnable  } // Global cache of dead G's.  gFree struct { lock mutex stack gList // Gs with stacks  noStack gList // Gs without stacks  n int32 } // Central cache of sudog structs.  sudoglock mutex sudogcache *sudog // Central pool of available defer structs of different sizes.  deferlock mutex deferpool [5]*_defer // freem is the list of m's waiting to be freed when their  // m.exited is set. Linked through m.freelink.  freem *m } midle ：空闲的 M 的链表； pidle ：空闲的 P 的链表； runq ：全局的可运行的 G 队列； gFree ：全局的可复用的 g 结构队列； freem ：等待释放的 M 的链表，当 M 新建时会将其释放，并复用 m 对象； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:4","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3 调度循环 前面 M 的启动看到，每个 M 创建后会进入 schedule() 的调度循环，并且不会返回，每个 M 执行大致流程如下： 执行 schedule()， 使得 M 找到一个可用的 G，并绑定； 执行 execute()，完成执行 G.fn 的准备工作； 调用 G 的函数； G 函数调用退出后，调用 goexit() 函数清理相关资源，并重新进入 schedule()； 当然，上面是正常 G 执行并退出的逻辑，多数情况下 G 执行的过程中都会经历抢占与调度，也就是说会 M 可能会切换 P、切换 G 执行。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:3:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.1 schedule() schedule() 是调度循环的第一步，M 会在这里尽力寻找一个 runnable G，然后进入 execute() 执行 G 的代码： // One round of scheduler: find a runnable goroutine and execute it. // Never returns. func schedule() { _g_ := getg() // 锁定了 G, 那么还是执行锁定的 G  if _g_.m.lockedg != 0 { stoplockedm() execute(_g_.m.lockedg.ptr(), false) // Never returns.  } top: pp := _g_.m.p.ptr() pp.preempt = false // STW 中，等待结束  if sched.gcwaiting != 0 { gcstopm() goto top } // 运行 timer  checkTimers(pp, 0) // gp 为新调度到的 G  var gp *g var inheritTime bool // Normal goroutines will check for need to wakeP in ready,  // but GCworkers and tracereaders will not, so the check must  // be done here instead.  tryWakeP := false if trace.enabled || trace.shutdown { gp = traceReader() if gp != nil { casgstatus(gp, _Gwaiting, _Grunnable) traceGoUnpark(gp, 0) tryWakeP = true } } // GC 扫描开始工作，尝试 GCWorker 的 G  if gp == nil \u0026\u0026 gcBlackenEnabled != 0 { gp = gcController.findRunnableGCWorker(_g_.m.p.ptr()) tryWakeP = tryWakeP || gp != nil } // 定期直接从 全局可运行队列 获取 G，防止饥饿  if gp == nil { // Check the global runnable queue once in a while to ensure fairness.  // Otherwise two goroutines can completely occupy the local runqueue  // by constantly respawning each other.  if _g_.m.p.ptr().schedtick%61 == 0 \u0026\u0026 sched.runqsize \u003e 0 { lock(\u0026sched.lock) gp = globrunqget(_g_.m.p.ptr(), 1) unlock(\u0026sched.lock) } } // 从当前 P 的 可运行队列 获取一个 G  if gp == nil { gp, inheritTime = runqget(_g_.m.p.ptr()) // We can see gp != nil here even if the M is spinning,  // if checkTimers added a local goroutine via goready.  } // 上面都尝试了，尽可能去找到一个 G，这里会阻塞  if gp == nil { gp, inheritTime = findrunnable() // blocks until work is available  } // This thread is going to run a goroutine and is not spinning anymore,  // so if it was marked as spinning we need to reset it now and potentially  // start a new spinning M.  if _g_.m.spinning { resetspinning() } // 帮忙唤醒别的 P  // If about to schedule a not-normal goroutine (a GCworker or tracereader),  // wake a P if there is one.  if tryWakeP { wakep() } // 如果选出的 G 是被别的 M 锁定的，那么只能重新走流程  if gp.lockedm != 0 { // Hands off own p to the locked m,  // then blocks waiting for a new p.  startlockedm(gp) goto top } // 执行新的 G  execute(gp, inheritTime) } M 有锁定的 G，执行锁定 G 代码； 从各个地方得到一个 runnable G，包括： GCWork 的 G，见： 定期直接从 全局可运行的队列 获取 G，防止全局队列的 G 长时间饥饿； 从绑定的 P 的 可运行队列 获取 G； 通过 fundrunnable() 尽可能获取一个 G； 最终，获取到一个 G 后，execute() 执行 G 的代码； 3.1.1 fundrunnable() 为了找到可以运行的 G，findrunnable() 会尝试各个手段。但是这个代码比较复杂，这里捡最关键的点看（runtime/proc.go）： // Finds a runnable goroutine to execute. // Tries to steal from other P's, get g from local or global queue, poll network. func findrunnable() (gp *g, inheritTime bool) { _g_ := getg() top: // 正在垃圾回收 STW，休眠轮询  _p_ := _g_.m.p.ptr() if sched.gcwaiting != 0 { gcstopm() goto top } now, pollUntil, _ := checkTimers(_p_, 0) if fingwait \u0026\u0026 fingwake { if gp := wakefing(); gp != nil { ready(gp, 0, true) } } // 从绑定的 P 队列获取  if gp, inheritTime := runqget(_p_); gp != nil { return gp, inheritTime } // 从全局队列获取  if sched.runqsize != 0 { lock(\u0026sched.lock) gp := globrunqget(_p_, 0) unlock(\u0026sched.lock) if gp != nil { return gp, false } } // 检查全局的 netpoll 的 G  if netpollinited() \u0026\u0026 atomic.Load(\u0026netpollWaiters) \u003e 0 \u0026\u0026 atomic.Load64(\u0026sched.lastpoll) != 0 { if list := netpoll(0); !list.empty() { // non-blocking  gp := list.pop() injectglist(\u0026list) casgstatus(gp, _Gwaiting, _Grunnable) if trace.enabled { traceGoUnpark(gp, 0) } return gp, false } } // 从别的 P 偷取一半的任务  // Steal work from other P's.  procs := uint32(gomaxprocs) ranTimer := false for i := 0; i \u003c 4; i++ { for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() { if sched.gcwaiting != 0 { goto top } stealRunNextG := i \u003e 2 // first look for ready queues with more than 1 g  p2 := allp[enum.position()] if _p_ == p2 { continue } if gp := runqsteal(_p_, p2, stealRunNextG); gp != nil { return gp, false } // Consider stealing timers from p2. // This call to checkTimers is the only place where // we hold a lock on a different P's timers. // Lock content","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:3:1","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.2 execute() execute() 让当前线程执行 G 的代码，并且不会返回： // Schedules gp to run on the current M. // If inheritTime is true, gp inherits the remaining time in the // current time slice. Otherwise, it starts a new time slice. // Never returns. func execute(gp *g, inheritTime bool) { _g_ := getg() // Assign gp.m before entering _Grunning so running Gs have an  // M.  _g_.m.curg = gp gp.m = _g_.m casgstatus(gp, _Grunnable, _Grunning) gp.waitsince = 0 gp.preempt = false gp.stackguard0 = gp.stack.lo + _StackGuard if !inheritTime { _g_.m.p.ptr().schedtick++ } // … gogo(\u0026gp.sched) } 不过 execute() 执行的是一些初始化的操作，切换 PC、SP 等操作只能通过汇编的 gogo 实现，注意关键的 g.sched 结构（gobuf 结构）的传参。 // func gogo(buf *gobuf) // restore state from Gobuf; longjmp TEXT runtime·gogo(SB), NOSPLIT, $16-8 MOVQ buf+0(FP), BX // gobuf 内容放到 BX 寄存器  MOVQ gobuf_g(BX), DX MOVQ 0(DX), CX // make sure g != nil  get_tls(CX) MOVQ DX, g(CX) MOVQ gobuf_sp(BX), SP // 将 SP 地址设置为 gobuf.sp。第一次执行 G 时，这里是 goexit 函数地址  MOVQ gobuf_ret(BX), AX MOVQ gobuf_ctxt(BX), DX MOVQ gobuf_bp(BX), BP MOVQ $0, gobuf_sp(BX) // clear to help garbage collector  MOVQ $0, gobuf_ret(BX) MOVQ $0, gobuf_ctxt(BX) MOVQ $0, gobuf_bp(BX) MOVQ gobuf_pc(BX), BX // 将 BX 寄存器设置为 gobuf.pc。第一次执行 G 时，这里是 G 的函数地址  JMP BX // JMP gobuf.pc，开始执行 这里最关键的就是切换为 G 的执行上下文： 将 SP 设置为 gobuf.sp。如果 G 没有执行过，那么值就是创建 g 结构时填入的 goexit() 函数的地址。 代码流通过 JMP 指令跳转到 gobuf.pc。如果 G 没有执行过，那么就是 G 对应代码的地址。 这里我们也可以知道了，因为函数调用就是将 返回函数、参数 压栈的过程。而这里将栈顶设置为 goexit() 函数，所以当 G 对应用户代码执行完后，就会继续执行 goexit() 函数。这就是 M 不断执行调度循环的关键。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:3:2","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.3 goexit() goexit() 在 G 用户代码执行后，执行的汇编代码，其最后通过切换到 g0 栈调用 goexit0() 函数： // goexit continuation on g0. func goexit0(gp *g) { _g_ := getg() // G running 变为 dead  casgstatus(gp, _Grunning, _Gdead) // 重置 g 的属性  if isSystemGoroutine(gp, false) { atomic.Xadd(\u0026sched.ngsys, -1) } gp.m = nil locked := gp.lockedm != 0 gp.lockedm = 0 _g_.m.lockedg = 0 gp.preemptStop = false gp.paniconfault = false gp._defer = nil // should be true already but just in case.  gp._panic = nil // non-nil for Goexit during panic. points at stack-allocated data.  gp.writebuf = nil gp.waitreason = 0 gp.param = nil gp.labels = nil gp.timer = nil if gcBlackenEnabled != 0 \u0026\u0026 gp.gcAssistBytes \u003e 0 { // Flush assist credit to the global pool. This gives  // better information to pacing if the application is  // rapidly creating an exiting goroutines.  scanCredit := int64(gcController.assistWorkPerByte * float64(gp.gcAssistBytes)) atomic.Xaddint64(\u0026gcController.bgScanCredit, scanCredit) gp.gcAssistBytes = 0 } // 解除 M 与 G 的绑定  dropg() // 将 dead G 放到 p.gFree 或者 sched.gFree  gfput(_g_.m.p.ptr(), gp) if locked { // 如果 M 与 G 是锁定的，那么 M 线程退出  // The goroutine may have locked this thread because  // it put it in an unusual kernel state. Kill it  // rather than returning it to the thread pool. // Return to mstart, which will release the P and exit // the thread. if GOOS != \"plan9\" { // See golang.org/issue/22227. gogo(\u0026_g_.m.g0.sched) } else { // Clear lockedExt on plan9 since we may end up re-using // this thread. _g_.m.lockedExt = 0 } } // 重新进入调度循环  schedule() } G 状态由 _Grunning 变为 _Gdead； 重置 G 的属性； 通过 dropg() 解除 M 与 G 的绑定； 将 g 对象放到 p.gFree 或者 sched.gFree，以便后续创建 G 时可以复用对象； 如果 M 与 G 是锁定着的，而 G 执行完毕，让 m 回到 mstart() 函数继续执行，这样 M 线程会被销毁并退出； 重新进入 schedule() 调度循环； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:3:3","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4 调度切换 当前，调度循环中描述的情况是一个 M 执行 G 不被抢占与调度的情况。大多数情况下，当 M 执行 G.fn 的过程中就会被切换，执行其他的 G 的情况。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.1 切换时机 先大体总结一下可能的切换时机： 主动挂起 遇到 runtime 级别阻塞（例如 channel 读写阻塞） 主动调度 调用 runtime.Gosched() 主动进行调度 系统调用 系统调用结束后，M 可能进行 G 的切换； 抢占 sysmon 判断 G 运行时间大于 10ms，进行抢占； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:1","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.2 主动挂起 在 G 遇到非系统调用的阻塞前，就会调用 gopark()，将 G 由 _Grunning -\u003e _Gwaiting，而 M 解绑 G，重新进入调度循环： // Puts the current goroutine into a waiting state and calls unlockf. // If unlockf returns false, the goroutine is resumed. func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) { if reason != waitReasonSleep { checkTimeouts() // timeouts may expire while two goroutines keep the scheduler busy  } mp := acquirem() gp := mp.curg status := readgstatus(gp) mp.waitlock = lock mp.waitunlockf = unlockf gp.waitreason = reason mp.waittraceev = traceEv mp.waittraceskip = traceskip releasem(mp) // can't do anything that might move the G between Ms here.  mcall(park_m) } // park continuation on g0. func park_m(gp *g) { _g_ := getg() // 改变 G 状态  casgstatus(gp, _Grunning, _Gwaiting) // M 解绑 G  dropg() if fn := _g_.m.waitunlockf; fn != nil { ok := fn(gp, _g_.m.waitlock) _g_.m.waitunlockf = nil _g_.m.waitlock = nil if !ok { if trace.enabled { traceGoUnpark(gp, 2) } casgstatus(gp, _Gwaiting, _Grunnable) execute(gp, true) // Schedule it back, never returns.  } } // 重新进入调度循环  schedule() } Note 上面没有任何地方记录 _Gwaiting 状态的 G，Why？ 因为这时 gopark() 调用者的责任，例如 channel 读写阻塞时，会将 g 记录到 channel 时，在唤醒时将 G 重新加入到可运行队列。 当进入 _Gwaiting 状态的 G 需要恢复时，调用 goready() / goparkunlock() 函数进行恢复： func goready(gp *g, traceskip int) { systemstack(func() { ready(gp, traceskip, true) }) } // Mark gp ready to run. func ready(gp *g, traceskip int, next bool) { status := readgstatus(gp) // 获取一个 M  // Mark runnable.  _g_ := getg() mp := acquirem() // disable preemption because it can be holding p in a local var // waiting -\u003e runnable  // status is Gwaiting or Gscanwaiting, make Grunnable and put on runq  casgstatus(gp, _Gwaiting, _Grunnable) // 放置到 M 对应 P 的 runq，等待调度执行  runqput(_g_.m.p.ptr(), gp, next) wakep() releasem(mp) } 调用 gopark() 的地方有许多，列出主要的几个地方： channel 的发送/接受阻塞； select 所有 case 不满足，陷入阻塞； time.Sleep 使得 goroutine 进入阻塞； GC 工作的 gcwork 挂起等待唤醒； main goroutine 挂起并且不会被唤醒； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:2","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.3 主动调度 标准库接口 runtime.Gosched() 可以在用户代码中使 G 主动让出 M： func Gosched() { checkTimeouts() mcall(gosched_m) } // Gosched continuation on g0. func gosched_m(gp *g) { goschedImpl(gp) } func goschedImpl(gp *g) { // running 状态变为 runnable  casgstatus(gp, _Grunning, _Grunnable) // M 与 G 解绑  dropg() // 放到 global runq  lock(\u0026sched.lock) globrunqput(gp) unlock(\u0026sched.lock) schedule() } 最后会调用 goschedImpl()，将 G 由 _Grunning -\u003e _Grunnable 状态，M 与 G 解绑，并将其放到全局运行队列。 Note 因为调用 goschedImpl() 是要切换正在运行的 G，所以放到全局队列。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:3","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.4 系统调用 Go 通过 syscall.Syscall 和 syscall.RawSyscall 来封装系统的所有系统调用。 syscall.Syscall 针对可能长时间阻塞的系统调用，例如 IO 操作。 使得在陷入系统调用之间，系统调用结束后，可以触发一些准备和情况工作。 syscall.RawSyscall 针对不太会长时间阻塞的系统调用，例如 读取时间等。 直接进行系统调用，不做其他处理。 系统调用分类 大神将各个系统调用分类了，见 这里 TEXT ·Syscall(SB),NOSPLIT,$0-56 CALL runtime·entersyscall(SB) // 调用 entersyscall()  MOVQ a1+8(FP), DI MOVQ a2+16(FP), SI MOVQ a3+24(FP), DX MOVQ trap+0(FP), AX // syscall entry  SYSCALL CMPQ AX, $0xfffffffffffff001 JLS ok MOVQ $-1, r1+32(FP) MOVQ $0, r2+40(FP) NEGQ AX MOVQ AX, err+48(FP) CALL runtime·exitsyscall(SB) // 结束调用 exitsyscall()  RET ok: MOVQ AX, r1+32(FP) MOVQ DX, r2+40(FP) MOVQ $0, err+48(FP) CALL runtime·exitsyscall(SB) // 结束调用 exitsyscall()  RET 系统调用前，执行 entersyscall()； 执行系统调用； 系统调用后，执行 exitsyscall()； 4.4.1 系统调用前的准备 entersyscall() 会调用 reentersyscall() 函数，执行进入系统调用前的准备工作： func reentersyscall(pc, sp uintptr) { _g_ := getg() _g_.m.locks++ _g_.stackguard0 = stackPreempt _g_.throwsplit = true save(pc, sp) _g_.syscallsp = sp _g_.syscallpc = pc casgstatus(_g_, _Grunning, _Gsyscall) _g_.m.syscalltick = _g_.m.p.ptr().syscalltick _g_.m.mcache = nil pp := _g_.m.p.ptr() pp.m = 0 _g_.m.oldp.set(pp) _g_.m.p = 0 atomic.Store(\u0026pp.status, _Psyscall) if sched.gcwaiting != 0 { systemstack(entersyscall_gcwait) save(pc, sp) } _g_.m.locks-- } 禁止线程上发生的抢占，防止出现内存不一致的问题； 保证当前函数不会触发栈分裂或者增长； 通过 save() 保存 PC、SP 值至 g.sched； G 状态 _Grunning -\u003e _Gsyscall； m.oldp 设置为当前 P，m.p 设置为 0，这意味着记录但是解绑当前的 P，而 P 状态为 _Psyscall； 这里比较重要的就是让 M 与 P 解绑，使得其他 M 可以获取到该 P 并执行 G。 因此，P 代表的是并发数，而不是线程数。 4.4.2 系统调用后的恢复 系统调用结束后，执行 exitsyscall() 进行恢复操作。 func exitsyscall() { _g_ := getg() // M 尝试绑定阻塞前使用的 P，或者一个新的 P  oldp := _g_.m.oldp.ptr() _g_.m.oldp = 0 if exitsyscallfast(oldp) { _g_.m.p.ptr().syscalltick++ casgstatus(_g_, _Gsyscall, _Grunning) ... return } // M 解绑 G，重新进入调用循环  mcall(exitsyscall0) _g_.m.p.ptr().syscalltick++ _g_.throwsplit = false } M 尝试获取一个空闲的 P，从两个地方： 如果 m.oldp 还是为 _Psyscall 状态，说明没被人使用，那么记录使用阻塞前的 P； 从全局空闲 P sched.pidle 链表中获取一个 P； 如果没有 P，那么确实 M 无法执行当前 G，就 M 解绑 G，将 G 放入全局队列； 无论如何，最后 M 都会调用 schedule() 重启进入调度循环，切换一个 G 执行。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:4","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.5 抢占 每个运行中的 G 会有一个运行的时间片，而 sysmon 会周期性检查部分 G，如果其执行时间大于 10ms，就会触发抢占。 抢占包括两种方式： 抢占标志：通过设置 g.stackguard0=stackPreempt。这必然会导致 G 执行下次函数调用时触发栈扩容逻辑，从而走到切换调度的逻辑； 信号抢占（v1.14）：信号抢占会使得 M 线程触发信号中断，执行信号处理函数，从而重新进入调度循环。 4.5.1 触发抢占时机 sysmon 会重启执行 retake() 函数，判断哪些正在运行的 G 是需要抢占的。 func retake(now int64) uint32 { n := 0 // Prevent allp slice changes. This lock will be completely  // uncontended unless we're already stopping the world.  lock(\u0026allpLock) // 遍历所有 P  for i := 0; i \u003c len(allp); i++ { _p_ := allp[i] pd := \u0026_p_.sysmontick s := _p_.status sysretake := false // 允许抢占 _Prunning 与 _Psyscall 状态的 P  if s == _Prunning || s == _Psyscall { t := int64(_p_.schedtick) if int64(pd.schedtick) != t { pd.schedtick = uint32(t) pd.schedwhen = now } else if pd.schedwhen+forcePreemptNS \u003c= now { // 到达抢占时间  preemptone(_p_) // In case of syscall, preemptone() doesn't  // work, because there is no M wired to P.  sysretake = true } } // 对于 _Psyscall，可以让其与 M 解绑，等其他的 M 绑定  if s == _Psyscall { t := int64(_p_.syscalltick) if !sysretake \u0026\u0026 int64(pd.syscalltick) != t { pd.syscalltick = uint32(t) pd.syscallwhen = now continue } if runqempty(_p_) \u0026\u0026 atomic.Load(\u0026sched.nmspinning)+atomic.Load(\u0026sched.npidle) \u003e 0 \u0026\u0026 pd.syscallwhen+10*1000*1000 \u003e now { continue } unlock(\u0026allpLock) incidlelocked(-1) if atomic.Cas(\u0026_p_.status, s, _Pidle) { n++ _p_.syscalltick++ handoffp(_p_) } incidlelocked(1) lock(\u0026allpLock) } } unlock(\u0026allpLock) return uint32(n) } 运行中的 G 运行超过 10ms，调用 preemptone() 进行抢占； 对于 _Psyscall 中的 P，将其解绑 M，使得其他 M 可以绑定该 P； 4.5.2 触发抢占 preemptone() 触发抢占，通过上面所述的两种方式。 func preemptone(_p_ *p) bool { mp := _p_.m.ptr() gp := mp.curg gp.preempt = true // 设置抢占标志  // Every call in a go routine checks for stack overflow by  // comparing the current stack pointer to gp-\u003estackguard0.  // Setting gp-\u003estackguard0 to StackPreempt folds  // preemption into the normal stack overflow check.  gp.stackguard0 = stackPreempt // 信号抢占  // Request an async preemption of this P.  if preemptMSupported \u0026\u0026 debug.asyncpreemptoff == 0 { _p_.preempt = true preemptM(mp) } return true } 设置 G 的抢占标志，gp.stackguard0 = stackPreempt； 执行 preemptM() 进行信号抢占； 4.5.3 通过抢占标志抢占 前面看到，设置 gp.stackguard0 = stackPreempt，而这在每次 G 函数调用前的检查是否扩容栈时，必然会触发 G 栈扩容逻辑 newstack()。 而在 内存管理 时，介绍了 newstack() 如果扩容 G 的栈，但是省略了一个重要的逻辑分支：newstack() 函数中还会进行 G 的调度： func newstack() { thisg := getg() gp := thisg.m.curg // …  preempt := atomic.Loaduintptr(\u0026gp.stackguard0) == stackPreempt // 特殊情况下不能抢占时，继续走 G 代码逻辑  if preempt { if !canPreemptM(thisg.m) { // Let the goroutine keep running for now.  // gp-\u003epreempt is set, so it will be preempted next time.  gp.stackguard0 = gp.stack.lo + _StackGuard gogo(\u0026gp.sched) // never return  } // …  if preempt { if gp == thisg.m.g0 { throw(\"runtime: preempt g0\") } if thisg.m.p == 0 \u0026\u0026 thisg.m.locks == 0 { throw(\"runtime: g is running but p is not\") } if gp.preemptShrink { // We're at a synchronous safe point now, so  // do the pending stack shrink.  gp.preemptShrink = false shrinkstack(gp) } if gp.preemptStop { preemptPark(gp) // never returns  } // Act like goroutine called runtime.Gosched.  gopreempt_m(gp) // never return  } // … } func gopreempt_m(gp *g) { // 老朋友了  goschedImpl(gp) } 判断到 gp.stackguard0 = stackPreempt 后，无论如何都会走重新调度的逻辑，M 重新进入调度循环。 4.5.4 信号抢占 执行 preemptM() 会对 M 进行信号抢占，通过发送 SIGURG 信号触发 M 线程的信号处理函数。 const sigPreempt = _SIGURG func preemptM(mp *m) { if GOOS == \"darwin\" \u0026\u0026 GOARCH == \"arm64\" \u0026\u0026 !iscgo { return } if atomic.Cas(\u0026mp.signalPending, 0, 1) { if GOOS == \"darwin\" { atomic.Xadd(\u0026pendingPreemptSignals, 1) } // If multiple threads are preempting the same M, it may send many  // signals to the same M such that it hardly make progress, causing  // live-lock problem. Apparently this could happen on darwin. See  // issue #37741.  // Only send a signal if there isn't already one pending.  signalM(mp, sigPreempt) } } M 会执行信号处理函数 asyncPreempt()，最后调用到 asyncPreempt2()，使得 M 进入重新调度： // asyncPreempt saves all user registers and calls asyncPreempt2. // // When stack scanning encounters an asyncPreempt frame","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:5","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"总结 相对于 内存管理 与 垃圾收集，竟然感觉并发调度的结构还算简单。 需要弄清楚的有以下几点： 协程出现的意义 调度器的工作 GMP 模型整体框架 G、M、P 代表的意义 调度循环的流程 进入调度循环 M 的行为 找寻 G 的各个途径 M 执行 G 时，如何切换上下文 如何退出回到调度循环 调度切换 切换的各个时机 系统调用进行调度切换的行为 抢占的两种方式 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:5:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"参考 《Golang 学习笔记》 《Golang 设计与实现》：调度器 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:6:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"Golang 三色标记收集算法，算法实现原理","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Golang 垃圾收集的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 下面代码都是基于 go 1.15.6。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:0:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1 背景知识 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:1:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.1 术语 垃圾回收算法有一些基本的术语，首先需要知道对应的含义： Mutator：具有“改变对象”的意思，GC 中就是改动对象间引用关系的意思，也就是程序； 堆：对象使用的内存空间，GC 就是将垃圾对象空间放回到堆中； 根对象：对象的指针的“起点”部分，一般就是全局对象和栈对象； ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:1:1","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.2 三色标记算法 三色标记算法是 GC 标记清除算法Mark-Sweep 的一种，也是 Golang 中使用的算法。 推荐阅读 推荐阅读文章，写的非常详细：垃圾收集器 首先，三色标记算法的最基本逻辑为： 标记的最开始，所有对象默认为白色； 将 根对象标记为灰色，放入灰色集合； 从 灰色集合 中取出灰色对象，将其子对象标记为灰色，加入灰色集合，该灰色对象标记为黑色； 重复第 3 步，直到灰色集合为空； 清理所有的白色对象； 整个逻辑很简单，就是一个树的层次遍历，将所有可达的结点标记，然后清理未标记的不可达结点。 但是，仅仅是普通的三色标记算法要求执行时，Mutator 不能同时运行。因为如果 Mutator 并行时，某个扫描过的结点的引用关系变化，就可能导致 悬挂指针dangling pointer 问题。 例如，上图中第 3 步将 A 指向 D，那么 D 还是无法被标记，被错误回收。 而想要让 Mutator 同时运行时，标记的结果还保持正确，那么每个时刻标记的结果要满足 三色不变性Tri-color invariant 强三色不变性：黑色对象不会指向白色对象，只会指向灰色对象或者黑色对象。 因为黑色对象不会再被扫描，如果黑色对象指向白色对象，那么肯定该白色对象会被错误回收。 当然，除非这种情况能够满足弱三色不变性。 弱三色不变性：黑色对象执行白色对象，那么必须包含一条灰色对象经由多个白色对象的可达路径。 因为有了后面这个可达路径，也就是说白色对象还是可以被标记的，那么黑色对象可以指向该白色对象。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:1:2","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.3 屏障技术 为了满足两个不变性，所以要在对象引用变更时，做出一些操作改变对象标记。这就是 GC 里 屏障技术barrier 的作用。 Go 中使用了写屏障，即在用户程序更新对象指针时，执行一些代码，使得继续满足不变性。 Note 这里的屏障技术似乎和我知道的 CPU 的屏障技术含义不太类似，更像是回调函数，也挺困惑 1.3.1 插入写屏障 Dijkstra 提出的 插入写屏障，在更新对象指针时，将其被指向的对象重新加入扫描集合（三色标记中也就是变为灰色），这样接下来还是能够被扫描。 可以看到，这样黑色对象始终指向的是灰色对象，永远都会满足强三色不变性。 但是，Dijkstra 也有一些缺点： 对象指针变动时，没有考虑旧的指针引用。例如 *field 原来的对象 oldobj 已经扫描成黑色了，那么 *field = newobj 变动后，可能 oldobj 变为垃圾对象，只有等到下一轮标记时才会被回收。 TODO 1.3.2 删除写屏障 Yuasa 提出的 删除写屏障，让老对象的引用被删除时，将白色的老对象涂成灰色，这样删除写屏障就可以保证弱三色不变性。 1.3.3 Go 中的屏障 Go 中使用 混合写屏障，即插入写屏障与删除写屏障都开启，并且在标记阶段开始后，将创建的所有新对象都标记为黑色，防止新分配的对象被错误的回收。 具体操作为： GC 开始将栈上的对象全部扫描并标记为黑色（之后不再进行第二次重复扫描，无需 STW)， GC 期间，任何在栈上创建的新对象，均为黑色。 被删除的对象标记为灰色。 被添加的对象标记为灰色。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:1:3","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2 三色标记算法实现 我们先不看整个的流程实现，而是从核心的标记算法入手。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:2:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.1 标记 2.1.1 并发标记框架 runtime 使用并发的标记方式，多个 groutine 同时执行一部分内存的标记操作。其整个就是基于一个工作池框架。 首先，有着一个全局的工作队列，其放在 work 全局变量中： var work struct { full lfstack // lock-free list of full blocks workbuf empty lfstack // lock-free list of empty blocks workbuf } full：全局的非空的队列组成的链表，单个 groutine 没有任务时就从这里取一个队列使用； empty：全局的空的队列组成的链表，单个 groutine 的工作队列满时就会这里去一个队列切换； 每个 goroutine 有着独立对应的 gcWork 对象，其类似一个队列，从队列中取出需要扫描的内存的地址。 type workbuf struct { workbufhdr obj [(_WorkbufSize - unsafe.Sizeof(workbufhdr{})) / sys.PtrSize]uintptr } type gcWork struct { // wbuf1 and wbuf2 are the primary and secondary work buffers.  //  // This can be thought of as a stack of both work buffers'  // pointers concatenated. When we pop the last pointer, we  // shift the stack up by one work buffer by bringing in a new  // full buffer and discarding an empty one. When we fill both  // buffers, we shift the stack down by one work buffer by  // bringing in a new empty buffer and discarding a full one.  // This way we have one buffer's worth of hysteresis, which  // amortizes the cost of getting or putting a work buffer over  // at least one buffer of work and reduces contention on the  // global work lists.  //  // wbuf1 is always the buffer we're currently pushing to and  // popping from and wbuf2 is the buffer that will be discarded  // next.  //  // Invariant: Both wbuf1 and wbuf2 are nil or neither are.  wbuf1, wbuf2 *workbuf … } wbuf1 与 wbuf2: 就是主队列与备队列。所有操作都会先操作 wbuf1，如果 wbuf1 空间不足或者没有对象，就会触发 wbuf1 与 wbuf2 切换。当两个缓冲区都空间不足或者满时，就会从 work.free 或者 work.list 得到一个空闲的，并赋值 wbuf1。 gcWork 有几个重要的方法： gcWork.tryGetFast() ：从 wbuf1 快速得到一个 obj 的地址； gcWork.tryGet() ：从 wbuf1 -\u003e wbuf2 -\u003e work.full 获取一个 obj； gcWork.putFast() ：将一个待扫描的 obj 放入 wbuf1； gcWork.put() ：将一个待扫描的 obj 放入 wbuf1 -\u003e wbuf2 -\u003e work.empty； gcWork.balance() ：将 wbuf2/wbuf1 放入 work.full 中； gcWork.empty() ：判断是否 gcWork 为空，为空表明没有会灰色 obj； 可以看到，整个过程就是围绕了各个工作队列的生产-消费过程。先从根对象开始，goroutine 从 gcWork 取出待扫描的 obj，将其标记，然后将 obj 指向的子 obj 再次放入 gcWork。不断循环，直到 gcWork 为空。 2.1.2 groutine 标记流程 下面看下核心的标记流程，这里我们仅仅关注一个 groutine 的工作。大致的标记步骤如下： 将根对象放入 gcWork； groutine 从 gcWork 取出一个 object 地址，将其标记； 将 object 包含的指针指向的 object 再次放入 gcWork； 重复 2-3 步，直到 gcWork 为空； 而对应于三色标记，我们可以确认不同颜色的对象在 runtime 中的对应： 灰色对象 -\u003e gcWork 中的 object； 黑色对象 -\u003e 不在 gcWork 中，但是被 mark 的 object； 白色对象 -\u003e 不在 gcWork 中，没有被 mark 的 object； 每个 P 会对应一个标记使用的 groutine，执行 gcDrain() 函数（runtime/mgcmark.go）： // gcDrain scans roots and objects in work buffers, blackening grey // objects until it is unable to get more work. It may return before // GC is done; it's the caller's responsibility to balance work from // other Ps. // // If flags\u0026gcDrainUntilPreempt != 0, gcDrain returns when g.preempt // is set. // // If flags\u0026gcDrainIdle != 0, gcDrain returns when there is other work // to do. // // If flags\u0026gcDrainFractional != 0, gcDrain self-preempts when // pollFractionalWorkerExit() returns true. This implies // gcDrainNoBlock. // // If flags\u0026gcDrainFlushBgCredit != 0, gcDrain flushes scan work // credit to gcController.bgScanCredit every gcCreditSlack units of // scan work. // // gcDrain will always return if there is a pending STW. // //go:nowritebarrier func gcDrain(gcw *gcWork, flags gcDrainFlags) { gp := getg().m.curg preemptible := flags\u0026gcDrainUntilPreempt != 0 flushBgCredit := flags\u0026gcDrainFlushBgCredit != 0 idle := flags\u0026gcDrainIdle != 0 initScanWork := gcw.scanWork // 配置退出标记的 check 函数，根据不同策略退出标记  checkWork := int64(1\u003c\u003c63 - 1) var check func() bool if flags\u0026(gcDrainIdle|gcDrainFractional) != 0 { checkWork = initScanWork + drainCheckThreshold if idle { check = pollWork } else if flags\u0026gcDrainFractional != 0 { check = pollFractionalWorkerExit } } // 扫描根对象放入 gcWork  if work.markrootNext \u003c work.markrootJobs { // Stop if we're preemptible or if someone wants to STW.  for !(gp.preempt \u0026\u0026 (preemptible || atomic.Load(\u0026sched.gcwaiting) != 0)) { job := atomic.Xadd(\u0026work.markrootNext, +1) - 1 if job \u003e= work.markrootJobs { break } markroot(gcw, job) if check != nil \u0026\u0026 check() { goto done } } } // 标记循环  for !(gp.preem","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:2:1","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.2 写屏障 在各个代码的注释中，可以看到 “//go:nowritebarrier”。显然，这样让编译器编译时不加入写屏障的意思，因此可以想到，默认的函数执行都会加入写屏障的逻辑。 在 SSA 中间代码生成阶段，编译器会在 Store、Move、Zero 操作中加入写屏障，写屏障函数为 writebarrier() 函数（cmd/compile/internal/ssa/writebarrier.go）。 writebarrier() 函数很复杂，这里不展开。再次看一下混合写屏障操作： GC 开始将栈上的对象全部扫描并标记为黑色（之后不再进行第二次重复扫描，无需 STW)。 GC 期间，任何在栈上创建的新对象，均为黑色。 被删除的对象标记为灰色。 被添加的对象标记为灰色。 当开始 GC 时，全局变量 runtime.writeBarrier.enabled 变为 true，所有的写操作都会经过 writebarrier() 的操作。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:2:2","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3 内存清理 内存清理与标记就是完全分隔的逻辑了，通过判断对象是否被标记就可决定是否将其内存回收。 gcSweep() 函数用于在 GC 标记结束后执行清理（src/runtime/mgc.go）： // gcSweep must be called on the system stack because it acquires the heap // lock. See mheap for details. // // The world must be stopped. // //go:systemstack func gcSweep(mode gcMode) { lock(\u0026mheap_.lock) // 关键的 sweepgen 变量  mheap_.sweepgen += 2 mheap_.sweepdone = 0 if !go115NewMCentralImpl \u0026\u0026 mheap_.sweepSpans[mheap_.sweepgen/2%2].index != 0 { // We should have drained this list during the last  // sweep phase. We certainly need to start this phase  // with an empty swept list.  throw(\"non-empty swept list\") } mheap_.pagesSwept = 0 mheap_.sweepArenas = mheap_.allArenas mheap_.reclaimIndex = 0 mheap_.reclaimCredit = 0 unlock(\u0026mheap_.lock) if go115NewMCentralImpl { sweep.centralIndex.clear() } // 阻塞清理  if !_ConcurrentSweep || mode == gcForceBlockMode { // Special case synchronous sweep.  // Record that no proportional sweeping has to happen.  lock(\u0026mheap_.lock) mheap_.sweepPagesPerByte = 0 unlock(\u0026mheap_.lock) // 清理 span !  // Sweep all spans eagerly.  for sweepone() != ^uintptr(0) { sweep.npausesweep++ } // Free workbufs eagerly.  prepareFreeWorkbufs() for freeSomeWbufs(false) { } // All \"free\" events for this mark/sweep cycle have  // now happened, so we can make this profile cycle  // available immediately.  mProf_NextCycle() mProf_Flush() return } // 后台清理（并发清理）  // Background sweep.  lock(\u0026sweep.lock) if sweep.parked { sweep.parked = false ready(sweep.g, 0, true) } unlock(\u0026sweep.lock) } 不断执行 sweepone() 来清理 mspan； 启动后台并发清理； ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:3:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.1 阻塞清理 通过不断执行 sweepone() 来进行 mspan 的清理，sweepone() 从 heap 得到一个 mspan 并清理（src/runtime/mgcsweep.go）： // sweepone sweeps some unswept heap span and returns the number of pages returned // to the heap, or ^uintptr(0) if there was nothing to sweep. func sweepone() uintptr { … // 得到一个被清理的 mspan var s *mspan sg := mheap_.sweepgen for { if go115NewMCentralImpl { s = mheap_.nextSpanForSweep() } else { s = mheap_.sweepSpans[1-sg/2%2].pop() } if s == nil { atomic.Store(\u0026mheap_.sweepdone, 1) break } // 设置标记 if s.sweepgen == sg-2 \u0026\u0026 atomic.Cas(\u0026s.sweepgen, sg-2, sg-1) { break } } // 清理 mspan npages := ^uintptr(0) if s != nil { npages = s.npages if s.sweep(false) { // Whole span was freed. Count it toward the // page reclaimer credit since these pages can // now be used for span allocation. atomic.Xadduintptr(\u0026mheap_.reclaimCredit, npages) } else { // Span is still in-use, so this returned no // pages to the heap and the span needs to // move to the swept in-use list. npages = 0 } } … return npages } 最终的回收工作靠 mspan.sweep() 完成，这给在 Go 内存管理总结 中可以看到具体实现，大致就是将 GC 后没有被 mark 的 object 记录为可用，以后续申请使用覆盖。如果整个 mspan 变回空，就由 mheap 回收其对应的 page。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:3:1","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.2 并发清理 并发清理就是一个死循环，被唤醒后开始执行清理任务。 func bgsweep(c chan int) { sweep.g = getg() lockInit(\u0026sweep.lock, lockRankSweep) lock(\u0026sweep.lock) sweep.parked = true c \u003c- 1 goparkunlock(\u0026sweep.lock, waitReasonGCSweepWait, traceEvGoBlock, 1) for { // 依旧通过 sweepone() 清理 for sweepone() != ^uintptr(0) { sweep.nbgsweep++ Gosched() } for freeSomeWbufs(true) { Gosched() } lock(\u0026sweep.lock) if !isSweepDone() { // This can happen if a GC runs between // gosweepone returning ^0 above // and the lock being acquired. unlock(\u0026sweep.lock) continue } // 等待唤醒 sweep.parked = true goparkunlock(\u0026sweep.lock, waitReasonGCSweepWait, traceEvGoBlock, 1) } } 依旧是通过不断执行 sweepone() 进行清理。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:3:2","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4 标记流程 下面来看如何触发的 GC 以及 GC 的大致流程。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:4:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.1 GC 触发 GC 有三个点会被触发： runtime 启动后会启动一个后台 gourtine，被唤醒后就会执行 GC，而唤醒操作由 sysmon 负责执行。 sysmon 会根据系统情况决定是否触发。 分配新 object 时（mallocgc() 函数），如果 mcache 需要重新刷新，或者是分配的是 large object，那么也会触发一次 GC。 通过接口 runtime.GC() 主动触发。 所有的触发都会使用 gcTrigger.test() 进行条件检测（runtime/mgc.go）： // test reports whether the trigger condition is satisfied, meaning // that the exit condition for the _GCoff phase has been met. The exit // condition should be tested when allocating. func (t gcTrigger) test() bool { if !memstats.enablegc || panicking != 0 || gcphase != _GCoff { return false } switch t.kind { case gcTriggerHeap: // 由 heap 触发，也就是分配 object 时触发  // Non-atomic access to heap_live for performance. If  // we are going to trigger on this, this thread just  // atomically wrote heap_live anyway and we'll see our  // own write.  return memstats.heap_live \u003e= memstats.gc_trigger case gcTriggerTime: // 由 sysmon 周期性触发  if gcpercent \u003c 0 { return false } lastgc := int64(atomic.Load64(\u0026memstats.last_gc_nanotime)) return lastgc != 0 \u0026\u0026 t.now-lastgc \u003e forcegcperiod case gcTriggerCycle: // 通过 runtime.GC() 主动触发  // t.n \u003e work.cycles, but accounting for wraparound.  return int32(t.n-work.cycles) \u003e 0 } return true } 对应的条件为： sysmon 周期性触发：触发间隔大于 2min； 分配 object 触发：堆内存的分配达到控制计算的触发堆大小； runtime.GC() 主动触发：当前没有正在 GC，则触发； ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:4:1","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.2 GC 开始 所有触发 GC 后调用的都是 gcStart() 函数（runtime/mgc.go）： // gcStart starts the GC. It transitions from _GCoff to _GCmark (if // debug.gcstoptheworld == 0) or performs all of GC (if // debug.gcstoptheworld != 0). // // This may return without performing this transition in some cases, // such as when called on a system stack or with locks held. func gcStart(trigger gcTrigger) { // Since this is called from malloc and malloc is called in // the guts of a number of libraries that might be holding // locks, don't attempt to start GC in non-preemptible or // potentially unstable situations. mp := acquirem() if gp := getg(); gp == mp.g0 || mp.locks \u003e 1 || mp.preemptoff != \"\" { releasem(mp) return } releasem(mp) mp = nil // 再次验证是否条件，并不断调用 sweepone() 来完成上一次垃圾收集的收尾工作 // Pick up the remaining unswept/not being swept spans concurrently // // This shouldn't happen if we're being invoked in background // mode since proportional sweep should have just finished // sweeping everything, but rounding errors, etc, may leave a // few spans unswept. In forced mode, this is necessary since // GC can be forced at any point in the sweeping cycle. // // We check the transition condition continuously here in case // this G gets delayed in to the next GC cycle. for trigger.test() \u0026\u0026 sweepone() != ^uintptr(0) { sweep.nbgsweep++ } // Perform GC initialization and the sweep termination // transition. semacquire(\u0026work.startSema) // Re-check transition condition under transition lock. if !trigger.test() { semrelease(\u0026work.startSema) return } // 获取 STW 的锁 // Ok, we're doing it! Stop everybody else semacquire(\u0026gcsema) semacquire(\u0026worldsema) // 每个 P 分配一个 G，准备开始执行后台的标记工作 gcBgMarkStartWorkers() // 执行 STW ! systemstack(stopTheWorldWithSema) // Finish sweep before we start concurrent scan. systemstack(func() { finishsweep_m() }) // 修改 GC 状态，进入标记 setGCPhase(_GCmark) // 初始化标记所需状态 gcBgMarkPrepare() // 计算 Data、BSS、Stack 等需要扫描的数量 gcMarkRootPrepare() // 直接标记 tiny object gcMarkTinyAllocs() // 可以开始运行标记 atomic.Store(\u0026gcBlackenEnabled, 1) // STW 结束，G 开始进行并行标记 // Concurrent mark. systemstack(func() { now = startTheWorldWithSema(trace.enabled) work.pauseNS += now - work.pauseStart work.tMark = now }) // 释放锁 semrelease(\u0026worldsema) releasem(mp) if mode != gcBackgroundMode { Gosched() } semrelease(\u0026work.startSema) } 该函数比较复杂，大致分为下面几个步骤： 主动进入休眠状态，并等待唤醒； 根据 P.gcMarkWorkerMode 决定标记的策略； 调用 gcDrain() 进行标记 所有标记任务完成后，调用 gcMarkDone() 完成标记阶段； 因为标记阶段是与用户进程并发的，所以会涉及到执行垃圾收集还是普通程序的问题。为此，每个垃圾收集的 G 有着不同的标记策略，其依赖于 P.gcMarkWorkerMode（由一个独立的 G 计算出不同模式的 P 的数量并设置）。 其包含三种标记策略： gcMarkWorkerDedicatedMode：P 专门用于标记对象，不会被抢占； gcMarkWorkerFractionalMode：当垃圾收集后台 CPU 使用率达不到 25%，会启动该类型工作协程帮助垃圾收集达到利用率目标，因为只占用一个 CPU 部分资源，可以被抢占； gcMarkWorkerIdleMode：当 P 没有可以执行的 G 时，会运行垃圾收集标记任务直到被抢占； ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:4:2","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.3 标记结束 当每个标记 Groutine 结束后，都会调用 gcMarkDone()，但是等待所有标记结束后，只有一个 groutine 会真正执行结束逻辑（runtime/mgc.go）。 func gcMarkDone() { // 第一个获取到锁的才会执行 gcMarkDone() // Ensure only one thread is running the ragged barrier at a // time. semacquire(\u0026work.markDoneSema) top: // 后续的 gouroute 会串行的在这里退出 if !(gcphase == _GCmark \u0026\u0026 work.nwait == work.nproc \u0026\u0026 !gcMarkWorkAvailable(nil)) { semrelease(\u0026work.markDoneSema) return } // 循环等待所有标记结束 gcMarkDoneFlushed = 0 systemstack(func() { gp := getg().m.curg casgstatus(gp, _Grunning, _Gwaiting) forEachP(func(_p_ *p) { wbBufFlush1(_p_) _p_.gcw.dispose() if _p_.gcw.flushedWork { atomic.Xadd(\u0026gcMarkDoneFlushed, 1) _p_.gcw.flushedWork = false } }) casgstatus(gp, _Gwaiting, _Grunning) }) if gcMarkDoneFlushed != 0 { goto top } … // Perform mark termination. This will restart the world. gcMarkTermination(nextTriggerRatio) } 在一大堆判断标记结束的逻辑后，调用 gcMarkTermination() 进入标记终止阶段。 在 gcMarkTermination() 会关闭混合写屏障，决定触发垃圾收集的 heap 阈值，并进行相关信息的统计，然后调用 gcSweep() 进行阻塞式清理。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:4:3","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"总结 垃圾回收真的很复杂，上面省略了大量的细节，也有可能理解错误的情况。但是忽略掉繁琐的细节，需要完全明白的有几个点： 三色标记算法的步骤 强三色不变性概念，以及为什么需要 写屏障的作用，以及 Go 使用的混合写屏障 GC 触发的时机与条件 GC 并发标记的实现 GC 标记的实现，与内存管理的协同 内存清理的时机 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:5:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"参考 《Golang 学习笔记》 《Golang 设计与实现》：内存分配器 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:6:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"Golang 协程栈实现，堆内存管理实现","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Golang 内存模型与内存管理的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 下面代码都是基于 go 1.15.6。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:0:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1 Linux 内存模型 所有语言的内存管理，在 Linux 上都是在以基本的进程内存模型基础上实现的，首先需要知道 Linux 进程内存布局。 在进程角度，看到的所有内存就是 虚拟地址空间virtual address space ，整个是一个线性的存储地址。其中一部分高地址区域用户态无法访问，是内核地址空间。而另一部分就是由栈、mmap、堆等内存区域组成的用户地址空间。 上面进程可以自己分配与管理的进程，就是 mmap 与 堆，对应的系统调用为 mmap() 与 brk()，因此所有语言的内存管理都是基于这两个内存区域在进一步实现的（包括 glibc 的 malloc() 与 free()）。 mmap 最基本有两个用途： 文件映射 ：申请一块内存区域，映射到文件系统上一个文件（这也是 page_cache 的基本原理，所以他们在内核中都使用 address_space 实现） 匿名映射 ：申请一块内存区域，但是没有映射到具体文件，相当于分配了一块内存区域（可以用于父子进程共享、或者自己管理内存的分配等功能） 而所有在内存上所说的地址，包括代码指令地址、变量地址都是上面地址空间的一个地址。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:1:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2 PC 与 SP Goroutine 将进程的切换变为了协程间的切换，那么就需要在用户空间负责执行代码与协程上下文的保留与切换。因此，有两个关键的寄存器：PC 与 SP。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:2:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.1 PC 程序计数器 PCProgram Counter 是 CPU 中的一个寄存器，保存着下一个 CPU 执行的指令的位置。顺序执行指令时，PC = PC + 1（一个指令）。而调用函数或者条件跳转时，会将跳到的指令地址设置到 PC 中。 所以，可以想到，当需要切换执行的 goroutine，调用 JMP 指令跳转到 G 对应的代码。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:2:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.2 SP 栈顶指针 SPstack pointer 是保存栈顶地址的寄存器，我们平时所说的临时变量在栈上，就是将临时变量的值写入 SP 保存的内存地址，然后 SP 保存的地址减小（栈是从高地址向低地址变化），然后临时变量销毁时，SP 地址又变为高地址。 不过，因为 goroutine 切换时，必须要保存当前 goroutine 的上下文，也就是栈里的变量。因此，goroutine 栈肯定是不能使用 Linux 进程栈了（因为进程栈有上限，也无法实现“保存”这种功能）。所以所说的协程栈，都是基于 mmap 申请内存空间（基于 Go 内存管理，内存管理基于 mmap），然后切换时修改 SP 寄存器地址实现的。 这也是为什么 goroutine 栈可以“无限大”的原因了。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:2:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3 Goroutine 栈 整体的一个 G 的栈如下图所示： stack.lo： G 栈的最大低地址（也就是上限）； stack.hi：G 栈的初始地址； stackguard0：阈值地址，用于判断 G 栈是否需要扩容； StackGuard：常量，栈的保护区，也就是预留的地址； StackSmall：常量，用于小函数调用的优化； 先看一下 g 的实现中包含的 stack 属性（runtime/runtime2.go），其实注释写的就很明白了： type g struct { // Stack parameters. // stack describes the actual stack memory: [stack.lo, stack.hi). // stackguard0 is the stack pointer compared in the Go stack growth prologue. // It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption. // stackguard1 is the stack pointer compared in the C stack growth prologue. // It is stack.lo+StackGuard on g0 and gsignal stacks. // It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash). stack stack // offset known to runtime/cgo stackguard0 uintptr // offset known to liblink stackguard1 uintptr // offset known to liblink // ... } stack 属性就是 G 对应的栈了（这也表明了不是使用的进程栈）； Note stack 与 stackguard0 属性一定要在 g 结构的开头，因为汇编中会使用指定的偏移 (0x10) 来获取对应的值； 具体看一下 stack 结构（runtime/runtime2.go）： // Stack describes a Go execution stack. // The bounds of the stack are exactly [lo, hi), // with no implicit data structures on either side. type stack struct { lo uintptr hi uintptr } ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.1 新 G 的栈 在 malg 函数中，可以看到对于新 G 的栈的分配（一开始为 2KB）： // Allocate a new g, with a stack big enough for stacksize bytes. func malg(stacksize int32) *g { newg := new(g) if stacksize \u003e= 0 { stacksize = round2(_StackSystem + stacksize) // 在公共的 goroutine(g0) 上调用函数 systemstack(func() { // 分配一个 stack newg.stack = stackalloc(uint32(stacksize)) }) // 设置 stackguard0 地址 newg.stackguard0 = newg.stack.lo + _StackGuard newg.stackguard1 = ^uintptr(0) // Clear the bottom word of the stack. We record g // there on gsignal stack during VDSO on ARM and ARM64. *(*uintptr)(unsafe.Pointer(newg.stack.lo)) = 0 } return newg } Note 注意 systemstack()，用于将当前栈切换到 M 的 g0 协程栈上执行命令。 Why? 因为 G 用于执行用户逻辑，而某些管理操作不方便在 G 栈上执行（例如 G 可能中途停止，垃圾回收时 G 栈空间也有可能被回收），所以需要执行管理命令时，都会通过 systemstack 方法将线程栈切换为 g0 的栈执行，与用户逻辑隔离。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.2 栈的分配 stackalloc() 函数用于分配一个栈，无论是给新 G 还是扩容栈时都会用到，因此栈空间的分配与回收是一个比较频繁的操作，所以栈空间采取了缓存复用的方式。 主要逻辑如下： 如果分配的栈空间不大，就走缓存复用这种方式分配。没有可以复用的就创建； 如果分配的栈空间很大（大于 32KB），就直接从 heap 分配； 这里主要关注第 1 中方式，会调用 stackpoolalloc() 函数。 // Allocates a stack from the free pool. Must be called with // stackpool[order].item.mu held. func stackpoolalloc(order uint8) gclinkptr { list := \u0026stackpool[order].item.span s := list.first // ... if s == nil { // 没有可以复用的栈，走内存管理创建 s = mheap_.allocManual(_StackCacheSize\u003e\u003e_PageShift, \u0026memstats.stacks_inuse) // ... } x := s.manualFreeList if x.ptr() == nil { throw(\"span has no free stacks\") } s.manualFreeList = x.ptr().next // ... return x } 可以看到，首先尝试从 stackpool 缓存的空闲的 stack 获取，如果没有则走 Go 内存管理申请一个。 再接下来就是 Go 内存管理模块负责的事了，不深入下去（后面再说）。底层创建都是使用 mmap 系统调用实现的，这里可以看下使用的参数： // Don't split the stack as this method may be invoked without a valid G, which // prevents us from allocating more stack. //go:nosplit func sysAlloc(n uintptr, sysStat *uint64) unsafe.Pointer { p, err := mmap(nil, n, _PROT_READ|_PROT_WRITE, _MAP_ANON|_MAP_PRIVATE, -1, 0) if err != 0 { if err == _EACCES { print(\"runtime: mmap: access denied\\n\") exit(2) } if err == _EAGAIN { print(\"runtime: mmap: too much locked memory (check 'ulimit -l').\\n\") exit(2) } return nil } mSysStatInc(sysStat, n) return p } 通过 mmap 调用的参数可以看到，申请了一个系统分配的匿名内存映射。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.3 栈的扩容 3.3.1 扩容判断 Go 编译器会在执行函数前，插入一些汇编指令，其中一个功能就是检查 G 栈是否需要扩容。看一个函数调用的实现： // main() 调用 test() $ go build -gcflags \"-l\" -o test main.go $ go tool objump -s \"main\\.test\" test TEXT main.test(SB) /root/yusihao/onething/BizImages/main.go main.go:3 0x45dc80 MOVQ FS:0xfffffff8, CX // CX 为当前 G 地址 main.go:3 0x45dc89 CMPQ 0x10(CX), SP // CX+0x10 执行 g.stackguard0 属性，与 SP 指针地址比较 main.go:3 0x45dc8d JBE 0x45dccf // 如果 SP \u003c=stackguard0 跳转到 0x45dccf，也就是调用 runtime.morestack_noctxt(SB) 函数 main.go:3 0x45dc8f SUBQ $0x18, SP // ... main.go:5 0x45dcce RET // 函数执行结束，RET 返回，不会执行后面两个指令 main.go:3 0x45dccf CALL runtime.morestack_noctxt(SB) // 执行栈扩容 main.go:3 0x45dcd4 MP main.test(SB) // 执行结束后，重新执行当前函数 逻辑很简单，如果 SP \u003c= stackguard0，那么就执行栈的扩容，扩容结束重新执行当前函数。 Note 上面比较 SP 时候，没有考虑当前函数调用使用的空间大小。Why? 因为测试程序这个函数中使用的空间比较小，而 stackguard0 与 stack.lo 有一段保护区，所以编译器允许这里 “溢出” 一些，所以这里就没有让 SP 考虑函数使用空间。 如果函数中使用的空间大过保护区时，比较时就会让 SP 减去当前函数使用空间再比较了。 3.3.2 扩容 扩容逻辑大致分为三步： 分配一个 2x 新栈； 拷贝当前栈数据至新栈； “释放\"掉旧栈； 从上面扩容判断可以看到，会调用 morestack 的汇编代码： // Called during function prolog when more stack is needed. // R3 prolog's LR // using NOFRAME means do not save LR on stack. // // The traceback routines see morestack on a g0 as being // the top of a stack (for example, morestack calling newstack // calling the scheduler calling newm calling gc), so we must // record an argument size. For that purpose, it has no arguments. TEXT runtime·morestack(SB),NOSPLIT|NOFRAME,$0-0 // Cannot grow scheduler stack (m-\u003eg0). MOVW g_m(g), R8 MOVW m_g0(R8), R4 CMP g, R4 BNE 3(PC) BL runtime·badmorestackg0(SB) B runtime·abort(SB) // Cannot grow signal stack (m-\u003egsignal). MOVW m_gsignal(R8), R4 CMP g, R4 BNE 3(PC) BL runtime·badmorestackgsignal(SB) B runtime·abort(SB) // Called from f. // Set g-\u003esched to context in f. MOVW R13, (g_sched+gobuf_sp)(g) MOVW LR, (g_sched+gobuf_pc)(g) MOVW R3, (g_sched+gobuf_lr)(g) MOVW R7, (g_sched+gobuf_ctxt)(g) // Called from f. // Set m-\u003emorebuf to f's caller. MOVW R3, (m_morebuf+gobuf_pc)(R8) // f's caller's PC MOVW R13, (m_morebuf+gobuf_sp)(R8) // f's caller's SP MOVW g, (m_morebuf+gobuf_g)(R8) // Call newstack on m-\u003eg0's stack. MOVW m_g0(R8), R0 BL setg\u003c\u003e(SB) MOVW (g_sched+gobuf_sp)(g), R13 MOVW $0, R0 MOVW.W R0, -4(R13) // create a call frame on g0 (saved LR) BL runtime·newstack(SB) // Not reached, but make sure the return PC from the call to newstack // is still in this function, and not the beginning of the next. RET TEXT runtime·morestack_noctxt(SB),NOSPLIT|NOFRAME,$0-0 MOVW $0, R7 B runtime·morestack(SB) 可以看到 g0，gsignal 的栈都不会扩容 在 g0 栈上会调用 newstack() 函数 调用的 newstack() 函数（runtime/stack.go），过程很复杂，只看一下关键点： // Called from runtime·morestack when more stack is needed. // Allocate larger stack and relocate to new stack. // Stack growth is multiplicative, for constant amortized cost. // // g-\u003eatomicstatus will be Grunning or Gscanrunning upon entry. // If the scheduler is trying to stop this g, then it will set preemptStop. // // This must be nowritebarrierrec because it can be called as part of // stack growth from other nowritebarrierrec functions, but the // compiler doesn't check this. // //go:nowritebarrierrec func newstack() { thisg := getg() gp := thisg.m.curg // ... // 新栈大小为当前两倍 oldsize := gp.stack.hi - gp.stack.lo newsize := oldsize * 2 // ... // 改变 G 状态为 copy stack，gc 会跳过该状态的 G casgstatus(gp, _Grunning, _Gcopystack) // 分配新栈，拷贝数据，释放旧站 // The concurrent GC will not scan the stack while we are doing the copy since // the gp is in a Gcopystack status. copystack(gp, newsize) if stackDebug \u003e= 1 { print(\"stack grow done\\n\") } casgstatus(gp, _Gcopystack, _Grunning) // 执行 G 代码 gogo(\u0026gp.sched) } // Copies gp's stack to a new stack of a different size. // Caller must have changed gp status to Gcopystack. func copystack(gp *g, newsize uintptr) { // 创建新 stack new := stackalloc(uint32(newsize)) // ... // 拷贝数据 memmove(unsafe.Pointer(new.hi-ncopy), unsafe.Pointer(old.hi-ncopy), ncopy) // ... gp.stack = new gp.stackguard0 = new.lo + _StackGuard // 释放旧 stack if stackPoisonCopy != 0 { fillstac","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:3","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.4 栈的释放 stackfree 栈的释放与申请相反，放入 stackpool，或者直接调用内存管理删除，重点还是内存管理的活，所以这里不展开。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:4","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.5 栈的切换 切换应该是属于 goroutine 调度的内容，不过这里可以关注一下栈时如何切换的。 当 M 执行的 G 需要切换，或者一个新创建 G 执行时，最后都会调用 execute() 函数，而 execute() 函数会调用 gogo 汇编实现的函数。 // func gogo(buf *gobuf) // restore state from Gobuf; longjmp TEXT runtime·gogo(SB), NOSPLIT, $16-8 MOVQ buf+0(FP), BX // gobuf MOVQ gobuf_g(BX), DX MOVQ 0(DX), CX // make sure g != nil get_tls(CX) MOVQ DX, g(CX) MOVQ gobuf_sp(BX), SP // restore SP （关键！) MOVQ gobuf_ret(BX), AX MOVQ gobuf_ctxt(BX), DX MOVQ gobuf_bp(BX), BP MOVQ $0, gobuf_sp(BX) // clear to help garbage collector MOVQ $0, gobuf_ret(BX) MOVQ $0, gobuf_ctxt(BX) MOVQ $0, gobuf_bp(BX) MOVQ gobuf_pc(BX), BX JMP BX gobuf 中保存着要执行的 G 的 sp、pc 指针，可以看到通过将对应 gobuf.sp 写入到 SP 寄存器中，也就是将使用的栈切换为了 G 的栈。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:5","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.6 g0 的栈 在阅读网上的文章时，许多文章都说 g0 使用的是系统栈，我理解为使用的是进程的栈内存区域。但是思考一下，每个 M 对应一个 g0，也就是说有多个线程要同时共享系统栈，这是不可能的。例如在 pthread 实现中，对应新建的线程也是使用 mmap 分配一个内存区域，然后调用 clone() 系统调用时传入栈地址参数。 看一下代码，确认一下到底 g0 的栈到底是啥，找到一个新建 m 的地方： mp := new(m) mp.mstartfn = fn mcommoninit(mp, id) // In case of cgo or Solaris or illumos or Darwin, pthread_create will make us a stack. // Windows and Plan 9 will layout sched stack on OS stack. if iscgo || GOOS == \"solaris\" || GOOS == \"illumos\" || GOOS == \"windows\" || GOOS == \"plan9\" || GOOS == \"darwin\" { mp.g0 = malg(-1) } else { mp.g0 = malg(8192 * sys.StackGuardMultiplier) } mp.g0.m = mp 可以看到，m 的 g0 属性还是使用的 [malg() 函数](#31-新-g-的栈） 去创建的，与普通的 g 创建一样，只不过初始大小为 8KB。malg() 流程上面有说到，就是走内存管理分配 mspan 作为栈的方式。 不过，g0 的栈还是有些不同的，不会进行栈的扩容（因为仅仅内部管理时用到，不需要进行自动扩容），在栈扩容的 morestack 汇编代码 里可以看到。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:6","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4 内存模型 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.1 概览 Golang 内存管理包含四个组件： object ：object 代表用户代码申请的一个对象，没有实际的数据结构，而是在 mspan 中以逻辑切分的方式分配； page：切分内存的单元，mheap 将内存以 8KB page 切分，然后组合成为 mspan； runtime.mspan ：内存管理的最小单元，由多个 8KB 大小的 page 构成，按照固定大小来切分为多个 object； runtime.mcache ：单个 P 对应的 mspan 的缓存，无锁分配； runtime.mcentral ：按照不同大小的 mspan 分组的管理链表，为 mcache 提供空闲 mspan runtime.mheap ：保存闲置的 mspan 与 largerspan 链表，与操作系统申请与释放内存； 上面的组件也可以看做分层，普通对象（object）的申请与释放就是按照上下层顺序申请与释放的。 Note 下面不会说（后面再说）具体的 object 分配流程，而是说明各个层次时的申请与释放操作。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.2 mspan 每个 mspan 由多个 8KB 的 page 组成，所有的 mspan 会以 list 的方式构建，而不同的模块（mcache、mcentral）通过引用指针，来不同方式来组织不同的 mspan。 每个 mspan 管理多个固定大小的 object，通过编号 (index) 方式来寻找 object 的地址。 结构如下图所示： 其数据结构如下（省略部分）： type mspan struct { next *mspan // next span in list, or nil if none  prev *mspan // previous span in list, or nil if none startAddr uintptr // address of first byte of span aka s.base()  npages uintptr // number of pages in span manualFreeList gclinkptr // list of free objects in mSpanManual spans  freeindex uintptr nelems uintptr // number of object in the span.  allocCache uint64 allocBits *gcBits gcmarkBits *gcBits // sweep generation: // if sweepgen == h-\u003esweepgen - 2, the span needs sweeping // if sweepgen == h-\u003esweepgen - 1, the span is currently being swept // if sweepgen == h-\u003esweepgen, the span is swept and ready to use // if sweepgen == h-\u003esweepgen + 1, the span was cached before sweep began and is still cached, and needs sweeping // if sweepgen == h-\u003esweepgen + 3, the span was swept and then cached and is still cached // h-\u003esweepgen is incremented by 2 after every GC  sweepgen uint32 spanclass spanClass // size class and noscan (uint8)  allocCount uint16 // number of allocated objects  elemsize uintptr // computed from sizeclass or from npages } next、prev ：链表前后 span； startAddr ：span 在 arena 区域的起始地址； npages ：占用 page(8KB) 数量； manualFreeList ：空闲 object 链表； freeindex ：下一个空闲的 object 的编号，如果 freeindex == nelem，表明没有空闲 object 可以分配 nelems ：当前 span 中分配的 object 的上限； allocCache ：freeindex 的 cache，通过 bitmap 的方式记录对应编号的 object 内存是否是空闲的； allocBits : 通过 bitmap 标识哪些编号的 object 是分配出去的； gcmarkBits : 经过 GC 后，gcmarkBits 标识出的 object 就是被 mark 的，没有 mark 的变为垃圾对象清除； sweepgen ：mspan 的状态，见注释； spanclass ：mspan 大小类别； allocCount ：已经分配的 object 数量； elemsize ：管理的 object 的固定大小； 可以看到，每个 mspan 管理着固定大小的 object，并通过一个 freeindex+allocCache 来记录空闲的 object 的编号。由此可以得出： mspan 的地址区域: [startAddr, startAddr + npages*8*1024) 某个 object 的起始地址: \u003cindex\u003e*elemsize + startAddr 4.2.1 object 分配 在创建新的 object 时，对于普通大小的 object 分配（16\u003csize\u003c32KB)，会在从 mcache 中选出具有空闲空间的 mspan，然后记录到 mspan.allocCache 中。 具体代码如下，nextFreeIndex() 函数就是用于得到下一个空闲 object，并移动 freeindex（runtime/mbitmap.go)： // nextFreeIndex returns the index of the next free object in s at // or after s.freeindex. // There are hardware instructions that can be used to make this // faster if profiling warrants it. func (s *mspan) nextFreeIndex() uintptr { sfreeindex := s.freeindex snelems := s.nelems if sfreeindex == snelems { return sfreeindex } aCache := s.allocCache bitIndex := sys.Ctz64(aCache) for bitIndex == 64 { // Move index to start of next cached bits.  sfreeindex = (sfreeindex + 64) \u0026^ (64 - 1) if sfreeindex \u003e= snelems { s.freeindex = snelems return snelems } whichByte := sfreeindex / 8 // Refill s.allocCache with the next 64 alloc bits.  s.refillAllocCache(whichByte) aCache = s.allocCache bitIndex = sys.Ctz64(aCache) // nothing available in cached bits  // grab the next 8 bytes and try again.  } result := sfreeindex + uintptr(bitIndex) if result \u003e= snelems { s.freeindex = snelems return snelems } s.allocCache \u003e\u003e= uint(bitIndex + 1) sfreeindex = result + 1 if sfreeindex%64 == 0 \u0026\u0026 sfreeindex != snelems { // We just incremented s.freeindex so it isn't 0.  // As each 1 in s.allocCache was encountered and used for allocation  // it was shifted away. At this point s.allocCache contains all 0s.  // Refill s.allocCache so that it corresponds  // to the bits at s.allocBits starting at s.freeindex.  whichByte := sfreeindex / 8 s.refillAllocCache(whichByte) } s.freeindex = sfreeindex return result } 注意：目前跳过了 “nextFreeFast” 实现，该获取 span 比 “nextFree” 更快，使用了 mspan.allocCache。 4.2.2 mspan 的清理 mspan.sweep() 用于进行一个 mspan 的清理，我们先看下旧版本的实现 mspan.oldSweep()： // Sweep frees or collects finalizers for blocks not marked in the mark phase. // It clears the mark bits in preparation for the next GC round. // Returns true if the span was returned to heap. // If preserve=true, don't return it to heap nor relink in mcentral lists; // caller takes care of it. /","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.3 mcache 每个 P 拥有一个 mcache，mcache 中保存着具有空闲空间的 mspan，用于分配 object 时，不需要加锁即可从 mspan 分配对象。 有两种 object 走 mcache 分配： tiny object：mcache 还单独使用一个 mspan 进行非指针微小对象的分配。与普通 object 对象分配不同的是，tiny object 不是固定大小分配的，而是通过 mcache 记录其 offset 偏移量，让 tiny object “挤在” 同一个 mspan 中。 normal object：普通大小的 object，会使用 mcache.alloc 进行分配。mcache.alloc 包含 134 个数组项（67 sizeclass * 2），对于每个大小规格的 mspan 有着两个类型： scan：包含指针的对象 noscan：不包含指针的对象，GC 时无需进一步扫描是否引用着其他活跃对象 mcache “永远” 有空闲的 mspan 用于 object 的分配，当 mcache 缓存的 mspan 没有空闲空间时，就会找 mcentral 去申请新的 mspan 用于使用。 数据结构如下（runtime/mcache.go）： // Per-thread (in Go, per-P) cache for small objects. // No locking needed because it is per-thread (per-P). // // mcaches are allocated from non-GC'd memory, so any heap pointers // must be specially handled. type mcache struct { // Allocator cache for tiny objects w/o pointers.  // See \"Tiny allocator\" comment in malloc.go. // tiny points to the beginning of the current tiny block, or  // nil if there is no current tiny block.  //  // tiny is a heap pointer. Since mcache is in non-GC'd memory,  // we handle it by clearing it in releaseAll during mark  // termination.  tiny uintptr tinyoffset uintptr local_tinyallocs uintptr // number of tiny allocs not counted in other stats // The rest is not accessed on every malloc. alloc [numSpanClasses]*mspan // spans to allocate from, indexed by spanClass stackcache [_NumStackOrders]stackfreelist } tiny tinyoffset ：用于小对象（\u003c16）的分配。tiny 指向当前为 tiny object 准备的 span 的起始地址，tinyoffset 指向对象使用的偏移地址； alloc ：最重要的属性，保存着不同大小的 mspan 各一个。目前，包含固定 64 类 sizeclass：0、8 … 32768； 4.3.1 mspan 的分配 先看下 tiny object 分配，在分配一个 object 时，如果大小小于 16 字节时，就会走 tiny object 逻辑。 func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { if size \u003c= maxSmallSize { if noscan \u0026\u0026 size \u003c maxTinySize { off := c.tinyoffset // Align tiny pointer for required (conservative) alignment. if size\u00267 == 0 { off = alignUp(off, 8) } else if size\u00263 == 0 { off = alignUp(off, 4) } else if size\u00261 == 0 { off = alignUp(off, 2) } if off+size \u003c= maxTinySize \u0026\u0026 c.tiny != 0 { // The object fits into existing tiny block. x = unsafe.Pointer(c.tiny + off) c.tinyoffset = off + size c.local_tinyallocs++ mp.mallocing = 0 releasem(mp) return x } // Allocate a new maxTinySize block. span = c.alloc[tinySpanClass] v := nextFreeFast(span) if v == 0 { v, span, shouldhelpgc = c.nextFree(tinySpanClass) } x = unsafe.Pointer(v) (*[2]uint64)(x)[0] = 0 (*[2]uint64)(x)[1] = 0 // See if we need to replace the existing tiny block with the new one // based on amount of remaining free space. if size \u003c c.tinyoffset || c.tiny == 0 { c.tiny = uintptr(x) c.tinyoffset = size } size = maxTinySize } else { ... } } else { ... } } 如果当前 tinyoffset+size \u003c 16B（因为每个 tiny span 的大小为 32B，而每个 tiny object 最大为 16，所以比较 16 即可），那么表明当前的 tiny span 肯定可以放下，那么移动 c.tinyoffset 偏移即可； 如果没有，那么就需要重新申请 tinySpanClass=5 的 span（32B），并替换当前 c.tiny 与 c.tinyoffset（当前 object 可以放在老的 span 或者新的 span）。 接着看下普通大小的 object 的分配，在外层函数计算好 spanClass 后，就会调用 nextFree() 函数（runtime/malloc.go）： // nextFree returns the next free object from the cached span if one is available. // Otherwise it refills the cache with a span with an available object and // returns that object along with a flag indicating that this was a heavy // weight allocation. If it is a heavy weight allocation the caller must // determine whether a new GC cycle needs to be started or if the GC is active // whether this goroutine needs to assist the GC. // // Must run in a non-preemptible context since otherwise the owner of // c could change. func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool) { s = c.alloc[spc] shouldhelpgc = false freeIndex := s.nextFreeIndex() if freeIndex == s.nelems { // The span is full.  if uintptr(s.allocCount) != s.nelems { println(\"runtime: s.allocCount=\", s.allocCount, \"s.nelems=\", s.nelems) throw(\"s.allocCount != s.nelems \u0026\u0026 freeIndex == s.nelems\") } c.refill(spc) shouldhelpgc = true s = c.alloc[spc] freeIndex = s.nextFreeIndex() } if freeIndex \u003e= s.nele","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:3","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.4 mcentral mcentral 是内存分配器的中心缓存，用于给 mcache 提供空闲的 mspan。因为不是 P 对应的，所以访问也需要锁。 mheap 会创建 64 个 sizeClass 的 mcentral，每个 mcentral 管理相同大小的所有 mspan，以两个链表结构管理： nonempty ：包含空闲空间的 mspan 组成的链表； empty ：不包含空闲空间，或者被 mcache 申请的 mspan 组成的链表（判断是否被 mcache 使用是通过 mspan.sweepgen 属性来判断）； 当 mcache 要申请某大小的 mspan 时，会回去指定大小的 mcentral 实例上申请。 数据结构如下： // Central list of free objects of a given size. // //go:notinheap type mcentral struct { lock mutex spanclass spanClass // For !go115NewMCentralImpl. nonempty mSpanList // list of spans with a free object, ie a nonempty free list empty mSpanList // list of spans with no free objects (or cached in an mcache) … } lock ：访问需要加的锁； spanclass ：当前 mcentral 管理的 mspan 大小； nonempty ：包含空闲空间的 mspan 链表； empty ：不包含空闲空间，或者被 mcache 申请了的 mspan 链表； Note 源码中存在 go115NewMCentralImpl 的注释，对 mcentral 结构做了很大的改动，但是在 go1.15 release 页面上并没有看到对应的说明。 其 commit 见：runtime: add new mcentral implementation 4.4.1 从 mcentral 申请 mspan 在 [mcache 的获取](#432-mspan-的获取） 中，可以看到 mcache 通过调用 mcentral.cacheSpan() 申请新的空闲 mspan。在 go1.15 中，因为有新版 mcentral 的实现，因此双链表方式移动到了 mcentral.oldCacheSpan() 方法中。 // Allocate a span to use in an mcache. func (c *mcentral) cacheSpan() *mspan { if !go115NewMCentralImpl { return c.oldCacheSpan() } … } // Allocate a span to use in an mcache. // // For !go115NewMCentralImpl. func (c *mcentral) oldCacheSpan() *mspan { lock(\u0026c.lock) sg := mheap_.sweepgen retry: var s *mspan // 走 nonempty 链表找 for s = c.nonempty.first; s != nil; s = s.next { if s.sweepgen == sg-2 \u0026\u0026 atomic.Cas(\u0026s.sweepgen, sg-2, sg-1) { c.nonempty.remove(s) c.empty.insertBack(s) unlock(\u0026c.lock) s.sweep(true) goto havespan } if s.sweepgen == sg-1 { // the span is being swept by background sweeper, skip continue } // we have a nonempty span that does not require sweeping, allocate from it c.nonempty.remove(s) c.empty.insertBack(s) unlock(\u0026c.lock) goto havespan } // 走 empty 链表找 for s = c.empty.first; s != nil; s = s.next { if s.sweepgen == sg-2 \u0026\u0026 atomic.Cas(\u0026s.sweepgen, sg-2, sg-1) { // we have an empty span that requires sweeping, // sweep it and see if we can free some space in it c.empty.remove(s) // swept spans are at the end of the list c.empty.insertBack(s) unlock(\u0026c.lock) s.sweep(true) freeIndex := s.nextFreeIndex() if freeIndex != s.nelems { s.freeindex = freeIndex goto havespan } lock(\u0026c.lock) // the span is still empty after sweep // it is already in the empty list, so just retry goto retry } if s.sweepgen == sg-1 { // the span is being swept by background sweeper, skip continue } // already swept empty span, // all subsequent ones must also be either swept or in process of sweeping break } unlock(\u0026c.lock) // 向 heap 申请新的 mspan // Replenish central list if empty. s = c.grow() if s == nil { return nil } lock(\u0026c.lock) c.empty.insertBack(s) unlock(\u0026c.lock) // At this point s is a non-empty span, queued at the end of the empty list, // c is unlocked. havespan: … freeByteBase := s.freeindex \u0026^ (64 - 1) whichByte := freeByteBase / 8 // Init alloc bits cache. s.refillAllocCache(whichByte) // Adjust the allocCache so that s.freeindex corresponds to the low bit in // s.allocCache. s.allocCache \u003e\u003e= s.freeindex % 64 return s } 上面逻辑可以大致分为几个步骤： 遍历 nonempty 链表，找到可用的 mspan（对于需要 sweep 的 mspan 先进行 sweep）； 没找到，遍历 empty 链表，仅仅遍历需要 sweep 的 mspan，执行 sweep 并判断是否可用； 还是没有，通过 mcentral.grow() 向 mheap 申请新的 mspan，mheap 中都没有，return nil； 找到空闲 mspan 后，会放置到 empty 链表尾部，并返回； 4.4.2 mcentral 扩容 在 mcentral 没有任何空闲 mspan 给 mcache 时，就会调用 mcentral.grow() 申请新的 mspan。 // grow allocates a new empty span from the heap and initializes it for c's size class. func (c *mcentral) grow() *mspan { npages := uintptr(class_to_allocnpages[c.spanclass.sizeclass()]) size := uintptr(class_to_size[c.spanclass.sizeclass()]) s := mheap_.alloc(npages, c.spanclass, true) if s == nil { return nil } // Use division by multiplication and shifts to quickly compute: // n := (npages \u003c\u003c _PageShift) / size n := (npages \u003c\u003c _PageShift) \u003e\u003e s.divShift * uintptr(s.divMul) \u003e\u003e s.divShift2 s.limit = s.base() + size*n heapBitsForAddr(s.b","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:4","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.5 mheap mheap 是最核心的组件了，runtime 只存在一个 mheap 对象，分配、初始化 mspan 都从 mheap 开始。 mheap 直接与虚拟内存打交道，并将在虚拟内存上创建 mspan 提供给上层使用。 mheap 的功能可以看做两个方面： 与下层（虚拟内存）：内存管理（类似文件系统）。申请虚拟内存得到多个 heaparena，每个 heaparena 将可用内存区域切分为 page 单元，以倍数组成 mspan 分配给上层； 与上层：提供创建 mspan 的接口。通过 mcentral 分类不同大小的 mspan，或者大内存需要直接走 mspan 分配； 其数据结构很大，省略了部分不会提到的属性（runtime/mheap.go），mheap_ 就是 heap 的单实例对象： var mheap_ mheap // Main malloc heap. // The heap itself is the \"free\" and \"scav\" treaps, // but all the other global data is here too. // // mheap must not be heap-allocated because it contains mSpanLists, // which must not be heap-allocated. // //go:notinheap type mheap struct { // arenas is the heap arena map. It points to the metadata for // the heap for every arena frame of the entire usable virtual // address space. // // Use arenaIndex to compute indexes into this array. // // For regions of the address space that are not backed by the // Go heap, the arena map contains nil. // // Modifications are protected by mheap_.lock. Reads can be // performed without locking; however, a given entry can // transition from nil to non-nil at any time when the lock // isn't held. (Entries never transitions back to nil.) // // In general, this is a two-level mapping consisting of an L1 // map and possibly many L2 maps. This saves space when there // are a huge number of arena frames. However, on many // platforms (even 64-bit), arenaL1Bits is 0, making this // effectively a single-level map. In this case, arenas[0] // will never be nil. arenas [1 \u003c\u003c arenaL1Bits]*[1 \u003c\u003c arenaL2Bits]*heapArena // central free lists for small size classes. // the padding makes sure that the mcentrals are // spaced CacheLinePadSize bytes apart, so that each mcentral.lock // gets its own cache line. // central is indexed by spanClass. central [numSpanClasses]struct { mcentral mcentral pad [cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize]byte } pages pageAlloc // page allocation data structure spanalloc fixalloc // allocator for span* cachealloc fixalloc // allocator for mcache* specialfinalizeralloc fixalloc // allocator for specialfinalizer* specialprofilealloc fixalloc // allocator for specialprofile* speciallock mutex // lock for special record allocators. arenaHintAlloc fixalloc // allocator for arenaHints } arenas ：内存管理的元信息数组，对于虚拟内存的逻辑切割与管理就靠这个数组了； central ：按照大小分类的各个 mcentral 对象； pages ：在 arena 区域上用于分配空闲的 pages，依旧使用空闲链表； spanalloc、cachealloc 等 ：各个数据结构的空闲链表分配器，通过连接空闲的 mspan、mcache 等对象，调用 fixalloc.alloc() 函数就获取下一个空闲的内存空间； 4.5.1 虚拟内存布局 网上大部分文章还是说 mheap 管理的虚拟内存以 spans+bitmap+arena 管理，如下图： 但是从 go 1.11 开始，Go 开始使稀疏内存方式管理，即管理相互之间不连续的连续的内存区域，如下图（图片来自 《Golang 设计与实现》）： 使用的就是 mheap.arenas，一个二维的 heapArena 数组。 不同平台的 heapArena 管理的 arena 大小不同，在 Linux 64bit 平台下，每个 heapArena 管理着 64MB 的 arena 内存区域。 // Currently, we balance these as follows: // // Platform Addr bits Arena size L1 entries L2 entries // -------------- --------- ---------- ---------- ----------- // */64-bit 48 64MB 1 4M (32MB) // windows/64-bit 48 4MB 64 1M (8MB) // */32-bit 32 4MB 1 1024 (4KB) // */mips(le) 31 4MB 1 512 (2KB) Note 这里不太好理解，但是我觉可以简单理解就是，将原来的 spans+bitmap+arena 管理方式，变为了多个 spans+bitmap+arena 实现。而不同 arena 之间的地址不是连续的。 但是为什么要用二维数组？目前不知道，但是 Linux x86-64 架构上一维数组大小为 1，就是相当于 1 维数组。 heapArena 数据结构如下（runtime/mheap.go）： // A heapArena stores metadata for a heap arena. heapArenas are stored // outside of the Go heap and accessed via the mheap_.arenas index. // //go:notinheap type heapArena struct { // bitmap stores the pointer/scalar bitmap for the words in // this arena. See mbitmap.go for a description. Use the // heapBits type to access this. bitmap [heapArenaBitmapBytes]byte // spans maps from virtual address page ID within this arena to *mspan. // For allocated spans, their pages map to the span itself. // For free spans, only the lowest and highest pages map to the span itself. // Internal pages map to an arbitrary span. // For pages that have never been allocated, spans entries are nil. // // Modifications are protected by m","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:5","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.6 总结 粗略地看完整个内存模型后，大概内存的结构如下： 其中比较核心的就是：内存被切分为 page，多个 page 组成不同大小的 mspan，而在 mspan 上又分割为固定大小的 object。 而上层的 mcache、mcentral 只是以不同的方式组织 mspan，通过多级缓存的思想，使得并发的获取一个可用的 mspan 更快。 mcache 将一部分 mspan 独立于 P 所有，使得不需要加锁既可以获取 mspan； mcentral 以大小来分类 mspan，将各个大小的 mspan 请求独立，缩小了锁的粒度； mheap 作为最底层，就好像文件系统一样，管理着整个内存分配的骨架。而与上层的交互就是靠 mspan 作为单位。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:6","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"5 对象分配流程 前面一直提到的，对象的分配分为三类： tiny object (0, 16B): 使用 tiny allocator 分配，使用 mcahe 一个独立的 mspan，挤压式的； object [16B, 32KB]: 使用 mcache 分配； large object (32KB, +∞): 直接通过 mheap 分配； 所有的分配逻辑在 mallocgc() 开始分叉，下面分别看下具体的分配代码。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:5:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"5.1 tiny object 分配 tiny object 分配的代码在 mspan 分配中已经说明了，这里再理一下大致步骤： 不包含指针 (noscan) 并且小于 16B 的对象才走微小对象分配； tiny object 分配仅仅是增大 mcache.tinyoffset 的值，所以是不同大小 tiny object 挤压在一个 mspan 中； 如果当前的 mspan 没有空间了，通过 mcache.nextFree() 来获取新的指定大小的 mspan，而获取的流程就是前面所说的（走 mcentral-\u003emheap); ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:5:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"5.2 object 分配 普通大小 object 分配流程就很简单了： func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { if size \u003c= maxSmallSize { if noscan \u0026\u0026 size \u003c maxTinySize { ... } else { var sizeclass uint8 if size \u003c= smallSizeMax-8 { sizeclass = size_to_class8[divRoundUp(size, smallSizeDiv)] } else { sizeclass = size_to_class128[divRoundUp(size-smallSizeMax, largeSizeDiv)] } size = uintptr(class_to_size[sizeclass]) spc := makeSpanClass(sizeclass, noscan) span = c.alloc[spc] v := nextFreeFast(span) if v == 0 { v, span, shouldhelpgc = c.nextFree(spc) } x = unsafe.Pointer(v) if needzero \u0026\u0026 span.needzero != 0 { memclrNoHeapPointers(unsafe.Pointer(v), size) } } } else { ... } } 计算出对应的 sizeclass； 从 mcache.alloc[] 得到对应的 mspan。如果没有，通过 nextFree() 申请； 调用 memclrNoHeapPointers() 清理空闲内存中所有数据； ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:5:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"5.3 large object 分配 func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { if size \u003c= maxSmallSize { if noscan \u0026\u0026 size \u003c maxTinySize { ... } else { ... } } else { shouldhelpgc = true systemstack(func() { span = largeAlloc(size, needzero, noscan) }) span.freeindex = 1 span.allocCount = 1 x = unsafe.Pointer(span.base()) size = span.elemsize } } func largeAlloc(size uintptr, needzero bool, noscan bool) *mspan { // print(\"largeAlloc size=\", size, \"\\n\") if size+_PageSize \u003c size { throw(\"out of memory\") } npages := size \u003e\u003e _PageShift if size\u0026_PageMask != 0 { npages++ } // Deduct credit for this span allocation and sweep if // necessary. mHeap_Alloc will also sweep npages, so this only // pays the debt down to npage pages. deductSweepCredit(npages*_PageSize, npages) spc := makeSpanClass(0, noscan) s := mheap_.alloc(npages, spc, needzero) if s == nil { throw(\"out of memory\") } if go115NewMCentralImpl { // Put the large span in the mcentral swept list so that it's // visible to the background sweeper. mheap_.central[spc].mcentral.fullSwept(mheap_.sweepgen).push(s) } s.limit = s.base() + size heapBitsForAddr(s.base()).initSpan(s) return s } large object 会切换到系统栈，然后走 mheap 申请； 计算对象需要的 page 数量，然后调用 mheap.alloc() 申请空闲的 mspan； 而 mheap.alloc() 就是 mcentral 申请 mspan 的方法。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:5:3","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"6 内存的释放 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:6:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"6.1 释放操作 前面 4.5.4 mheap 回收 mspan 中看到，mheap 不会真正的释放内存，而是等待其被复用。但是不可能一直扩展内存，而不释放。 释放内存由 mheap.page 的 pageAlloc.scavenge() 函数负责（runtime/mgcscavenge.go）: // scavenge scavenges nbytes worth of free pages, starting with the // highest address first. Successive calls continue from where it left // off until the heap is exhausted. Call scavengeStartGen to bring it // back to the top of the heap. // // Returns the amount of memory scavenged in bytes. // // s.mheapLock must be held, but may be temporarily released if // mayUnlock == true. // // Must run on the system stack because s.mheapLock must be held. // //go:systemstack func (s *pageAlloc) scavenge(nbytes uintptr, mayUnlock bool) uintptr { var ( addrs addrRange gen uint32 ) released := uintptr(0) for released \u003c nbytes { if addrs.size() == 0 { // 通过标记选出一部分需要释放的内存区域 if addrs, gen = s.scavengeReserve(); addrs.size() == 0 { break } } // 释放内存 r, a := s.scavengeOne(addrs, nbytes-released, mayUnlock) released += r addrs = a } // Only unreserve the space which hasn't been scavenged or searched // to ensure we always make progress. s.scavengeUnreserve(addrs, gen) return released } 释放的流程比较复杂，没有研究过看不懂，目前知道下最后会调用 sysUnused() 函数释放（runtime/mem_linux.go）： func sysUnused(v unsafe.Pointer, n uintptr) { // huge page 处理 ... var advise uint32 if debug.madvdontneed != 0 { advise = _MADV_DONTNEED } else { advise = atomic.Load(\u0026adviseUnused) } if errno := madvise(v, n, int32(advise)); advise == _MADV_FREE \u0026\u0026 errno != 0 { // MADV_FREE was added in Linux 4.5. Fall back to MADV_DONTNEED if it is // not supported. atomic.Store(\u0026adviseUnused, _MADV_DONTNEED) madvise(v, n, _MADV_DONTNEED) } } 通过系统调用 madvise() 告知操作系统某段内存不适用，建议内核回收对应物理内存。 当然，内核在物理内存充足情况下可能不会实际回收内存，以减少无谓的回收消耗。 而当再次使用此内存块时，会引发缺页异常，内核会自动重新关联物理内存页。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:6:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"6.2 释放时机 scavenge() 有两个地方会被调用： 周期性的触发（每 5 min?）； mheap 扩容时 或者 调用 runtime/debug.FreeOSMemory() 主动触发； ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:6:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"参考 《Golang 学习笔记》 Blog：Go 内存管理可视化 《Golang 设计与实现》：内存分配器 知乎：图解 Go 语言内存分配 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:7:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["VM"],"content":"总结 KVM 虚拟机使用存储与网络的方式","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文主要总结一些遇到的虚拟机使用存储与网络的方式，挖了许多坑，不断补充中。 下面所有的虚拟机启动都使用 libvirt，通过修改其配置文件来设置网络或者存储，省略了虚拟机的启动、停止等操作命令。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:0:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1 virtio 概述 KVM 在 IO 虚拟化方面，传统的方式是使用 QEMU 纯软件方式模拟网卡、磁盘。 KVM 中，可以在客户机使用 半虚拟化驱动Paravirtualized Drivers 来提高客户机性能。 目前，采用的是 virtio 这个设备驱动标准框架。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1.1 全模拟 I/O 设备基本原理 纯软件模拟 I/O 如下图所示： 客户机中设备驱动程序（Device Driver）发起 I/O 操作请求时，KVM 内核模块会拦截这次请求，然后将请求信息放到 I/O 共享页（sharing page）； KVM 模块通知用户空间 QEMU 程序（客户机对应 qemu 进程的一个线程）有 I/O 请求； QEMU 模拟程序根据 I/O 请求信息，交由硬件模拟代码（Emulation Code）模拟 I/O 请求，阻塞等待 I/O 结果； 如果是普通的 I/O 请求完成后，将结果放回到 I/O 共享页； 如果客户机是通过 DMA 访问大块 I/O 时，QEMU 模拟程序会直接通过内存映射的将结果直接写到客户机内存中，并通知 KVM 模块客户机 DMA 操作已经完成； KVM 内核模块读取 I/O 共享页中结果，将结果返回给客户机； 或者，接受到 QEMU 通知，通知客户机 DMA 操作已经完成； QEMU 模拟 I/O 设备的优点：可以通过软件模拟出各种设备，兼容性好。但是也有明显缺点：每次 I/O 操作比较长，有较多的 VMEntry、VMExit 发生，需要多次上下文切换与数据复制，性能很差。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:1","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1.2 virtio 基本原理 virtio 是一个在 Hypervisor 上抽象 API 接口，让客户机知道运行与虚拟化环境中，进而根据 virtio 标准与 Hypervisor 协作，从而在客户机达到更好性能。 其 virtio 基本结构如下： 前端驱动fronded 是在客户机中的驱动模块，如 virtio-blk、virtio-net 等； 后端处理程序backend 在 QEMU 中实现，执行具体的 I/O 操作； virtio 是虚拟队列接口，在逻辑上将前端驱动附加到后端处理程序，虚拟队列实际上被实现为跨越客户机操作系统和 Hypervisor 的衔接点，该衔接点可以用任意方式实现，前提是前后端都遵循标准实现。 一个前端驱动可以有 0 或多个队列，例如 virtio-net 使用两个虚拟队列（接受+发送）。 virtio-ring 实现了环形缓冲区（ring buffer），用于保存前端驱动和后端处理程序执行的信息。 环形缓冲区可以一次性保存前端驱动多个 I/O 请求，并交由后端驱动批量处理，最后实际调用宿主机设备驱动实现的设备 I/O 操作，以提升效率。 virtio 的性能很好，一般都推荐使用 virtio。但是，virtio 要求客户机必须安装特定的 virtio 驱动，并且按照 virtio 规定格式传输数据，一些老的 Linux 系统可能不支持。 在客户机中查看是否存在 virtio 相关内核模块，以确定系统是否支持 virtio： [root@kvm-guest ~]# lsmod | grep virtio virtio_net 28024 0 virtio_pci 22913 0 virtio_ring 21524 2 virtio_net,virtio_pci virtio 15008 2 virtio_net,virtio_pci 其中，virtio、virtio_ring、virtio_pci 等驱动提供对 virtio API 支持，任何 virtio 前端驱动都必须使用。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:2","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1.3 vhost 正如 前面所述，virtio 分为前后端，而后端默认为用户空间的 QEMU 进程。由 QEMU 进程模拟虚拟机的命令，在将结果返回给虚拟机。可以看到，这里存在着一次 用户空间至内核空间的转换。 而 vhost 出现就是用于优化这一次的转换。vhost 是宿主机中的一个内核模块，用于和虚拟机直接进行通信，也是通过 virtio 提供的数据队列进行通信。 目前，网络支持有 vhost-net，块设备支持有 vhost-blk，它们都依赖于基础的内核模块 vhost。 $ lsmod | grep vhost vhost_net 28672 1 vhost 53248 1 vhost_net vhost_iotlb 16384 1 vhost ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:3","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1.4 总结 首先，需要明白 virtio 为什么比纯模拟的方式快？ 最主要因为纯模拟的方式存在多次内核 KVM 与用户空间 QEMU 进程的数据交互，简单点说，客户机需要 KVM 进行数据的中转。 而 virtio 消除了 KVM 进行数据的中转，通过一套标准框架实现虚拟机与 QEMU 进程的直接信息交互。也是因此，虚拟机需要使用特殊的 virtio 相关的驱动，也就知道了自己处于虚拟化环境，所以是半虚拟化。 其次，需要明确，virtio 由前后端组成，其中后端是由 QEMU 实现的，目前主流的 QEMU 版本都实现了，不需要 care。而需要注意的是，virtio 的前端是要在虚拟机中满足，也就是相关的 virtio 驱动，这在使用时需要进行确认。 而 virtio 的通信方式中，还存在 QEMU 模拟命令执行这一个优化点，因此，vhost 出现优化了这一次的用户空间至内核空间的切换。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:4","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"2 设备直接分配 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:2:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"2.1 PCI 设备直接分配 PCI 设备直接分配 允许将宿主机的物理 PCI（或 PCI-E）设备直接分配给客户机完全使用。 设备直接分配相当于虚拟机直接使用硬件设备，也就没有了 QEMU 进程的模拟，因此速度最快。 但是，设备直接分配有一些条件： 硬件平台需要支持 Intel 硬件支持设备直接分配规范为 “Intel(R)Virtualization Technology for Directed I/O”（VT-d）或者 AMD 的规范为 “AMD-Vi”（也叫作 IOMMU）。 目前市面上的 x86 硬件平台基本都支持 VT-d，在 BIOS 中设置打开 VT-d 特性。 内核配置支持相关 VT-d 的特性，并且加载内核模块 vfio-pci（内核 \u003e= 3.10）。 内核启动参数需要打开 intel_iommu=on（intel 平台） $ modprobe vfio_pci $ lsmod | grep vfio vfio_pci 53248 0 vfio_virqfd 16384 1 vfio_pci vfio_iommu_type1 32768 0 vfio 36864 2 vfio_iommu_type1,vfio_pci irqbypass 16384 8 vfio_pci,kvm ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:2:1","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"2.2 SR-IOV VT-d 技术只能讲一个物理设备分配给一个客户机使用，为了让多个虚拟机共享同一个物理设备，PCI_SIG 组织发布了 SR-IOVSingle Root I/O Virtualizaiton and Sharing 规范，该规范定义了标准化机制，用以原生的支持多个共享的设备（不一定网卡）。 目前，SR-IOV 最广泛应用在以太网设备的虚拟化方面。 SR-IOV 引入了两个新的 function： PF 物理功能Physical Function ：PF 是一个普通的 PCI-e 设备（带有 SR-IOV 功能），在宿主机中配置和管理其他 VF，本身也可以作为独立 function 使用； VF 虚拟功能Virtual Function ：由 PF 衍生的 “轻量级” PCI-e 功能，可以分配到客户机中作为独立 function 使用； SR-IOV 为客户机中使用的 VF 提供了独立的内存空间、中断、DMA 流，从而不需要 Hypervisor 介入数据传输。 一个具有 SR-IOV 功能的设备能够被配置为 PCI 配置空间中呈现出多个 Function（1 个 PF + 多个 VF），每个 VF 有独立的配置空间和完整的 BAR（Base Address Register，基址寄存器）。 Hypervisor 通过将 VF 实际的配置空间映射到客户机看到的配置空间的方式，将一个或多个 VF 分配给一个客户机。并且，通过 Intel VT-x 和 VT-d 等硬件辅助虚拟化技术提供的内存转换技术，允许直接的 DMA 传输去往或来自一个客户机，因此几乎不需要 Hypervisor 的参与。 在客户机中看到的 VF，表现给客户机操作系统的就是一个完整的普通的设备。 SR-IOV 需要硬件平台支持 Intel VT-x 和 VT-d（或 AMD 的 SVM 和 IOMMU）虚拟化特性，并且需要具体设备支持 SR-IOV 规范。 在 Linux 中，可以查看 PCI 信息的“Capabilities”项目，以确定设备是否具备 SR-IOV 功能： $ lspci -s 82:00.0 -v 82:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01) Flags: bus master, fast devsel, latency 0, IRQ 38, NUMA node 1 Memory at d0100000 (32-bit, non-prefetchable) [size=1M] I/O ports at c020 [size=32] Memory at d0204000 (32-bit, non-prefetchable) [size=16K] Expansion ROM at d0400000 [disabled] [size=1M] Capabilities: [40] Power Management version 3 Capabilities: [50] MSI: Enable- Count=1/1 Maskable+ 64bit+ Capabilities: [70] MSI-X: Enable+ Count=10 Masked- Capabilities: [a0] Express Endpoint, MSI 00 Capabilities: [100] Advanced Error Reporting Capabilities: [140] Device Serial Number a4-dc-be-ff-ff-17-8d-52 Capabilities: [150] Alternative Routing-ID Interpretation (ARI) Capabilities: [160] Single Root I/O Virtualization (SR-IOV) Capabilities: [1a0] Transaction Processing Hints Capabilities: [1c0] Latency Tolerance Reporting Capabilities: [1d0] Access Control Services Kernel driver in use: igb Kernel modules: igb “Capabilities: [160] Single Root I/O Virtualization (SR-IOV)\" 表示网卡支持 SR-IOV 功能 具体使用方式见网络模式中的示例。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:2:2","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3 存储模式 下面的磁盘测试都是以 4k 随机读写测试，命令如下： fio -thread -name=${DISK} -filename=${DISK} \\ -ioengine=libaio -direct=1 -bs=4k -rw=randrw -iodepth=32 \\ -size=8G -rw=readrw 说明，测试仅仅适用于简单对比各个模式之间性能差异，而不是标准的基准测试，不能作为靠谱数据。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.1 纯模拟 纯模拟是最简单的方式，所有的 IO 请求发送给 QEMU，由 QEMU 在宿主机执行后将结果返回给虚拟机中（见 1.1）。 创建一个 qcow2 文件，并将其以 sata 驱动方式提供给虚拟机： … \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/root/disk1.qcow2'/\u003e \u003ctarget dev='sda' bus='sata'/\u003e \u003caddress type='drive' controller='0' bus='0' target='0' unit='0'/\u003e \u003c/disk\u003e … 在虚拟机中，可以看到对应的磁盘与对应的驱动： [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 500G 0 disk vda 253:0 0 500G 0 disk └─vda1 253:1 0 500G 0 part / [root@localhost ~]# lspci … 00:09.0 SATA controller: Intel Corporation 82801IR/IO/IH (ICH9R/DO/DH) 6 port SATA Controller [AHCI mode] (rev 02) 压测结果： read: IOPS=9183, BW=35.9MiB/s (37.6MB/s)(4098MiB/114246msec) write: IOPS=9172, BW=35.8MiB/s (37.6MB/s)(4094MiB/114246msec) ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:1","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.2 virtio-blk virtio-blk 实现了 virtio 标准，在虚拟机中使用 virtio 驱动，加速存储的访问。当然，需要客户机中系统支持 virtio-blk 内核模块。 [root@localhost ~]# lsmod | grep blk virtio_blk 18323 2 目前，virtio-blk 可用于宿主机文件、裸设备、LVM 设备，挂载到虚拟机后命名为 vdx。 将 \u003cdisk\u003e.\u003ctarget\u003e 中的 bus 改为 virtio，就是使用 virtio-blk 方式。 … \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/root/disk1.qcow2'/\u003e \u003ctarget dev='sda' bus='virtio'/\u003e \u003caddress type='pci' domain='0x0000' bus='0x00' slot='0x0a' function='0x0'/\u003e \u003c/disk\u003e … 不过使用 virtio 驱动，设置的 dev 名称会失效，自动使用 vdx 这种命名方式。在虚拟机中看到对应的磁盘以及驱动： [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 253:0 0 500G 0 disk └─vda1 253:1 0 500G 0 part / vdb 253:16 0 500G 0 disk [root@localhost ~]# lspci … 00:0a.0 SCSI storage controller: Red Hat, Inc. Virtio block device 压测结果，可以看到，比纯模拟要快很多： read: IOPS=41.7k, BW=163MiB/s (171MB/s)(4098MiB/25145msec) write: IOPS=41.7k, BW=163MiB/s (171MB/s)(4094MiB/25145msec) virtio-blk 虽然提供了很高的存储访问性能，但是其设计上也有着一些缺点： virtio blk 的范围有限，这使得新的命令实现变得复杂。每次开发一个新命令时，virtio blk 驱动程序都必须在每个客户机中更新 virtio blk 将 PCI 功能和存储设备映射为 1:1，一个映射就需要占用虚拟机一个 PCI 地址，限制了可扩展性。 virtio blk 不是真正的 SCSI 设备。这会导致一些应用程序在从物理机移动到虚拟机时中断。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:2","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.3 virtio-scsi virito-scsi 有着与 virtio-blk 相同的性能，但是改善了 virtio-blk 的相关缺点，其中最大的优势就是 virtio-scsi 可以允许虚拟机处理数百个设备，而 virtio-blk 只能处理大约 30 个设备就会耗尽 PCI 插槽。 因为 virtio-scsi 在虚拟机里使用的是 scsi 驱动，因此设备的命名也变为了 sdx。 将磁盘的驱动设置为 scsi，同时将 scsi 驱动设置为使用 virtio-scsi（默认使用的驱动是 lsi）： … \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/root/disk1.qcow2'/\u003e \u003ctarget dev='sdc' bus='scsi'/\u003e \u003caddress type='drive' controller='0' bus='0' target='0' unit='2'/\u003e \u003c/disk\u003e \u003ccontroller type='scsi' index='0' model='virtio-scsi'/\u003e … 可以看到，\u003caddress\u003e 中设置相关设备地址时，可以看到不是设置 pci 槽地址，而是设置设备对应 SCSI 映射地址，因此通过增加 “target” 属性就可以添加多个硬盘，不需要占用多个 PCI 槽； 虚拟机中看到对应的块设备 “sda”，以及驱动 “Virtio SCSI”： [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 500G 0 disk vda 253:0 0 500G 0 disk └─vda1 253:1 0 500G 0 part / [root@localhost ~]# lspci … 00:08.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI 压测结果： read: IOPS=45.5k, BW=178MiB/s (186MB/s)(4098MiB/23055msec) write: IOPS=46.2k, BW=180MiB/s (189MB/s)(4094MiB/22695msec) 目前，virtio-scsi 还存在一种设备直通的模式，IO 性能更高（具体实现原理还不了解）。但是，这种模式还能用于块设备，并且是使用 scsi 协议的块设备（测试文件、usb 都不支持）。 通过设置 device=“lun” 使用： ... \u003cdisk type='block' device='lun'\u003e \u003cdriver name='qemu' type='raw'/\u003e \u003csource dev='/dev/sda'/\u003e \u003ctarget dev='sdc' bus='scsi'/\u003e \u003caddress type='drive' controller='0' bus='0' target='0' unit='2'/\u003e \u003c/disk\u003e \u003ccontroller type='scsi' index='0' model='virtio-scsi' /\u003e ... 使用 lun 方式，并指定使用宿主机块设备 sda 虚拟机中看到对应块设备 “sda”，以及驱动 \"” ： [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 465.8G 0 disk vda 253:0 0 500G 0 disk └─vda1 253:1 0 500G 0 part / [root@localhost ~]# lspci … 00:08.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI 发现的问题 在使用 lun 分配磁盘时，发现虚拟机内部对磁盘的修改，在宿主机上是无法可见的（可能出现磁盘文件系统 UUID 都不一致的问题）。并且，在虚拟机内部给磁盘创建文件系统还会出现有报错的情况。 如果是要将 scsi 磁盘单独分给虚拟机，推荐使用设备直接分配中的 scsi 设备分配。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:3","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.4 设备直接分配 3.4.1 PCI 设备 在 2.1 PCI 设备直接分配 中所说，PCI 设备都支持进行设备直接分配，使得虚拟机中直接对 PCI 设备进行访问，速度最快。nvme 磁盘可以使用这种方式。 使用设备直接分配时，需要现在宿主机上取消对应设备的驱动绑定，然后将其分配给虚拟机，然后 libvirt 中配置好相关参数后，会自动帮我们执行这些步骤。 在设备 xml 配置文件中添加 \u003chostdev\u003e 项，指定对应的设备 PCI 地址，来直接分配设备（也可以使用 \u003cinterface type=‘hostdev’/\u003e，这是一种 libvirt 提供的较新的配置方式，但是不兼容所有设备）。 … \u003chostdev mode='subsystem' type='pci' managed='yes'\u003e \u003csource\u003e \u003caddress domain='0x0000' bus='0x08' slot='0x00' function='0x0'/\u003e \u003c/source\u003e \u003c/hostdev\u003e … 3.4.2 SCSI 设备 对于 SCSI 磁盘，通过 PCI 无法单独分配某个磁盘，而 libvirt 中提供了 type=‘scsi’ 类型的直接分配。但是与 PCI 不同的是，scsi 无法设置 ‘managed’ 参数，也就是说宿主机上还是能够看见该 scsi 设备，如果想不可见，需要自己 unbind。 添加 scsi 类型的 \u003chostdev\u003e 项，其中 \u003csource\u003e.\u003caddress\u003e 中指定宿主机对应磁盘的 scsi 地址。 ... \u003chostdev mode='subsystem' type='scsi' managed='no' sgio='filtered' rawio='yes'\u003e \u003csource\u003e \u003cadapter name='scsi_host0'/\u003e \u003caddress bus='2' target='1' unit='0'/\u003e \u003c/source\u003e \u003caddress type='drive' controller='0' bus='0' target='0' unit='2'/\u003e \u003c/hostdev\u003e \u003ccontroller type='scsi' index='0' model='virtio-scsi'/\u003e ... Note 这种模式还是会使用 virtio-scsi 驱动，我不确定这是否属于设备直接分配，但是在文档中其属于 passthrough 的一类 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:4","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.5 libvirt 提供的存储模型 TODO ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:5","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"4 网络模式 虚拟机网络的构建一般都是需要构建 宿主机网络环境 + 虚拟机虚拟网络设备，得益于 libvirt，一些常用的网络的构建只需要配置好相关配置就行，libvirt 会进行网络的构建。在 libvirt 中，一个可用的网络就称为 netowrk 首先需要明确的，下面所说的网络模式不同的在于如何让宿主机接收的数据包，到达 qemu 进程 或者 vhost 内核模块。而 qemu 进程与 vhost 模块如何将数据包传递到虚拟机中，这是虚拟化方式的问题，即全虚拟化或者半虚拟化。 更简单点说，虚拟机的网络收发有三个点：宿主机 \u003c-\u003e qemu/vhost \u003c-\u003e 虚拟机。而不同网络模式不同点在 宿主机至 qemu/vhost 阶段，而数据包到虚拟机，这就是不同虚拟化的工作了。 在下面的配置中，你会发现网络模式还需要配置 driver 选项，这就是配置具体的虚拟化方式了。但是这不是这里的重点，所以不会有特殊的说明。 libvirt 支持的网络模式有很多，下面仅仅提到我使用过的。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:4:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"4.1 虚拟机网络 4.1.1 NAT Mode NAT 网络是最简单的网络，不需要任何的依赖。通过虚拟的 bridge 网卡创建一个属于虚拟机的内网，然后在宿主机上通过 iptables 实现内网地址的 NAT。（没错，这和 docker bridge network 的原理一样） libvirt 会存在一个名为 default 的 network，其就是一个 NAT 网络。通过 virsh net-list 查看： $ virsh net-list Name State Autostart Persistent -------------------------------------------- default active yes yes 其默认会被 active，也就是说宿主机环境在 libvirtd 启动后就会构建好，你可以看到对应的 bridge 网卡。 $ ip a ... 3: virbr0: \u003cNO-CARRIER,BROADCAST,MULTICAST,UP\u003e mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:12:34:56 brd ff:ff:ff:ff:ff:ff inet 172.27.0.1/16 brd 172.27.255.255 scope global galaxybr0 valid_lft forever preferred_lft forever ... 默认下，default 网络的内网为 192.168.122.1/24，我通过修改其配置文件 /etc/libvirt/qemu/networks/default.xml 修改了内网范围： $ cat /etc/libvirt/qemu/networks/default.xml \u003cnetwork\u003e \u003cname\u003edefault\u003c/name\u003e \u003cuuid\u003e341adece-b07a-4eb0-92f2-d92f59ed266f\u003c/uuid\u003e \u003cforward mode='nat'/\u003e \u003cbridge name='virbr0' stp='on' delay='0'/\u003e \u003cmac address='52:54:00:12:34:56'/\u003e \u003cip address='172.27.0.1' netmask='255.255.0.0'\u003e \u003cdhcp\u003e \u003crange start='172.27.0.2' end='172.27.255.254'/\u003e \u003c/dhcp\u003e \u003c/ip\u003e \u003c/network\u003e libvirtd 会为启动的 nat 网络启动一个 dnsmasq 进程，用于提供 DNS 与 DHCP 功能，其对应配置就是对应网络的配置 $ ps x | grep dns 2185 ? S 0:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper $ cat /var/lib/libvirt/dnsmasq/default.conf strict-order pid-file=/var/run/libvirt/network/default.pid except-interface=lo bind-dynamic interface=virbr0 dhcp-range=172.27.0.2,172.27.255.254,255.255.0.0 dhcp-no-override dhcp-authoritative dhcp-lease-max=65533 dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts Note 这里与 docker bridge network 不同点，因为 docker 中仅仅是 namespace 隔离，容器内网卡 IP 还是可以由 docker 进入 namespace 分配，所以是 docker 承担了 dhcp 服务器的职责。 但是虚拟机和宿主机是强隔离的，因此需要虚拟机启动后去作为 dhcpclient 申请地址，因此需要一个 dhcp 服务，这就是 dnsmasq 的作用。 在 xml 配置文件中设置 \u003cinterface type=‘network’\u003e 项，并设置 \u003csource network=‘default’/\u003e 使用 default network。 \u003cinferface type='network'\u003e \u003cmac address='50:54:00:87:bc:c3'/\u003e \u003csource network='default'/\u003e \u003cmodel type='virtio'/\u003e \u003cdriver name='vhost' txmode='iothread' ioeventfd='on' event_idx='off' queues='16'\u003e \u003chost csum='off' gso='off' tso4='off' tso6='off' ecn='off' ufo='off' mrg_rxbuf='off'/\u003e \u003cguest csum='off' tso4='off' tso6='off' ecn='off' ufo='off'/\u003e \u003c/driver\u003e \u003c/interface\u003e 启动虚拟机后，可以看到虚拟机内部存在对应的网卡，通过 dhclient -v eth0 命令申请一个 IP，广播的 dhcp request 会被宿主机 dnsmasp 进程响应，并返回 dhcp offer。 [root@localhost ~]# ip a ... 2: eth0: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 50:54:00:87:bc:c3 brd ff:ff:ff:ff:ff:ff [root@localhost ~]# dhclient -v eth0 ... Listening on LPF/eth0/50:54:00:87:bc:c3 Sending on LPF/eth0/50:54:00:87:bc:c3 Sending on Socket/fallback DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 4 (xid=0x57740476) DHCPREQUEST on eth0 to 255.255.255.255 port 67 (xid=0x57740476) DHCPOFFER from 172.27.0.1 DHCPACK from 172.27.0.1 (xid=0x57740476) bound to 172.27.163.138 -- renewal in 1653 seconds. [root@localhost ~]# ping www.baidu.com PING www.a.shifen.com (14.215.177.38) 56(84) bytes of data. 64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=1 ttl=48 time=7.93 ms ... 4.1.2 Routed Mode TODO ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:4:1","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"4.2 共享物理设备网络 4.2.1 Bridge TODO 4.2.2 Macvtap 在 docker 的网络中，存在着 macvlan 网络，其在宿主机的物理网卡上创建 macvlan 设备，使得在二层上构建多个接口，从而让多个容器处于与宿主机同一个局域网内。在虚拟机网络中，单单 macvlan 网卡无法连接虚拟机设备与宿主机网卡，还需要一个 tap 设备连接，将 macvlan + tap 组合就是 macvtap 设备。 与 macvlan 设备相同，macvtap 同样存在：private、vepa、bridge、passthru 四种模式，下面的示例中都以 bridge 模式为例。 在 libvirt 中，当 interface type 为 direct 时，表明设备是直接附加到物理网卡上，而就会使用 macvtap 构建网络。通过 \u003csource dev=xx mode=xx\u003e 指定附加的物理网卡，以及 macvtap 的模式。 ... \u003cinterface type='direct'\u003e \u003cmac address='50:54:00:87:bc:c3'/\u003e \u003csource dev='eth3' mode='bridge'/\u003e \u003cmodel type='virtio'/\u003e \u003cdriver name='vhost' txmode='iothread' ioeventfd='on' event_idx='off' queues='16'\u003e \u003chost csum='off' gso='off' tso4='off' tso6='off' ecn='off' ufo='off' mrg_rxbuf='off'/\u003e \u003cguest csum='off' tso4='off' tso6='off' ecn='off' ufo='off'/\u003e \u003c/driver\u003e \u003caddress type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/\u003e \u003c/interface\u003e ... 启动虚拟机后，在宿主机上可以看到对应的 macvtap 设备被创建，其 mac 地址也与配置中的一致。 $ ip a ... 5: eth3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether a4:dc:be:0a:7d:37 brd ff:ff:ff:ff:ff:ff inet 192.168.1.107/24 brd 192.168.1.255 scope global eth3 valid_lft forever preferred_lft forever 10: macvtap0@eth3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 500 link/ether 50:54:00:87:bc:c3 brd ff:ff:ff:ff:ff:ff 在虚拟机内部，可以看到对应的虚拟网卡，通过 dhcp 上层路由器获取 IP，可以发现，其网关就是宿主机的网关，其网段与宿主机一致。因此，使用 macvtap 相当于让虚拟机与宿主机处于同一个二层。 [root@localhost ~]# dhclient -v eth0 ... Listening on LPF/eth0/50:54:00:87:bc:c3 Sending on LPF/eth0/50:54:00:87:bc:c3 Sending on Socket/fallback DHCPREQUEST on eth0 to 255.255.255.255 port 67 (xid=0x4e586341) DHCPNAK from 192.168.1.1 (xid=0x4e586341) DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 8 (xid=0x1d605c78) DHCPREQUEST on eth0 to 255.255.255.255 port 67 (xid=0x1d605c78) DHCPOFFER from 192.168.1.1 DHCPACK from 192.168.1.1 (xid=0x1d605c78) bound to 192.168.1.105 -- renewal in 3423 seconds. [root@localhost ~]# ip a 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:00:87:bc:c1 brd ff:ff:ff:ff:ff:ff inet 192.168.1.105/24 brd 192.168.1.255 scope global dynamic eth0 valid_lft 7023sec preferred_lft 7023sec ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:4:2","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"4.3 设备直接分配 前面的各种网络模式，最后还是靠着全虚拟化或者半虚拟化将数据包传递给虚拟机，而如果你有着多个可用的网卡，那么可以考虑使用设备直接分配，这是性能最高的方式。 4.3.1 PCI 网卡 在设备 xml 配置文件中添加 \u003chostdev\u003e 项，指定对应的设备 PCI 地址，来直接分配设备（也可以使用 \u003cinterface type=‘hostdev’/\u003e，这是一种 libvirt 提供的较新的配置方式，但是不兼容所有设备）。 … \u003chostdev mode='subsystem' type='pci' managed='yes'\u003e \u003csource\u003e \u003caddress domain='0x0000' bus='0x01' slot='0x00' function='0x0'/\u003e \u003c/source\u003e \u003c/hostdev\u003e … 启动虚拟机后，在宿主机上就无法看到对应的网卡了，因为对应的驱动被 libvirt 解绑了。 关闭虚拟机后，libvirt 又会重新将网卡绑定驱动，在宿主机上有可见了。 4.3.1 SR-IOV TODO ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:4:3","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"参考 libvirt domain 配置官方文档 RedHat：Guest Virtual Machine Device Configuration virtio-scsi passthrough virtio-scsi 和 virtio-blk 的理解 libvirt: Networking libvirt: VirtualNetworking ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:5:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["Docker 原理总结"],"content":"容器启动背后的执行过程","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文主要描述容器启停背后的步骤，但是不涉及源码。 示例的基于 ubuntu 20.04.1 LTS 虚拟机运行，docker 版本如下： $ docker version Client: Version: 19.03.8 API version: 1.40 Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 19:43:43 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.8 API version: 1.40 (minimum version 1.12) Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 16:41:21 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.3-0ubuntu2 GitCommit: runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit: ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:0:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1 启动 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:1:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1.1 Create 通过 docker create 命令可以创建一个容器，但是容器并不会真正的运行，其对应进程不会存在。 create 容器经过的具体流程为： 容器参数检查与调整； 容器对应的读写层（RWLayer）的创建； 容器元信息的记录（主要是配置信息）； (1) 容器参数检查与调整 检查容器运行参数是否合法，并调整一些参数的值，这一步不具体描述。 (2) 读写层的创建 容器的 “层” 可以分为： 读写层 ：保存容器对 rootfs 写、修改结果的层，并在容器退出后被 docker 删除； 例如，容器中对系统盘 root 目录中某个文件的修改，这个修改后的文件会被复制一份在系统盘上。 init 层 ：用于处理一些与镜像不绑定，但是与运行容器相关的文件修改，主要是 /etc/resolve.conf /etc/hosts 等； 为什么要有 init 层？ 我的理解是：镜像层（只读层）提供的是一个静态的环境，即所有容器看到的环境都是一样。 而有些东西并不是想让所有容器看到的相同，例如 hostname、nameserver，这些 docker 都提供了参数配置，所 docker 单独抽出了一个 “机器维度” 的只读层，init 层 置于为什么不是在读写层修改，个人觉得是因为读写层是可以被 “导出” 的（docker save），而这些 /etresolve.conf 的内容又是不应该被导出的，所以放在了 init 层。 只读层 ：镜像包含的所有层，仅仅只读。对只读层任何修改都会以 COW 形式放在读写层。 具体读写层的概念这里不展开，可以阅读官方文档：storagedriver。 这里读写层的创建仅仅指的是创建了 init 层与读写层的目录，并没有做 union mount，毕竟容器没有运行嘛，没必要。 找个例子看一下： $ docker create --rm -t ubuntu 361c520da78f848d639d65f042fcf5d448c13cbc4ce8c251dcba2250162b48fe inspect 可以看到对应的读写层的目录： $ docker inspect 361c520da78f … \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff:/var/lib/docker/overlay2/0336c489d40e65588748265a95f18328ddb1f5bcb9ebf10909fbf3f5f35b9496/diff:/var/lib/docker/overlay2/77d3ac91877751678bfec0576dab39ccd4b73666f8040aef387ef47ff30b4cf1/diff:/var/lib/docker/overlay2/ec8326178c990b52970a65371fd375737fdf256db597aa821a2b0f7d79bcc6f3/diff:/var/lib/docker/overlay2/385038374d3d369e98724926d0e1c240dcb74e31b1663ec1cb434c43ca2826f1/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/work\" }, \"Name\": \"overlay2\" }, … $ ls /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/ diff link lower work 所有的层（包含容器、镜像）都位于 /var/lib/docker/[driver] 目录下，不过不同的 driver 有着不同的目录结构； 每个层的目录结构也和对应的 driver 有关，overlay2 中就会包含 diff、work 等子目录，而真正容器运行后看到的就是 diff 目录被挂载后的内容； 在 /var/lib/docker/overlay2/ 目录下，我们还可以看到一个同读写层类似名字的 “xxx-init” 目录，这就是 init 层目录，对应的 diff 子目录也是用于挂载的目录： $ ls /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init committed diff link lower work $ tree /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff |-- dev | |-- console | |-- pts | `-- shm `-- etc |-- hostname |-- hosts |-- mtab -\u003e /proc/mounts `-- resolv.conf 但是这两个目录仅仅是被创建，如果执行 mount 命令可以看到，这些目录是没有被挂载的。 (3) 元信息的记录 执行 docker create 后，通过 docker ps -a 可以看到对应的容器，并且 inspect 可以看到对应的配置信息，因此，create 之后是有元信息的记录的。 而这个元信息就保存在 /var/lib/docker/containers 目录下，每个子目录的名字就是对应的容器 ID： $ cd /var/lib/docker/containers \u0026\u0026 ls 361c520da78f848d639d65f042fcf5d448c13cbc4ce8c251dcba2250162b48fe $ ls 361c520da78f848d639d65f042fcf5d448c13cbc4ce8c251dcba2250162b48fe checkpoints config.v2.json hostconfig.json config.v2.json 保存的就是对应的容器配置信息； 可以看到， /var/lib/docker/containers 目录就是所有容器信息的 “数据库”，这与层的概念是解耦的。 如果，你设置了 docker daemon 退出后不停止所有容器（默认情况 docker daemon 退出前会停止并删除所有容器，通过配置可以改变这个行为），那么 docker daemon 重新启动后，就会依靠这个目录进行 container 信息的恢复。 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:1:1","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1.2 Start 通过 docker start 命令运行一个容器，其主要的步骤如下： 状态检查，只有 Create 与 Stop 状态容器才可以被 Start； 进行容器 rootfs 的构建，这里会进行读写层、init 层、镜像层的 union mount； 容器网络构建； 调用 containerd 进行容器的启动； docker run 等同于 docker create + docker run，所以不需要特别说明。 (2) rootfs 的构建 rootfs 指定是容器最后看到的根文件系统，也就是 读写层 + init 层 + 镜像层经过 union mount 后的读写层。 我感觉，union mount 主要由两个特点： 统一视图 ：将各个层 “压扁”，最后得到一个层。而上下层之间相同的文件、目录，就会被上层的覆盖。 写时复制 ：对于整个视图的操作，只会影响最顶层（读写层），不会影响其他层，并且是写时复制的。 看一下具体示例： $ docker start 361c520da78f 361c520da78f 通过 mount 命令，可以看到对应容器的 union mount 已经出现： $ mount … overlay on /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ:/var/lib/docker/overlay2/l/UHTTQ5AJKPR23Y3V7J4ZLOIFDR:/var/lib/docker/overlay2/l/VWIFLRAQOPMH7LBAQQ5DDGIYVM:/var/lib/docker/overlay2/l/LQBRTVETGGWVU2OHWC42443K7X:/var/lib/docker/overlay2/l/5PDNI5HSOH6UMUDNWF4VMR46TS,upperdir=/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/diff,workdir=/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/work,xino=off) nsfs on /run/docker/netns/4500ea4f0025 type nsfs (rw) $ ls -lh /var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ lrwxrwxrwx 1 root root 77 Nov 14 15:51 /var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ -\u003e ../d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff 第一个挂载信息看出，将 init 层与镜像层的目录挂载到了读写的 merged 目录； 后面执行的命令看待，使用的 /var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ 这样的目录是软链，指向前面说的对应的各个层的目录，因为整个层的名字太长，所以使用软链别名让挂载属性短一些； (3) 容器网络构建 首先明确，先有网络环境，再有的容器运行。也就是说，是容器中的进程加入一个特定的 net namespace。所以，网络环境的构建的目标就是：得到一个配好网络的 net namespace。 所以网络构建可以分为三个大概的步骤： 创建一个新的 net namespace； 在这个 net namespace 里操作，构建好网络； 保留这个 net namesapce； 如何 “配好网络的 net namespace” 就和具体的网络模式有关了，具体见：容器网络总结。 默认下，当一个 namespace 中没有任何进程时，该 namespace 就自动被内核销毁了（垃圾回收），而要将一个 namespace 持久化，就需要将其挂载到一个具体文件，这样该 net namespace 就会保留。 因此，docker 会通过这种方式先保留 net namespace，并让容器运行后的进程可以加入。当容器停止是，docker 将其手动删除。 通过 mount 命令，你可以看到对应容器的 net namespace 的挂载会随着容器运行出现。 $ mount … nsfs on /run/docker/netns/4500ea4f0025 type nsfs (rw) … 当然，上面说的 “构建网络”，“挂载文件”，包括 “namespace 文件如何映射到对应的容器” 这些行为与信息，都是由 libnetwork 库中负责的。 Tip 看 libnetwork 源码时发现一点，当某个代码需要进入 namespace，一定需要将当前 goroutine 与 thread 绑定（runtime.LockOSThread()）接口。 Why ？举个例子，一段代码由 G1 groutine 绑定到了 T1 线程执行，创建了 N1 namespace，并且期望在 N1 namespace 下执行。但是代码运行中，可能由于 Go 的调度，变成了 G1 groutine 绑定到 T2 线程执行。这时，就切换了 namespace 了，代码也就不是在期望的 namespace 下执行，这可能带来很大的问题。 (4) 调用 containerd 启动容器 docker daemon 在设计到容器进程的运行时，都是交给 containerd，然后 containerd 调用 shim，shim 调用 runc 库执行。 真正容器内进程怎么运行还包括很多内容，特别是 runc 如何调整进程 namespace，如何启动进程这些内容，这里不再深入说。 如果有空，等后面出个文章详述（挖坑。 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:1:2","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2 停止 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:2:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.1 Stop stop 的步骤就是对 run 操作的回滚，包括： 停止容器的运行； 销毁容器网络； 解除容器读写层的挂载； 其中第 2、3 步就是反着来操作，没啥好说的。 (1) 停止容器运行 停止容器运行，其实就是停止容器内所有进程的运行。 有趣的是停止容器运行的过程里，并没有使用 runc 库，而是在 shim 这一层中，对 shim 进程发送信号。这一块也是后面细说。 大致的停止流程就是下面两步： 发送 配置/默认 (SIGTERM) 的停止信号； 上一步停止失败/超时，那么就发送 SIGKILL 信号； ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:2:1","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.2 Remove 老样子，操作的逆向，这也没啥好说的。 删除对应读写层目录与 init 层目录； 删除对应的容器元信息； ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:2:2","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3 暂停与恢复 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:3:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.1 Pause 与 Resume Pause 操作的原理很简单，通过 cgroup.freezer 冻结进程的运行，也就是不让进程被内核调度运行。 在容器运行状态，读取对应 cgroup 的 freezer.state 文件可以看到是 THAWED 状态。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f4b92a61851a ubuntu \"/bin/bash\" 2 seconds ago Up 1 second host_container $ cat /sys/fs/cgroup/freezer/docker/f4b92a61851a7f5f85569318c830722c275796b36d34a506eae05b547b31cb7c/freezer.state THAWED 执行 docker pause 后，看到对应的 freezer.state 变为 FROZEN，表明被冻结了。 $ docker pause host_container host_container $ cat /sys/fs/cgroup/freezer/docker/f4b92a61851a7f5f85569318c830722c275796b36d34a506eae05b547b31cb7c/freezer.state FROZEN 执行 docker unpause 恢复运行后，对应状态又变为了 THAWED： $ docker unpause host_container $ cat /sys/fs/cgroup/freezer/docker/f4b92a61851a7f5f85569318c830722c275796b36d34a506eae05b547b31cb7c/freezer.state THAWED 而这个状态的变化，其实就是通过 echo \"\u003cstate\u003e\" \u003e freezer.state 实现的。 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:3:1","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"描述 docker 下容器网络模型与实现","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对自己使用过的 docker 使用的网络模式的原理的总结。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:0:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1 概览 docker 容器网络目前包含 5 中模式，包括： bridge：默认的网络模式，使用 bridge 虚拟网卡 + iptables 实现一个内地内网，所有容器都处于该内网内，并且可以相互访问； host：与宿主机处于同一个 net namespace，使用宿主机网络环境； overlay：在多个 docker daemon 之间建立 overlay 网络，使得不同 docker daemon 的容器之间可以相互通信； macvlan：使用 macvlan 虚拟网卡，将容器物理地址暴露在宿主机局域网中，你可以认为就是一台同局域网的物理机； none：不进行任何网络配置，通常与自定义网络 driver 配合使用； 除了上述模式之外，每个容器也可以加入其它容器的网络中（通过加入对应的 net namespace）。 docker 还支持使用自定义的网络插件，这块不了解，具体见官方文档。 下面所有示例都在虚拟机 ubuntu 20.04 与内核 5.4.0-52-generic 中完成，docker 版本如下： $docker version Client: Version: 19.03.8 API version: 1.40 Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 19:43:43 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.8 API version: 1.40 (minimum version 1.12) Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 16:41:21 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.3-0ubuntu2 GitCommit: runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit: ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:1:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2 背景知识 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.1 cgroup 与 namespace 这部分网上知识很多，这里就不复制别人的了。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:1","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.2 docker 如何使用 net namespace namespace 用于各个进程间的环境的隔离，而容器运行（非 host 与 container 模式）的就是处于一个独立的 net namespace。 当处于一个 net namespace 时，可以认为，内核协议栈、iptables(net_filter)、网络设备等与其他 net namespace 都是隔离的。（这里只是说可以这么认为，但是真正还是只有一个内核，内核为 namespace 做了逻辑上的隔离） 在容器运行之间，docker 就会创建容器对应的 net namespace，并构建好对应的网络，然后将其 ‘持久化’（因为默认 namespace 是随着进程消失而消失的，如果想进程消失而 namespace 存在，那么需要将其 mount 到一个文件上）。 例如，当我们创建了一个容器后，可以看到这么一个挂载： $ mount … nsfs on /run/docker/netns/9779108cb6b0 type nsfs (rw) 该文件就是对应 net namespace 的挂载，通过对比容器进程的 netns inode 与 文件 inode 可以证明： $ docker top br0_container UID PID PPID C STIME TTY TIME CMD root 92658 92640 0 Nov06 pts/0 00:00:00 /bin/bash $ ls -lhi /proc/92658/ns/net 474863 lrwxrwxrwx 1 root root 0 Nov 7 12:42 /proc/92658/ns/net -\u003e 'net:[4026532287]' # 文件 inode 与进程 net 指向 inode 相同 $ ls -lhi /run/docker/netns/9779108cb6b0 4026532287 -r--r--r-- 1 root root 0 Nov 6 19:47 /run/docker/netns/9779108cb6b0 在容器被删除后，对应 net namespace 就会被销毁。 而各个网络模式最大的不同，就是在于 namespace 创建后，对应的 “构建网络” 的操作了。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:2","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.3 bridge 虚拟网络设备 bridge 网络设备 相当于一个 “交换机”，让任何其他网络设备链接上 bridge 时，所有包的都会无条件经过 bridge 转发，而链接的网络设备就变成了一根 “网线”。 不过与真实的交换机不同，brdige 网卡可以被赋值 IP，当 bridge 拥有 IP 后，它就与内核协议栈连接了，因此接收到的包可以到达内核协议栈的 IP 层处理，也就会经过 net_filter 处理。 推荐阅读 bridge 网卡推荐阅读：Linux 虚拟网络设备之 bridge（桥） ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:3","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.4 veth-pair 虚拟网络设备 veth-pair 设备 总是成对的出现，当数据包进入一端 veth 设备时，会从另一端 veth 设备出。veth-pair 两个设备可以处于不同的 net namespace，也就可以实现不同 net namespace 间数据传输。 默认下，veth 设备链接的两端是内核协议栈。不过 veth 设备链接上 bridge，这样另一端发送的数据都会由 bridge 处理。 推荐阅读 veth-pair 设备了解推荐文章：Linux 虚拟网络设备之 veth ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:4","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.5 macvlan 虚拟网络设备 macvlan 网络设备 可以有 mac 地址与 ip 地址，用于将 net namespace 连接到宿主机的物理网络中，相当于，容器直接连接着物理网络。 macvlan 网络设备有着多种的模式，包括：bridge、private 等，这影响着各个 macvlan 网络设备之间的通信。 更多 macvlan 网络设备推荐文章：Linux interfaces for virtual networking ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:5","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3 Bridge 网络 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.1 创建/删除 Bridge 网络 (1) 创建网络 先试着创建一个自定义的 bridge 网络，观察 docker 会对应创建哪些东西。 $ docker network create --driver=bridge \\ --subnet=192.168.100.0/24 \\ --ip-range=192.168.100.0/26 \\ --gateway=192.168.100.1 \\ --opt com.docker.network.bridge.name=mybr0 \\ mybridge0 2e61a7dc333c1bc61d9cb86503ce4cd5a7435977ea2f9b7cc97fc71ae0e2bb93 --driver=bridge 指定创建的网络 driver； --subnet=192.168.100.0/24 指定对应 bridge 网络的网段； --ip-range=192.168.100.0/26 指定运行分配给容器的 ip 范围，当然，这个是要在指定的网段内的； --gateway=192.168.100.1 指定该内网的网关 IP； --opt com.docker.network.bridge.name=mybr0 指定创建虚拟 bridge 网卡的命名； mybridge0 为创建的 docker network 的命名； 通过 ifconfig 可以看到，bridge 网络创建会对应创建一个 bridge 网络设备，作为整个内网的 ‘交换机’。其 IP 就是指定的 gateway IP。 $ ifconfig … mybr0: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 192.168.100.1 netmask 255.255.255.0 broadcast 192.168.100.255 ether 02:42:46:8a:cf:34 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 … $ brctl show bridge name bridge id STP enabled interfaces mybr0 8000.0242efdb0984 no 但是与虚拟机网络中的 bridge 网卡不同，该 bridge 不会连接任何的物理网卡，仅仅是作为内网的 ‘交换机’ 使用。但是，毕竟内网是虚拟的，没有实际与物理网络连接，如何访问外网呢？ 答案是，通过内核 iptables 进行 NAT，然后将包从实际的物理网卡上发送与接受。因此还有一部分的改变在于 iptables，主要会建立的是 nat 与 filter 表的规则。 先看 nat 表的相关规则（下面输出中省略了不相关规则）： $ iptables -t nat -L -nv Chain PREROUTING (policy ACCEPT 2 packets, 88 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL Chain INPUT (policy ACCEPT 2 packets, 88 bytes) target prot opt in out source destination Chain OUTPUT (policy ACCEPT 124 packets, 8797 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT 124 packets, 8797 bytes) target prot opt in out source destination MASQUERADE all -- * !mybr0 192.168.100.0/24 0.0.0.0/0 Chain DOCKER (2 references) target prot opt in out source destination RETURN all -- mybr0 * 0.0.0.0/0 0.0.0.0/0 PREROUTING 与 OUTPUT 链中规则，使得所有入和出的包都会经过 DOCKER 链； POSTROUTING 链中，将 mybridge0 网络（192.168.100.0/24）的内网 ip 通过 MASQUERADE 行为进行伪装（可以简单认为内网 ip 会变为当前网卡的 ip）； 当然，如果包发往的是 mybr0 网卡，说明是 mybridge0 网络内部通信，就不需要进行 MASQUERADE 伪装（!mybr0）； 当容器发包时，会通过 mybr0 网卡转发进入内核栈，因此在 filter 表中，相关的规则都是针对于 “in=mybr0”。看一下 filter 表的规则： $ iptables -t filter -L -nv Chain INPUT (policy ACCEPT 61774 packets, 79M bytes) Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- * mybr0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED DOCKER all -- * mybr0 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- mybr0 !mybr0 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- mybr0 mybr0 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 42290 packets, 55M bytes) target prot opt in out source destination Chain DOCKER (2 references) target prot opt in out source destination Chain DOCKER-ISOLATION-STAGE-1 (1 references) target prot opt in out source destination DOCKER-ISOLATION-STAGE-2 all -- mybr0 !mybr0 0.0.0.0/0 0.0.0.0/0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-ISOLATION-STAGE-2 (2 references) target prot opt in out source destination DROP all -- * mybr0 0.0.0.0/0 0.0.0.0/0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-USER (1 references) target prot opt in out source destination RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 FORWARD -\u003e DOCKER-ISOLATION-STAGE-1 -\u003e DOCKER-ISOLATION-STAGE-2 表明允许包从 mybr0 进入并转发（即容器可以向外正常发包）； FORWARD 中对 mybr0 进入的包设置了 conntrack，使得能够收到连接建立后的正常的回包； (2) 删除网络 通过 docker network remove 删除网络时，会发现对应的 bridge 网卡与 iptables 规则都被删除。 $ docker network remove 5a17670afb6f 5a17670afb6f ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:1","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.2 启动容器后的网络 下面看下容器启停后，带来的网络变化。先启动最简单的一个容器，指定使用网络为上面创建的 mybridge0。 $ docker run -dt --rm --network=mybridge0 --name br0_container ubuntu 676f7f9eab12a20fb3a975fa99cc2c92433a9581b5774ea58e63d447d86aa5ad $ docker inspect 676f7f9eab12 … \"Networks\": { \"mybridge0\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": [ \"882cac3e472f\" ], \"NetworkID\": \"af1dbf619ac62be1ad8a6b63696d3e6edff77cceab6cd0ee78de4b51e0d33683\", \"EndpointID\": \"56cd85c5121d7d14146fcacc75599f6c56034e758e81f405a51437276ac6ac9f\", \"Gateway\": \"192.168.100.1\", \"IPAddress\": \"192.168.100.2\", \"IPPrefixLen\": 24, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:c0:a8:64:02\", \"DriverOpts\": null } } … --network=mybridge0 表明以 mybridge0 网络启动容器； 观察容器具体参数，可以看到，容器被随机分配 mybridge0 设置的 ip-range 一个 ip，并且 gateway 就是 mybridge0 网络的网关地址； 观察网络设备，可以看到一个 veth-pair 设备 出现在宿主机上，并且连接到了 mybr0 网卡： $ ifconfig … vethef6b174: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet6 fe80::e8ad:86ff:fefe:14ca prefixlen 64 scopeid 0x20\u003clink\u003e ether ea:ad:86:fe:14:ca txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 23 bytes 1882 (1.8 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 … $ brctl show bridge name bridge id STP enabled interfaces mybr0 8000.0242efdb0984 no vethef6b174 veth-pair 都是成对出现的，可以简单被看做一个通道，一端发入的包会从另一端发出，并进入内核协议栈。不过，在 bridge 网络环境下，veth5b480f8 连接到 mybr0，所以所有从 veth5b480f8 发出的包都会被 mybr0 接手转发（相当于就是一根网线插入了交换机）。 可以进入容器 namespace，看一下容器内的 veth 设备。 $ docker exec -it br0_container bash # 以下在容器 namesapce 环境执行 root@676f7f9eab12:/# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.100.2 netmask 255.255.255.0 broadcast 192.168.100.255 ether 02:42:c0:a8:64:02 txqueuelen 0 (Ethernet) RX packets 1928 bytes 21609413 (21.6 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1817 bytes 102779 (102.7 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@676f7f9eab12:/sys/class/net/eth0# ethtool -i eth0 driver: veth version: 1.0 firmware-version: expansion-rom-version: bus-info: supports-statistics: yes supports-test: no supports-eeprom-access: no supports-register-dump: no supports-priv-flags: no root@676f7f9eab12:~# cat /sys/class/net/eth0/iflink 15 在容器内的仅仅有一个 eth0 网卡，ip 设置为了容器的 ip。但是其实这个网卡就是 veth 设备改了名字； 通过 ethtool -i eth0 命令看到，其对应 driver 是 veth，并且 /sys/class/net/eth0/iflink 文件表明了对端的 veth 网卡编号为 15（即宿主机看到的 veth 网卡设备）； 现在，我们试着启动容器并添加一个 tcp 端口映射。 $ docker run -dt --rm \\ --network=mybridge0 --publish 12211:8080 \\ --name br0_container ubuntu 2502f7397a37e2ab482f8a9152d1ed968dd2e2825c71eb2a6737e4900f7236c1 而这个端口映射，就是将宿主机的 12211 端口映射给容器的 8080，所以所有发往宿主机的 12211 端口的包，都会被修改端口并转发到容器内部。这也是通过 iptables 实现的： iptables -t nat -L -nv Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL Chain INPUT (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 10 617 MASQUERADE all -- * !mybr0 192.168.100.0/24 0.0.0.0/0 0 0 MASQUERADE tcp -- * * 192.168.100.2 192.168.100.2 tcp dpt:8080 Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- mybr0 * 0.0.0.0/0 0.0.0.0/0 0 0 DNAT tcp -- !mybr0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:12211 to:192.168.100.2:8080 PREROUTING -\u003e DOCKER 链中，所有不是从 mybr0 进入的包，并且发往 tcp 12211 端口的包，都会被 DNAT 为发往 192.168.100.2:8080。这样就实现了端口映射的功能。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:2","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.3 bridge 网络总结 中心思想：bridge 网络使用 bridge 网卡创建了一个本地的内网，而 bridge 网卡 + iptables 规则成为了这个内网的 ‘路由器'。其中： bridge 网卡作为二层的交换机，bridge 网卡 ip 作为路由器的网关 ip。 iptables 规则实现了 brdige 网卡与物理网络的连接 宿主机内核栈实现了这个 ‘路由器’ 的路由功能。 下图展示了整个 bridge 网络的模型（图片来自网络）： 其中比较关键的点： veth pair 设备将容器 net namespace 连接到 bridge 网卡（可以看做将 veth pair 作为网线插到了 bridge 这个 ‘路由器’ 上）。 iptables 实现了 bridge 网卡与物理网络的 ‘连接’。 bridge 网卡收到的包，经过 iptables 的 MASQUERADE 将包进行地址转换，并经过内核协议栈的路由通过物理网卡发送到物理网络。而回包通过 conntrack 机制正常接收与逆地址转换。 容器与宿主机的端口映射，也是通过 iptables 的 DNAT 实现的。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:3","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"4 Host 网络 Host 网络没啥好说的，启动容器不创建新的 namespace，依旧在宿主机的 net namespace 下。 $ docker run -dt --rm --network=host --name host_container ubuntu da1c426a7c7501b329258b12cb475ff42669837ca686d6e946511632461cc946 观察 mount，可以看到对应还是有 net namespace 的文件挂载，文件名为 default： $ mount … nsfs on /run/docker/netns/default type nsfs (rw) 文件 inode 对比当前宿主机 net namespace inode，是一致的： $ ls -lh /proc/self/ns/net lrwxrwxrwx 1 root root 0 Nov 7 14:47 /proc/self/ns/net -\u003e 'net:[4026531992]' $ ls -lhi /run/docker/netns/default 4026531992 -r--r--r-- 1 root root 0 Oct 30 16:50 /run/docker/netns/default ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:4:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5 macvlan 网络 macvlan 网络使用 macvlan 虚拟网络设备，将容器 net namespace 网络暴露在与当前宿主机同级的局域网内，相当于容器就是当前网络内的一台 “主机”。 macvlan 网络设备也包括多种模式：bridge mode、802.1q trunk bridge mode。下面示例都是基于普通的 brdige mode。 因为 macvlan 网络在虚拟机网络下不太好验证，所以下面例子来自于一台物理机上。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5.1 创建/删除 macvlan 网络 通过 docker network create 创建 macvlan 网络。 $ docker network create -d macvlan \\ --subnet=192.168.67.130/24 --gateway=192.168.67.1 \\ -o parent=eth0 mymacvlan0 633aae3d4f430352e5439e2650c02fe9c2092b99b5b8252f8141fa5d62ec7e70 -d macvlan，指定 macvlan 网络 -subnet=192.168.67.130/24，因为 macvlan 网络下的容器会直接连入物理网络，所以子网也是要在当前子网内； --gateway=192.168.67.1，同样，gateway 就是宿主机的网关地址； -o parent=eth0，指定 macvlan 设备链接的物理网卡，一定要是一个真正可联网的物理网卡； 不过与 bridge 网络不同的是，创建一个 macvlan 网络仅仅是记录其对应的配置，不会创建对应的 macvlan 网卡或者 iptables 规则。因为 macvlan 网卡是与 net namespace 绑定的，所以当创建 net namespace 时才会出现对应网络设备。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:1","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5.2 启停容器后的网络 $ docker run --net=mymacvlan0 \\ -dt --rm --name macvlan_container \\ --ip=192.168.67.139 --privileged \\ centos_ctr bash 2eff4835733734b6819c7f97ae41585985d95c1ea4c66a6a478a43e71b60b6d6 启动容器，如果不指定 IP，Docker 会在配置的网段里分配一个。 tip 为了能够方便排查网络问题，使用的容器镜像 centos_ctr 是由 centos 镜像以 host 网络启动，预装一些命令后，才由容器导出的镜像。 但是发现进入容器后，发现静态配置 IP 无法 ping 通网关（宿主机是正常无法 ping 通，因为内核会丢弃 macvlan 网卡的包）。研究后不清楚具体原因，但是这台宿主机接的是交换机，不知道是不是不是路由器导致的。 $ docker exec -it macvlan_container bash # 以下是容器中命令 [root@2eff48357337 /]# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.67.139 netmask 255.255.255.0 broadcast 192.168.67.255 ether 02:42:c0:a8:43:8b txqueuelen 0 (Ethernet) RX packets 433282 bytes 27463954 (26.1 MiB) RX errors 0 dropped 26809 overruns 0 frame 0 TX packets 91342 bytes 6649728 (6.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@2eff48357337 /]# ping 192.168.67.1 PING 192.168.67.1 (192.168.67.1) 56(84) bytes of data. From 192.168.67.139 icmp_seq=1 Destination Host Unreachable From 192.168.67.139 icmp_seq=2 Destination Host Unreachable 因此，换个思路，静态 IP 不行，就通过 DHCP 获取一个 IP 尝试是否能够连通网络。 在删除静态 IP 之后，调用 dhclient 从上层路由器获取一个 IP。 # 删除 eth0 网卡 IP [root@2eff48357337 /]# ip address del 192.168.67.139 dev eth0 Warning: Executing wildcard deletion to stay compatible with old scripts. Explicitly specify the prefix length (192.168.67.139/32) to avoid this warning. This special behaviour is likely to disappear in further releases, fix your scripts! [root@2eff48357337 /]# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether 02:42:c0:a8:43:8b txqueuelen 0 (Ethernet) RX packets 435540 bytes 27608133 (26.3 MiB) RX errors 0 dropped 26983 overruns 0 frame 0 TX packets 91763 bytes 6678670 (6.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 调用 dhclient 获取新的 IP [root@2eff48357337 /]# dhclient -r \u0026\u0026 dhclient -v Removed stale PID file Internet Systems Consortium DHCP Client 4.3.6 Copyright 2004-2017 Internet Systems Consortium. All rights reserved. For info, please visit https://www.isc.org/software/dhcp/ Listening on LPF/eth0/02:42:c0:a8:43:8b Sending on LPF/eth0/02:42:c0:a8:43:8b Sending on Socket/fallback DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 3 (xid=0xdf4e0e25) DHCPREQUEST on eth0 to 255.255.255.255 port 67 (xid=0xdf4e0e25) DHCPOFFER from 192.168.9.253 DHCPACK from 192.168.9.253 (xid=0xdf4e0e25) System has not been booted with systemd as init system (PID 1). Can't operate. Failed to create bus connection: Host is down bound to 192.168.9.235 -- renewal in 38783 seconds. [root@2eff48357337 /]# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.9.235 netmask 255.255.255.0 broadcast 192.168.9.255 ether 02:42:c0:a8:43:8b txqueuelen 0 (Ethernet) RX packets 435635 bytes 27614880 (26.3 MiB) RX errors 0 dropped 27001 overruns 0 frame 0 TX packets 91767 bytes 6679438 (6.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@2eff48357337 /]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.9.253 0.0.0.0 UG 0 0 0 eth0 192.168.9.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 可以看到，DHCP 获得的 IP 与宿主机都不是同一个网段的，并且网关地址也不是同一个，因此上层连着交换机有多个网段（这块不太理解了）。 但是，测试后是可以 ping 通网关，并且可以访问外网的： [root@2eff48357337 /]# ping 192.168.9.253 PING 192.168.9.253 (192.168.9.253) 56(84) bytes of data. 64 bytes from 192.168.9.253: icmp_seq=1 ttl=64 time=0.637 ms 64 bytes from 192.168.9.253: icmp_seq=2 ttl=64 time=0.250 ms ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:2","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5.3 总结 中心思想：将 macvlan 网络启动容器看做一个与宿主机同级的网络，其获取 IP 方式都与正常的机器相同。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:3","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"参考 Docker 容器网络官方文档 Linux 虚拟网络设备之 bridge（桥） Linux interfaces for virtual networking ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:6:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["VM"],"content":"制作虚拟机镜像","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"中心思想：通过 libvirt 运行一个虚拟机（domain），并保存其对应的 domain 的镜像文件与配置文件，然后就可以在其他机器通过 virsh define + start 或者 virt-install 启动。 说明：下面环境都是在 centos 上制作基于 KVM 的虚拟机镜像。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:0:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"1 从 ISO 镜像安装 最基本的安装方式，通过安装并运行一个新的虚拟机，然后得到对应的配置文件与镜像文件。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:1:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"1.1 手动安装 最基本的安装方式，通过 ISO 文件进行安装。 下载 ISO 镜像文件，镜像文件在各个镜像站中就可以找到。（因为不使用图形界面，所以下载的是非图形化的 ISO） 通过 virt-install 命令安装镜像（因为使用的是非图形化的安装，所以参数有一些不一样） virt-install --name guest1_fromiso --memory 2048 --vcpus 2 \\ --disk size=8 --location CentOS-7-x86_64-DVD-2003.iso \\ --os-type Linux --os-variant=centos7.0 --virt-type kvm \\ --boot menu=on --graphics none --console pty --extra-args 'console=ttyS0' 其中要注意几个参数： 因为我们是安装非图形化，所以需要 --location 参数指定 iso，并指定 --boot menu=on 打开安装菜单，最后还需要指定安装信息的输出 --console pty --extra-args 'console=ttyS0' 这样安装菜单才能正常展示出来 --graphics none 指定非图形化； --network bridge=virbr0，指定网络模式，这里指定 libvirt 默认创建的 bridge 网卡，可以认为这是一个 libvirt 维护内网，安装时选择 dhcp 就可以获得一个可用的内网地址； 具体 libvirt 的网络模型，后面在单独研究下。 -- disk size=8，disk 参数用于指定系统盘，这里指定自动创建一个 8G 的 qcow2 文件，作为系统盘（默认镜像文件保存在 /var/lib/libvirt/images/）目录下； 这时就会进入虚拟机的安装步骤，具体安装步骤就不赘述了。 安装成功后，可以看到 domain 就被创建了，这就可以得到它的配置文件与镜像文件了。 $ virsh list Id Name State ---------------------------------------------------- 18 guest1_fromiso running ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:1:1","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"1.2 自动安装 可以看到，手动安装需要人为在菜单中选择、配置，这不适用于多个虚拟机的安装。而 RedHat 创建了 kickstart 安装方法，使得整个虚拟机安装流程变得自动化。 这块不了解，具体见红帽官方文档：KICKSTART INSTALLATIONS ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:1:2","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"2 使用 Cloud Image 当然，上面制作过程中耗时都在安装系统上了，而各个云厂商的虚拟机数量那么多，肯定不会一台台去安装操作系统了。所以，目前最常见的都是直接下载已经安装过系统的虚拟机镜像文件。 但是这样的虚拟机是没有特殊配置的，例如密码、hostname 都是一致的，所以 cloud-init 出现，用于在第一次启动虚拟机时进行系统的配置。 所以，最快速的制作方法就是：虚拟机镜像文件 + cloud-init 配置。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:2:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"2.1 cloud-init 下面内容都来自于文档，这里仅为自己做个记录。 首先明确 cloud init 的功能：系统第一次启动时，cloud init 相关的进程会根据配置信息去进行系统的配置，包括：设置 hostname、ssh key、password 等； (1) 基本概念 metadata：包含服务器信息，用于 cloud-init 配置； userdata：包含 cloud-init 系统配置信息，可以是 文件，脚本，yaml 文件等； datasource：cloud-init 读取配置数据的来源，包含大部分云厂商，当然，也可以来自本地的文件 (NoCloud datasource)； (2) 运行过程 cloud init 设置包括五个阶段： Generator 机器启动阶段，systemd 的一个 generator 将会决定是否将 cloud-init.target（target 可以简单认为特定事件下触发的一组 unit）包含在启动过程中。这就表示启动 cloud-init。 默认情况下，generator 会启动 cloud-init，除非以下情况： /etc/cloud/cloud-init.disabled 文件存在； 内核启动命令行 /proc/cmdline 中包含 “cloud-init=disabled”。如果是容器中运行，会读取环境变量 KERNEL_CMDLINE； 而下面的步骤，就是由 target 包含的各个 unit 执行的。 Local 由 cloud-init-local.service 执行，主要目的：找到 “local” 的 datasource，根据配置网络。 配置网络有三种情况： 首先，根据传入配置 “metadata” 配置网络； 当上面情况失败，直接配置 “dhcp on eth0”； 如果 /etc/cloud/cloud.cfg 配置文件中禁用了网络：network: {config: disabled}，那么就不进行网络配置； Init、Config、Final 阶段 对应 service 为 cloud-init.service、cloud-config.service、 cloud-final.service。 通过 local 阶段，网络已经配置好了，并且已经得到了 metadata。而 /etc/cloud/cloud.cfg 配置定义了剩下三个阶段对应的任务，也就是 module。 cloud init 通过一些缓存信息来判断机器是否经过初始化，通过 cloud-init clean 也可以手动清理缓存信息。 /var/log/cloud-init.log 记录了 cloud-init 运行的完整过程。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:2:1","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"2.2 制作镜像 下面就开始进行镜像制作： 下载 cloud image，这里使用中科大提供的： $ wget https://mirrors.ustc.edu.cn/centos-cloud/centos/7/images/CentOS-7-x86_64-GenericCloud-2003.qcow2 修改密码 当然，该镜像其实就可以直接进行 virt-install 来启动（因为我们没有配置文件，所以通过 virt-install 来启动并生成配置文件），但是不知道密码，也搜不到，无法登陆进入。不过你也可以使用下面命令来设置密码后进行登陆： $ virt-customize -a CentOS-7-x86_64-GenericCloud-2003.qcow2 --root-password password:yourpassword 因为 cloud-init 需要一个 datasource，而我们没有使用云厂商，所以使用 NoCloud 形式，按照官方的 s 示例创建一个 disk 文件。 # 创建 user-data 与 meta-date 配置文件 $ cat meta-data instance-id: guest1 local-hostname: guest1 $ cat user-data #cloud-config chpasswd: expire: false list: | root: password1 ssh_pwauth: True # 生成 disk 文件，包含 userdata 与 metadata 配置数据 $ genisoimage -output seed.iso -volid cidata -joliet -rock user-data meta-data 创建并运行虚拟机。 $ virt-install --memory 2048 --vcpus 2 --name guest2 \\ --disk CentOS-7-x86_64-GenericCloud-2003.qcow2 --disk seed.iso \\ --os-type Linux --os-variant centos7.0 --virt-type kvm \\ --graphics none --network default \\ --import 几个比较重要的参数： --disk CentOS-7-x86_64-GenericCloud-2003.qcow2：制定系统盘； --import：跳过安装过程，因为已经安装好操作系统，不需要进行安装过程； --disk seed.iso：传递 cloud-init datasource 信息； 虚拟机启动过程中，可以看到 cloud-init 配置信息的一些打印： 最后根据配置的密码成功进入： 而 CentOS-7-x86_64-GenericCloud-2003.qcow2 就是虚拟机经过配置的镜像文件，而 libvirt 启动所需的配置文件就是 /etc/libvirt/qemu/guest1.xml。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:2:2","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"参考 CREATING GUESTS WITH VIRT-INSTALL KICKSTART INSTALLATIONS cloud-init Documentation ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:3:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["k8s 实践"],"content":"使用 PV 与 PVC 为 Pod 提供存储","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/pv-pvc-%E4%B8%8E-storageclass/","tags":["k8s","云计算"],"title":"PV PVC 与 StorageClass","uri":"/posts/cloud_computing/k8s_practice/pv-pvc-%E4%B8%8E-storageclass/"},{"categories":["k8s 实践"],"content":"1 PV 与 PVC 目的：使用 NFS 做 PV，创建 Pod 使用该 PV Node-1 构建 nfs 服务，位于 “/nfs” 目录。 创建 PV，使用 nfs 类型。 声明、创建 PVC，使用上述指定的 StorageClass，形成指定的绑定关系。 pvc 状态为 Bound，表明已经成功绑定到了 pvc。查询 pv，可以看到 pv 也是被绑定了。(注意：一个 PV 只能绑定一个 PV) 创建 Pod Deployment，在 Pod 的 Volume 使用 PVC。 其中两个 Pod 分别调度到了 Node-2 Node-3，在 Node-2 Node-3 中可以看到对应的 nfs mount： 对应容器配置也可以看到指定的挂载： 在 Node-3 节点的容器环境内写入挂载路径文件，可以看到同步到了主节点的 /nfs 目录上，因此，存储配置成功。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/pv-pvc-%E4%B8%8E-storageclass/:1:0","tags":["k8s","云计算"],"title":"PV PVC 与 StorageClass","uri":"/posts/cloud_computing/k8s_practice/pv-pvc-%E4%B8%8E-storageclass/"},{"categories":["k8s 实践"],"content":"使用 RBAC 进行访问控制与授权","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"RBAC 实践","uri":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"Kubernetes 中，通过 RBAC 机制来实现集群 Pod 中对 APIServer 的访问权限控制与授权。 RBAC 机制有三个最基本的概念： Role：一组规则，定义了对 API 对象的操作权限； Subject：被作用者，集群内部常常使用的是 ServiceAccount； RoleBinding：绑定 Role 与 Subject； ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"RBAC 实践","uri":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 Role 与 ServiceAccount 实践 目的：通过 Role 与 RoleBinding 限制一类 ServiceAccount，并在 Pod 中使用该 ServiceAccount 观察权限控制。 创建需要使用的自定义 namespace [mynamespace] 创建需要限制访问权限的 ServiceAccount [example-sa] 可以看到，每个 namespace 有个默认的 ServiceAccount default，提供完整的 APIServer 访问权限。 每个 ServiceAccount 在容器维度看到就是 Secret 对象，包含证书内容。 创建 Role，定义允许的权限规则。 可以看到，rules 指定了该 Role 为：允许对 mynamespace 下的 pod 进行 get、watch、list 操作。 创建 RoleBinding，关联刚刚创建的 Role 与 ServiceAccount。 创建 Pod，指定使用的 ServiceAccount，在 Pod 内观察权限是否被限制了 进入容器中，可以看到，k8s 将 ServiceAccount 对应的 Serects 对象挂载到了 /run/secrets/kubernetes.io/serviceaccount 目录下，包含 client 需要使用的【证书 ca.crt】、【token】: ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"RBAC 实践","uri":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 CRD 自定义资源，通过 Kubernetes 编排","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"CRD 是 Kubernetes 可扩展性的第一个体现，因为 Kubernetes 提供的是一个编排的框架，因此不止可以对 Pod 进行编排，也支持通过 CRD 对你自定义的类型的编排。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 CRD 构建 目的：通过 CRD 构建一个自定义资源。 编写 CRD manifest，完成自定义资源的定义。 可以看到，CRD 中只有类型的简单定义，没有该 CR 的元属性的定义，因为这些需要通过代码中定义。 通过 kubectl apply 创建 CR 对应 CRD 对象，让 Kubernetes “认识” 这个自定义资源。 调用 kubectl apply 创建自定义资源： 这里其实 Kubernetes 不知道该资源具体的代码类型，它只是知道有 CRNetwork 这个资源，并支持创建删除，将其保存下来了。 编写 CR 相关定义代码。其实这里编写代码是为生成 kubectl 的 Client，使其在编写 Controller 时候能够正确的解析你的 CR 对象。 整个的目录结构如下： 具体代码见仓库：k8s_practice 通过 k8s 生成代码，最终生成的目录结构如下： ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2 编写自定义控制器 因为 Kubernetes 中是基于声明式 API 的业务实现，所以需要控制器来“监控”对象变化，执行对应的操作。 编写自动以控制器主要有三个过程：编写 main 函数、编写自定义控制器定义，编写控制器业务逻辑。 整个实践代码见：k8s_practice Controller 代码编写。主要逻辑：处理 Informer 通知的 Event，执行 CR Sync 操作。 Controller 主要包含三个部分： Informer：包含从 APIServer 同步的 CR 对象的 Cache，并且处理 CR Event，调用 Event Handler。 Event Handler：Informer 的各个 Event 调用的事件回调，一般都是放入 workQueue，延后处理。 Workers：根据各个 Event 进行真正的业务处理，例如真实资源的创建、删除、更新等。 main 函数代码编写。主要逻辑：创建 Informer、Controller，执行 Controller 的启动。 编译后运行 Controller。 第一同步后，所有的 CR 对象都可以被任务是“新添加的”，因此会一个个调用 HandleAdd 接口。上图可以看到，因为集群中已经有了一个 CR 对象，因此 Controller 会进行该对象的 Sync。 创建一个新的 CR 对象 example-crnetwork-2，观察 Controller。 可以看到 Controller 处理完成。 删除刚创建的 example-crnetwork-2，观察 Controller。 可以看到 Controller 正确的执行删除的逻辑。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/:2:0","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"3 将自定义资源控制封装为 Pod TODO ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/:3:0","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 Job 或者 CronJob 部署一次性任务","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"Job CronJob 实践","uri":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"Job 用于运行一次性的任务，即“离线任务”。CronJob 在 Job 之上提供了周期性任务支持。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"Job CronJob 实践","uri":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 Job 目的：部署 Job 任务，使用并行运行（Batch）的功能。 创建部署 Job 任务。 “img1.png” \u003e}} 可以看到，并行的两个 Pod 正在运行。 经过一段时间可以看到，因为设置 deadline 为 100，所以 pod 异常被退出。而 restartPolicy: Never 使得不会再次运行。 去除 deadline 设置，重新部署，可以看到，本次 4 次成功完成。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"Job CronJob 实践","uri":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2 CronJob 目的：体验 CronJob 的周期性任务功能，明确 CronJob 是基于 Job 管理实现的。 创建部署 CronJob 。可以看到，CronJob 中需要指定 JobTemplate，因此 CronJob 完全是基于 Job 管理的。 部署后可以看到，CronJob 创建了一个 Job： 经过一分钟，新 Job 被创建，并运行成功： ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/:2:0","tags":["k8s","云计算"],"title":"Job CronJob 实践","uri":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 DaemonSet 部署常驻容器","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/daemonset-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"DaemonSet 实践","uri":"/posts/cloud_computing/k8s_practice/daemonset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"DaemonSet 会为匹配的 Node 运行一个 Daemon Pod，与 Deployment 类最大不同，DaemonSet 没有副本的概念。 目的：部署 DaemonSet，试用 toleration 与 nodeAffinity，观察滚动升级流程。 部署 DaemonSet。其中，是用 nodeAffinity 指定选择调度的节点，使用 toleration 容器 Node 的 taint: 创建后，可以看到，只选择调度到了 node-1 node-2 节点，和配置的 Affinity 匹配： 改变 DaemonSet 使用的镜像版本，观察滚动升级流程: 通过 kubectl rollout status 可以看到，滚动升级流程与 Deployment 过程一致： 观察 Event，可以看到也是按照 delete -\u003e create 的流程进行升级的。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/daemonset-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"DaemonSet 实践","uri":"/posts/cloud_computing/k8s_practice/daemonset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 StatefulSet 进行副本控制","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"Deployment 对于“无状态”的任务，已经能够做到副本控制，滚动升级功能了。但是 Deployment 无法适用于“有状态”的任务，因为其中 Pod 都是相同的，没有任何的对应关系。 而 StatefulSet 通过“固定命名、域名的 Pod，以及固定的创建顺序”作为基础，加上与命名对应的网络、存储，搭建一个“有状态”Pod 管理。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 HeadlessService StatefulSet 会使用 HeadlessService 的概念，首先来部署 HeadlessService。 创建描述文件 headless_service 可以看到，Headless 与普通 Service 最大区别是 clusterIP 为 None。 通过 kubectl create 部署 HeadlessService，成功后可以看到： 查看 Endpoint，可对应的 Endpoint 包含了 Node-1 Node-2 的 Pod 的地址了。并且 Endpoint 命名和Service 名字一样。 你按照这样的方式创建了一个 Headless Service 之后，它所代理的所有 Pod 的 IP 地址，都会被绑定一个这样格式的 DNS 记录： \u003cpod-name\u003e.\u003csvc-name\u003e.\u003cnamespace\u003e.svc.cluster.local 但是好像单独使用 Headless Service 是无法访问这些域名的。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2 StatfulSet ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:2:0","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.1 StaefulSet 的固定域名 下面开始部署 StatefulSet： 先要创建对应的 Headless Service，如前面一样。 创建对应的 StatefulSet 描述文件： 可以看到，StatefulSet 与 Deployment 最大的区别就是指定了 serviceName 字段，指定了使用的 HeadlessService。 创建 StatefulSet 对象，类似于 Deployment，开始创建 Pod 了 但是 StatefulSet 并没有创建任何的 ReplicaSet，所以实现上与 Deployment 不一样： 观察创建出的 Pod，可以看到，其 Pod 命名不是加上随机字符串了，而是有序的数字： 并且，其创建顺序也是有序的，先创建 web-0 ，web-0 运行后并 Ready 后，创建 web-1。 运行一个 busybox 测试容器，执行 nslookup 访问 HeadlessService 为其绑定的域名，可以看到正常返回了。 删除 web-0 Pod，可以看到 StatefulSet 会重新立刻创建同名的 Pod。所以，Pod 名字是固定的 再次通过 web-0.nginx.default.svc.cluster.local 进行域名解析，还是能够正确的解析： 注意：虽然域名可以正确解析，但是其域名对应的 IP 不是保证固定的，所以不能保存 Pod 的 IP ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:2:1","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.2 StatefulSet 的固定存储 目的：使用 StatefulSet 中的 Template PVC 自动构建持久化存储，并观察其 PVC 是否与 PodName 绑定。 构建 nfs 的 2 个 PV，给 2 个 Pod 准备。（构建过程见\u003cPV、PVC 与 StorageClass\u003e一节） 创建 StatefulSet，其指定 PVC 模板，使得能够为每个 Pod 自动创建其对应的 PVC。 创建后可以看到，StatefulSet 为 2 个 Pod 创建了对应的 PVC： 可以看到，PVC 是和名字对应的，格式为 [volume name]-[pod name]，因此，PVC 与 Pod 名字的映射关系是固定的。 所以这就实现了，当旧 Pod 删除，新 Pod 被创建后，因为其 Pod Name 没有变，所以就找到了旧 Pod 使用 PVC。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:2:2","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 Deployment 进行部署","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 ReplicaSet Deploment 管理的是 ReplicaSet，所以先运行 ReplicaSet 观察。 ReplicaSet 只包含副本控制功能，没有滚动升级等高级的功能。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1.1 部署 ReplicaSet 创建 manifest 文件。 调用 kubectl create 创建资源。 观察下 ReplicaSet 的事件，可以看到各个 Pod 的创建流程。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:1:1","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1.2 副本保证 目前，3 个 Pod 都运行在了 Node-3 上： 下线 Node-3 ，可以看到 node-3 变为 NotReady: 过了一段时间后，原来三个 Pod 变为 Terminating 状态，而新的 Pod 被创建被调度。 可以看到，新创建 3 个 Pod 被调度到了 Node-2 上: 恢复 Node-3 上线，kubelet 会同步任务，因此不会再次运行旧的三个 Pod。Pod 的状态也从 Terminating 变为被删除。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:1:2","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1.3 水平缩扩 通过 kubectl scale 进行副本扩展: 可以看到，新的 Pod 在 Node-3 被运行： 依旧 kubectl scale 进行副本缩容，可以看到，两个 Pod 被停止: 可以看到，ReplicaSet 启动和停止任务都是由 Scheduler 选择的，而不能认为的控制选择指定的 Pod，也就是说，所有的 Pod 应该被认为是“无状态的”，随时可能被停止。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:1:3","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2 Deployment Deployment 操作与管理的是 ReplicaSet，而不是 Pod。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:0","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.1 部署 构建 manfiest 文件: 通过 kubectl create 创建 Deployment。 可以看到，Event 中打印的是对应的ReplicaSet 的自动被创建，所以Deployment 是创建 ReplicaSet 的。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:1","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.2 水平扩展 Deployment 水平扩展方式与 ReplicaSet 一致，并且就是操作 ReplicaSet 的 replica 的值来实现，跳过。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:2","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.3 滚动升级 (1) 正常流程 Deployment 在 ReplicaSet 基础上添加了“滚动升级”的功能。 依旧创建 Deployment，并直接扩展到副本数为 3。 修改 Deployment 的配置文件，将 image 版本升级。 可以看到，Deployment 新建了一个 ReplicaSet （部署新版本 Pod），而旧的 ReplicaSet 副本变为了 0。 通过 Deployment 的 Event 可以看到，旧版 ReplicaSet 的副本数逐渐减少，而新版本 ReplicaSet 副本数逐渐增加。这样使得集群中 Pod 会维持在一个最低数量（示例中为 3） (2) 错误流程 观察下当升级出现错误时，Deployment 会处于怎样的状态。 通过 kubectl set image 将 Deployment 使用镜像变为一个不存在的镜像。 通过 Event 看到，滚动升级停止在了最新版本的 Replicaset 的第一个副本部署。 因为新旧版本是交替部署的，所以当第一个副本部署失败时，也就不会继续进行旧版本 Pod 的停止了。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:3","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.4 回滚 在错误流程看到，当发布错误的版本后，Deployment 会停止新版本的发布，而这时，就需要通过 kubectl rollout 进行 Deployment 的回滚。 执行 kubectl rollout history，查看每次 Deployment 变更对应的版本。（因为 -record，所有的 kubectl 命令都会被记录）。 可以看到，两次的版本变更都被记录了下来。 通过 –revision 参数，查看对应的命令细节。 通过 kubectl rollout undo 进行版本的回退，默认为上一次版本，通过 –to-revision 可以执行回退的版本。 事实上，回退也是一次“升级”，通过 history 可以看到一个新的部署记录： 这个 4，就是最新的一次回滚执行的命令了。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:4","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"单机使用虚拟机搭建 k8s 集群","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"1 虚拟机集群搭建 目标：创建 3 个虚拟机，用作一个 Master Node，两个 Work Node；当然，三个节点处于同一个网段。 具体步骤如下: 构建节点 构建三个虚拟机，基于 centos 7、内存 2 GB，并通过虚拟机复制功能（其实就是 copy 系统盘），完全复制出 Node 1，Node 2，Node 3。 搭建网络 三个节点需要互相访问，所以将其位于 VirtualBox 创建的 Nat网络下，给予每个 Node 静态的 IP（10.0.2.10 - 10.0.2.12），为了方便访问，并设置 ssh 的 DNAT。 设置每个虚拟机网卡加入其创建的 “NodeNatNetwork”。例如： 启动每个虚拟机，设置其 hostname，与网卡静态 IP。例如： 至此，三个虚拟机位于同一个网段，并且能够相互访问；对外，则通过 VirtualBox 的 Nat 网络能够访问。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:1:0","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2 部署 K8s 目标：通过 kubeadm 部署整个 k8s，用 Node-1 为 Master 节点，其他为工作节点。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:0","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2.1 安装 kubeadm、kubelet、kubectl 安装 kubeadm、kubelet、kubectl。这个官方文档写的很详细，见 Installing kubeadm 。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:1","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2.2 kubeadm init 初始化 Master 节点 Node-1 节点执行 kubeadm init，将其作为 Master 节点初始化。执行成功后，kubeadm 生成了 Kubernetes 组件的各个配置，以及提供服务的各类证书，位于 /etc/kubernetes 目录下: 并且已经以 static pod 的形式启动了：apiserver、controller-manager、etcd、scheduler。 还有最重要的，kubeadm 为集群生成一个【bootstrap token】，需要加入集群的节点都需要通过这个 token 加入。 * 问题 kubeadm 检查 swap 打开着，kubeadm 推荐不使用 swap，通过 swapoff -a 关闭交换区。 kubectl 默认通过 8080 端口访问，无法执行。 设置 kubectl 的配置文件为 kubeadm 生成的 /etc/kubernetes/admin.conf。（其实就是配置公钥，或者将 admin.conf 移动到 ~/.kube/config 文件，作为默认配置。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:2","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2.3 kubeadm join [token] 设置工作节点 通过 kubead init 最后返回的提示信息，执行对应 kubeadm join 将 Node-1 Node-2 加入到集群中，作为工作节点。 [root@Node-2 kubeadm join 10.0.2.10:6443 --token mahrou.d3uodof21i3d6yxk --discovery-token-ca-cert-hash sha256:21dfe4ef6b3bbd89f803bf44ff6eda587874336d103d0e4a3b --v 5 可以看到，kubelet 启动后就通过 pod 方式启动了本节点上 kube-proxy 容器： * 问题 无法访问到 Node-1 节点，nc ip 失败，但是可以 ping 通。通过在 Node-1 tcpdump 可以抓取到来自 Node-3 的包，因此应该是防火墙的问题，通过 iptables 对 Node-2 Node-3 IP 开放。 kubectl 无法访问问题，与上述问题一致。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:3","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2.4 结果 目前为止，就完成了集群的搭建，但是 通过 kubectl get nodes，可以看到所有节点都是 NotReady： kubectl describe node node-1 可以看到，原因是因为没有设置正确的 Network Plugin： ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:4","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"3 部署网络插件 目的：部署网络插件，使各个节点为 Ready 状态，并其内部 Pod 能够相互通信。 以 Weave 部署为例，部署网络插件： kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" 其描述文件中定义所有 Weave 需要的 BAAC 权限组件，以及最重要的网络插件 Pod 对应的 DaemonSet: 应用成功后，可以看到对应的 DaemonSet 就运行起来，并开始给三个 Node 部署 Pod: 在节点上，可以看到 weave-net 对应的 pod ，包括两个容器： ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:3:0","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"4 部署容器存储插件 目的：为了能够让容器使用网络存储，使得容器数据持久化，需要部署存储插件。 以 Rook 项目为例，部署存储插件： $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/exampleskubernetes/ceph/common.yaml $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/exampleskubernetes/ceph/operator.yaml $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml 安装成功后，可以看到，rook 有着自己的 namespace，并且已经部署了 DaemonSet： 可以看到，Pod 也部署成功了： ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:4:0","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":null,"content":"博客使用 Hugo 框架进行搭建, 主题使用 LoveIt. Logo 由 gopherize.me 生成, 并使用 realfavicongenerator.net 转化. ","date":"2020-10-15","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"}]