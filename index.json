[{"categories":["网络"],"content":"Domain，Name Server","date":"2021-12-23","objectID":"/posts/net/dns/","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"主机使用 IP 地址作为通信地址是很难记忆的，用户更愿意使用比较容易记忆的 Hostname。而 Domain Name System（DNS）就是用于将 Domain 转换为 IP 地址。 ","date":"2021-12-23","objectID":"/posts/net/dns/:0:0","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"1 Domain 体系 Domain 有着严格的层次划分，例如 main.cctv.com. 就由三个标号组成，其中 . 为根，com 为顶级域名，cctv 为二级域名，mail 为三级域名。 Note 因为域名的根永远是 . 号，因此用户在使用时常常会省略，域名解析程序会自动加上根 . 号。 根据其层级结构，main.cctv.com 也被称为 cctv.com 的 SubDomain。 ","date":"2021-12-23","objectID":"/posts/net/dns/:1:0","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"2 Domain Server Domain Server（也称为 Name Server），是用于处理 DNS 解析请求的服务区。 基于 Domain 的分层设置，用于处理 Domain 转换为 IP 的服务器也是按照不同层级分类。 根域名服务器 - 最顶级的域名服务器 全球一共有 13 台根域名服务器，每台根域名服务器知晓所有的顶级域名服务器。 根域名服务器通常不会直接对域名进行解析，而是根据其顶级域名，返回对应的顶级域名服务器的 IP 地址。 顶级域名服务器 - 负责所有顶级域名 同样，顶级域名服务器收到 DNS 请求时，可能直接回复 IP 地址，也可能返回下一级权限服务器的 IP 地址。 权限域名服务器 - 负责管理某个区域的域名 本地域名服务器 - 域名解析的入口 本地域名服务器并不负责域名的某一层级，而是作为域名解析的入口。主机发起的 DNS 解析请求都会发送到本地域名服务器，由本地域名服务器来执行域名解析的过程。 每个因特网服务商 ISP，甚至一个大学里都可以有着本地域名服务器。 本地域名服务器也被称为 DNS Resolver，域名服务器也被称为 Name Server。 ","date":"2021-12-23","objectID":"/posts/net/dns/:2:0","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"3 域名解析过程 主机向本地域名服务器的查询一般是递归查询：如果本地服务器无法处理 DNS 请求，那么就由本地域名服务器向其他上级域名服务器发出 DNS 请求。 为了提高 DNS 查询效率，域名服务器会缓存 Domain-\u003eIP 的映射关系，并在设定的 TTL 时间后清理缓存记录。不仅仅是域名服务器，许多主机也会在本地维护自己的域名缓存。 ","date":"2021-12-23","objectID":"/posts/net/dns/:3:0","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"4 DNS Record 域名服务器中保存着多个 DNS Record，接受到 DNS 解析请求后，会查询保存的 DNS Record 来进行回复。 一个 DNS Record 可以理解为 Domain -\u003e Content 的映射。根据其 Content 的不同内容，DNS Record 分类不同的类型： A - Content 为 IP 地址 AAAA - Content 为 IPv6 地址 CNAME - Content 为另一个权威域名canonical name MX - Content 为邮件交换服务器mail exchange NS - Content 为另一个域名服务器name server TXT - Context 是对该 Domain 的描述信息 ALIAS - （AWS Route 53 特有）Content 为另一个 Domain Note DNS Record 的类型就是 DNS 报文中的 Type。 ","date":"2021-12-23","objectID":"/posts/net/dns/:4:0","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"4.1 A 类型 A 类型 DNS Record 表示 Domain 对应的 IP 地址。当域名服务器接受到 DNS 请求时，查询到 Domain 对应的 DNS Record 是 A 类型，那么就可以直接返回 IP 地址。 $ dig www.google.com ; \u003c\u003c\u003e\u003e DiG 9.16.1-Ubuntu \u003c\u003c\u003e\u003e www.google.com ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 55926 ;; flags: qr rd ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; WARNING: recursion requested but not available ;; QUESTION SECTION: ;www.google.com. IN A ;; ANSWER SECTION: www.google.com. 0 IN A 172.217.26.228 ;; Query time: 0 msec ;; SERVER: 172.23.64.1#53(172.23.64.1) ;; WHEN: Sat Dec 25 22:42:48 CST 2021 ;; MSG SIZE rcvd: 62 ","date":"2021-12-23","objectID":"/posts/net/dns/:4:1","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"4.2 AAAA 类型 AAAA 类型 DNS Record 表示 Domain 对应的 IPv6 地址。 dig 命令默认仅仅查询 A 记录，需要通过命令行参数指定： $ dig www.google.com AAAA ; \u003c\u003c\u003e\u003e DiG 9.16.1-Ubuntu \u003c\u003c\u003e\u003e www.google.com AAAA ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 57495 ;; flags: qr rd ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0 ;; WARNING: recursion requested but not available ;; QUESTION SECTION: ;www.google.com. IN AAAA ;; ANSWER SECTION: www.google.com. 0 IN AAAA 2404:6800:4004:801::2004 ;; Query time: 40 msec ;; SERVER: 172.23.64.1#53(172.23.64.1) ;; WHEN: Sat Dec 25 22:44:47 CST 2021 ;; MSG SIZE rcvd: 74 ","date":"2021-12-23","objectID":"/posts/net/dns/:4:2","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"4.3 CNAME 类型 CNAME 类型表示使用别名的权威域名canonical name。也就是说，CNAME 是 Domain 的域名的别名。 当 Name Server 返回 CNAME 记录时，表明 DNS Resolver 应该通过返回的 CNAME 继续进行 DNS 解析。 例如，当我们使用 CDN 服务时，用户提供源站后，通常 CDN 服务会提供一个 CNAME，例如 “xx.cdn.dnsv1.com”。而这时，CDN 服务器会在 Name Server 添加一条 DNS Record。 CNAME \u003csource site\u003e \"xx.cdn.dnsv1.com\" 这样，用户访问 source site 时后会返回 CNAME 地址，接着用户就会对 CNAME 地址的服务器发起 DNS 解析以及访问，从而用户就放回到了 CDN 的缓存服务器。 ","date":"2021-12-23","objectID":"/posts/net/dns/:4:3","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"4.4 MX 类型 MX 记录，表示邮件交换mail exchange服务。每个邮件厂商都有一个自己的域名，查询该域名后，就会返回对应的邮件服务器的域名。 $ dig qq.com MX ; \u003c\u003c\u003e\u003e DiG 9.16.1-Ubuntu \u003c\u003c\u003e\u003e qq.com MX ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 1841 ;; flags: qr rd ad; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0 ;; WARNING: recursion requested but not available ;; QUESTION SECTION: ;qq.com. IN MX ;; ANSWER SECTION: qq.com. 0 IN MX 20 mx2.qq.com. qq.com. 0 IN MX 30 mx1.qq.com. qq.com. 0 IN MX 10 mx3.qq.com. ;; Query time: 10 msec ;; SERVER: 172.23.64.1#53(172.23.64.1) ;; WHEN: Sat Dec 25 23:10:03 CST 2021 ;; MSG SIZE rcvd: 108 Note MX 类型的出现是历史的原因，现在理论上可以使用 A 类型记录替代。 ","date":"2021-12-23","objectID":"/posts/net/dns/:4:4","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"4.5 NS 类型 NS 类型记录，表示该 Domain 需要继续访问其他的 Name Server 进行解析，即该 DNS Record 会返回另一个 Name Server 的域名。 以访问 qq.com，当 DNS 请求发到 com Name Server 时，com 根据自身的 NS 记录告知应该去哪个 Name Server 继续进行 DNS 解析。 因此，NS 类型实际上实现了 Domain 的层级结构，并形成了迭代的 DNS 解析流程。 $ dig qq.com NS ; \u003c\u003c\u003e\u003e DiG 9.16.1-Ubuntu \u003c\u003c\u003e\u003e qq.com NS ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 25101 ;; flags: qr rd ad; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 0 ;; WARNING: recursion requested but not available ;; QUESTION SECTION: ;qq.com. IN NS ;; ANSWER SECTION: qq.com. 0 IN NS ns4.qq.com. qq.com. 0 IN NS ns1.qq.com. qq.com. 0 IN NS ns2.qq.com. qq.com. 0 IN NS ns3.qq.com. ;; Query time: 60 msec ;; SERVER: 172.23.64.1#53(172.23.64.1) ;; WHEN: Sat Dec 25 23:15:17 CST 2021 ;; MSG SIZE rcvd: 126 ","date":"2021-12-23","objectID":"/posts/net/dns/:4:5","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["网络"],"content":"4.6 TXT 类型 TXT 类型记录不是实际上影响 DNS 解析的记录，而是保存一些文本信息，其中一些信息是 DNS 解析的配置。 ","date":"2021-12-23","objectID":"/posts/net/dns/:4:6","tags":["网络"],"title":"DNS 域名解析系统","uri":"/posts/net/dns/"},{"categories":["Kubernetes 实践"],"content":"1 概述 ExternalDNS 用于将 Kubernetes 集群中的 Service/Ingress 暴露的服务同步给外部的 DNS 服务商，例如 AWS Route53、GCP CloudDNS 等。 ExternalDNS 本身并不是一个 DNS Nameserver，而是负责将 Kubernetes 集群的 DNS 记录写入到外部的 DNS Nameserver。 ","date":"2021-12-22","objectID":"/posts/cloud_computing/k8s_practice/use-external-dns/:1:0","tags":["k8s","云计算"],"title":"K8s 实践 - 使用 External DNS","uri":"/posts/cloud_computing/k8s_practice/use-external-dns/"},{"categories":["Kubernetes 实践"],"content":"2 为 AWS Route53 设置 External DNS ","date":"2021-12-22","objectID":"/posts/cloud_computing/k8s_practice/use-external-dns/:2:0","tags":["k8s","云计算"],"title":"K8s 实践 - 使用 External DNS","uri":"/posts/cloud_computing/k8s_practice/use-external-dns/"},{"categories":["Kubernetes 实践"],"content":"2.1 启动 EKS 集群 执行 eksctl create cluster 命令启动一个 EKS 集群。 $ eksctl create cluster --name test-cluster --with-oidc --managed --region us-west-2 --nodes 3 注意，这里必须使用 --with-oidc 参数来创建 OIDC 服务，因为要让 ServiceAccount 绑定 IAM Policy 需要 OIDC 服务提供。 ","date":"2021-12-22","objectID":"/posts/cloud_computing/k8s_practice/use-external-dns/:2:1","tags":["k8s","云计算"],"title":"K8s 实践 - 使用 External DNS","uri":"/posts/cloud_computing/k8s_practice/use-external-dns/"},{"categories":["Kubernetes 实践"],"content":"2.2 准备 ServiceAccount 因为 ExternalDNS 需要将 DNS 记录同步给 AWS Route53，因此需要为 ExternalDNS 准备一个绑定了 IAM Role 的 ServiceAccount。 ExternalDNS 所需权限的 IAM Policy 如下，可以看到其需要 Route53 的修改与查看权限。 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\" ], \"Resource\": [ // 这里给予了访问所有 hostzone 的权限，也可以限制为你需要的 \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ListHostedZones\", \"route53:ListResourceRecordSets\" ], \"Resource\": [ \"*\" ] } ] } 执行 aws iam create-policy 命令来创建 IAM Policy，其名为 external-dns $ aws iam create-policy --policy-name external-dns --policy-document file://external-dns-policy.json { \"Policy\": { \"PolicyName\": \"external-dns\", \"PolicyId\": \"ANPAVTR2JPDXDMH3BN6RZ\", \"Arn\": \"arn:aws:iam::385595570414:policy/external-dns\", \"Path\": \"/\", \"DefaultVersionId\": \"v1\", \"AttachmentCount\": 0, \"PermissionsBoundaryUsageCount\": 0, \"IsAttachable\": true, \"CreateDate\": \"2021-12-22T14:24:39+00:00\", \"UpdateDate\": \"2021-12-22T14:24:39+00:00\" } } 记录这里的 IAM Policy 的 ARN，后续创建 ServiceAccount 时需要使用。 通过 eksctl create iamserviceaccount 命令创建绑定 IAM Role 的 ServiceAccount： eksctl create iamserviceaccount --cluster=test-cluster --name=external-dns --namespace=external-dns-admin --attach-policy-arn=arn:aws:iam::385595570414:policy/external-dns --approve --override-existing-serviceaccounts --cluster 与 --region 指定了 EKS 集群 --name 与 --namepsace 指定创建的 ServiceAccount 的 name 与 ns --attach-policy-arn 指定了要绑定的 IAM Policy --override-existing-serviceaccounts 表明允许覆盖已经存在的 ServiceAccount --approve 表明应用修改 Note 也可以通过 aws cli 来创建 ServiceAccount，具体见官方文档：Creating an IAM role and policy for your service account。 ","date":"2021-12-22","objectID":"/posts/cloud_computing/k8s_practice/use-external-dns/:2:2","tags":["k8s","云计算"],"title":"K8s 实践 - 使用 External DNS","uri":"/posts/cloud_computing/k8s_practice/use-external-dns/"},{"categories":["Kubernetes 实践"],"content":"2.3 准备 Route53 Host Zone 创建 ExternalDNS 同步 DNS 的 Route53 Host Zone，当然如果需要使用已有的 Host Zone，那么可以直接跳过。 通过 aws route53 create-hosted-zone 创建一个新的 Host Zone： $ aws route53 create-hosted-zone --name \"external-dns-test.my-org.com.\" --caller-reference \"external-dns-test-$(date +%s)\" { \"Location\": \"https://route53.amazonaws.com/2013-04-01/hostedzone/Z10422583QOCYXWPPFU3S\", \"HostedZone\": { \"Id\": \"/hostedzone/Z10422583QOCYXWPPFU3S\", \"Name\": \"external-dns-test.my-org.com.\", \"CallerReference\": \"external-dns-test-1640156982\", \"Config\": { \"PrivateZone\": false }, \"ResourceRecordSetCount\": 2 }, // ... } 记录这里的 Host Zone 的 ID，在配置 External DNS 时需要使用。 ","date":"2021-12-22","objectID":"/posts/cloud_computing/k8s_practice/use-external-dns/:2:3","tags":["k8s","云计算"],"title":"K8s 实践 - 使用 External DNS","uri":"/posts/cloud_computing/k8s_practice/use-external-dns/"},{"categories":["Kubernetes 实践"],"content":"2.4 部署 ExternalDNS 创建 ExternalDNS 的定义 yaml，其中包含了 ClusterRole、ClusterRoleBinding 与 Deployment。 apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:external-dnsrules:- apiGroups:[\"\"]resources:[\"services\",\"endpoints\",\"pods\"]verbs:[\"get\",\"watch\",\"list\"]- apiGroups:[\"extensions\",\"networking.k8s.io\"]resources:[\"ingresses\"]verbs:[\"get\",\"watch\",\"list\"]- apiGroups:[\"\"]resources:[\"nodes\"]verbs:[\"list\",\"watch\"]---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:external-dns-viewerroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:external-dnssubjects:- kind:ServiceAccountname:external-dnsnamespace:external-dns-admin---apiVersion:apps/v1kind:Deploymentmetadata:name:external-dnsnamespace:external-dns-adminspec:strategy:type:Recreateselector:matchLabels:app:external-dnstemplate:metadata:labels:app:external-dnsspec:serviceAccountName:external-dnscontainers:- name:external-dnsimage:k8s.gcr.io/external-dns/external-dns:v0.7.6args:- --source=service- --source=ingress- --domain-filter=external-dns-test.my-org.com- --provider=aws- --policy=upsert-only- --aws-zone-type=public- --registry=txt- --txt-owner-id=my-hostedzone-identifier 需要注意的是，因为 ExternalDNS 的 Pod 需要使用前面创建的 ServiceAccount，因此需要部署在相同的 ns，ClusterRoleBinding 也是如此。 看下 ExternalDNS 的启动参数： --source 指定了从哪些资源查找需要同步的 Endpoint --domain-filter 限制处理的目标 DNS Zone --provider 指明了 DNS 提供商 --policy 指明与 DNS Provider 的数据同步方式，默认 sync，可选 upsert-only, create-only ","date":"2021-12-22","objectID":"/posts/cloud_computing/k8s_practice/use-external-dns/:2:4","tags":["k8s","云计算"],"title":"K8s 实践 - 使用 External DNS","uri":"/posts/cloud_computing/k8s_practice/use-external-dns/"},{"categories":["Kubernetes 实践"],"content":"2.5 部署 Service 以 LB Service 为例，来验证 ExternalDNS 的工作。 先创建需要使用的 Service，其定义如下： apiVersion:v1kind:Servicemetadata:name:nginxannotations:external-dns.alpha.kubernetes.io/hostname:nginx.external-dns-test.my-org.comspec:type:LoadBalancerports:- port:80name:httptargetPort:80selector:app:nginx 其关键在于，添加了 key 为 external-dns.alpha.kubernetes.io/hostname 的 annotation，其 value 为需要同步到 Route 53 的 DNS 域名。 等待几分钟后，查询 Route 53 的 DNS 记录可以看到其创建了一个新的 DNS 记录： $ aws route53 list-resource-record-sets --output json --hosted-zone-id /hostedzone/Z10422583QOCYXWPPFU3S { \"ResourceRecordSets\": [ // ... { \"Name\": \"nginx.external-dns-test.my-org.com.\", \"Type\": \"A\", \"AliasTarget\": { \"HostedZoneId\": \"Z368ELLRRE2KJ0\", \"DNSName\": \"a8ffeecc9c0c048e5bdff2d90d66f307-118096740.us-west-1.elb.amazonaws.com.\", \"EvaluateTargetHealth\": true } } // ... } 可以看到，“nginx.external-dns-test.my-org.com.” 为一个 DNS 别名，其指向的是 Service 的 LB 域名。 ","date":"2021-12-22","objectID":"/posts/cloud_computing/k8s_practice/use-external-dns/:2:5","tags":["k8s","云计算"],"title":"K8s 实践 - 使用 External DNS","uri":"/posts/cloud_computing/k8s_practice/use-external-dns/"},{"categories":["Kubernetes 实践"],"content":"2.6 验证 我们部署一个 nginx 服务来验证 DNS 记录已经被同步，其定义如下： apiVersion:apps/v1kind:Deploymentmetadata:name:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- image:nginxname:nginxports:- containerPort:80name:http 然后通过访问在 Service annotation 指定的需要同步的 DNS 域名，来测试 DNS 的同步是否成功： $ curl nginx.external-dns-test.my-org.com. ","date":"2021-12-22","objectID":"/posts/cloud_computing/k8s_practice/use-external-dns/:2:6","tags":["k8s","云计算"],"title":"K8s 实践 - 使用 External DNS","uri":"/posts/cloud_computing/k8s_practice/use-external-dns/"},{"categories":["Kubernetes 实践"],"content":"参考 Github：external-dns Doc：Setting up ExternalDNS for Services on AWS Blog：通过 ExternalDNS 集成外部 DNS 服务 ","date":"2021-12-22","objectID":"/posts/cloud_computing/k8s_practice/use-external-dns/:3:0","tags":["k8s","云计算"],"title":"K8s 实践 - 使用 External DNS","uri":"/posts/cloud_computing/k8s_practice/use-external-dns/"},{"categories":["Kubernetes 学习"],"content":"HPA 算法定义","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"HPA 的核心原理就是就是从 Metric Server 采集指标数据，然后根据缩扩容算法进行计算，得到目标的 Pod 副本数量。当目标 Pod 副本数量与当前副本数量不同时，HPA Controller 调用指定的资源对象的 scale 操作，进行缩扩容。 ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:0:0","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"1 缩扩容算法 从最根本上看，缩扩容算法按照以下的公式来计算目标的 Pod 副本数量。 desiredReplicas = ceil[currentReplicas * (currentMetricValue / metricValue)] currentReplicas - 当前 Pod 的副本数量 currentMetricValue - 当前的 Metrics 值 metricValue - 预定义的 Metric 值 currentMetricValue 与 metricValue 可以是按照固定值、平均值或者使用率的方向去计算。 Value - desiredMetricValue 为固定值，currentMetricValue 为采集到的值 AverageValue - targetAverageValue 为固定值，currentMetricValue 为所有 Pod Metric 的平均值 Utilization - targetAverageUtilization 为固定的比例，为固定值，currentMetricValue 将所有 Pod 的 Metric 值求平均后除以 Metric 总和 可以设置基于多个 metric 指标计算，HPA 会对每个 metric 算出各个 desiredReplicas，然后以全部结果的最大值作为最终结果。不过，如果其中一个 metric 无法算出 desiredReplicas，那么就会跳过本次缩扩容。 ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"1.1 容忍度 为 desiredReplicas 与 currentReplicas 比例接近于 1 时，可以设置 tolerance 值来让 HPA Controller 不进行缩扩容。 默认下，tolerance 设置为 0.1（10%），那么当 desiredReplicas 与 currentReplicas 比例在 [-10%, +10%] 区间时，不会进行自动缩扩容。 ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:1:1","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"1.2 异常处理 当计算 desiredReplicas 时，如果部分 Pod 处于异常状态，那么对于这些 Pod 的数据会进行特殊的对待。 Pod 的异常状态包括： Pod 正在被删除（DeletionTimeStamp != nil） Pod 的 metric value 无法采集 Pod 还未达到 Ready 状态（仅仅对于 CPU 指标的缩扩容使用） 对于 desiredMetricValue 的方式，所有异常的 Pod 会被忽略，不计入 currentReplicas 与算出 desiredReplicas。 对于 targetAverageValue / targetAverageUtilization，在计算平均值会使用 “保守” 地策略。 在需要 Scaleup 计算时，认为 Pod 的 MetricValue 比例为 0%，以减小算出的 desiredReplicas 在需要 Scaledown 计算时，认为 Pod 的 MetricValue 比例为 100%，以增大算出 desiredReplicas ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:1:2","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"1.3 冷却时间 HPA 计算目标副本数量时，可能因为指标的抖动导致 Pod 副本数量频繁变化。可以配置 kube-controller-manager 的启动参数 –horizontal-pod-autoscaler-downscale-stabilization 来设置两次缩扩容操作的间隔时间（默认该值为 5min）。 当然需要理解，当该参数过小时，可能导致副本数量的抖动，当该参数过大时，HPA 可能不能很好的改变副本数量。 ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:1:3","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"2 Spec HorizontalPodAutoscaler 基本的定义如下： spec:scaleTargetRef:apiVersion:apps/v1kind:Deploymentname:testminReplicas:1maxReplicas:10metrics:- type:Resourceresource:name:cputarget:type:UtilizationaverageUtilization:50behavior:scaleUp:stabilizationWindowSeconds:0selectPolicy:# policies# scaleDown minReplicas - 定义自动缩扩容的最小副本值 maxReplicas - 定义自动缩扩容的最大副本值 scaleTargetRef - 指定 HPA 操作的资源，例如 Deployment/StatefulSet/ReplicaSet metrics - 用于计算副本值的数据，支持使用 Kubernetes 集群内部数据，或者来自于 metric 数据 behavior - 控制缩容与扩容的行为 ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"2.1 缩扩容的目标 HPA 在经过算法计算，需要实际进行缩扩容时，会调用定义的目标对象的 scale 接口。因此，只要实现了 scale 接口的资源都可以作为 HPA 操作的目标对象。例如 Deployment 和 StatefulSet，以及定义了 scale subresource 的 CRD。 在定义中，通过 spec.scaleTargetRef 来定义操作的目标对象。 spec:scaleTargetRef:apiVersion:apps/v1kind:Deploymentname:test ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"2.2 Metric 指标 HPA 支持读取 Kubernetes 内置的或者第三方的 Metric 指标，来进行副本的计算。 HPA 会通过访问 Kubernetes APIServer 的特定接口读取 Metric 值，但是这些 Metric 的接口都不是 Kubernetes APIServer 内置的，需要用户先部署提供接口的 Custom APIServer（Aggregate APIServer）。 HPA 根据不同类型的 Metric 指标，会请求不同的接口。 Resource 指标 - CPU/Mem 资源相关，会请求 metrics.k8s.io 接口 Kubernetes 提供了实现 metrics-server 可以直接部署使用。 Custom 指标 - 自定义 Kubernetes 集群内部的指标，会请求 custom.metrics.k8s.io 接口 其他指标会提供名为 “Adapter” 作为 Custom APIServer 实现。如果想自己编写，可以参考 示例项目 开始。 External 指标 - 从 Kubernetes 集群外部得到的指标（例如从云厂商的数据），会请求 external.metrics.k8s.io 接口 依旧由名为 “Adapter” 的 Custom APIServer 实现。自行编写与 Custom 指标一样。 上面不同的 Metric 指标体现在 spec.metrics 字段： spec:metrics:- type:Resourceresource:name:cputarget:type:UtilizationaverageUtilization:50 其中 type 字段指明了指标的类型，不同类型的指标会使用不同的字段。 Resource -（Resource 指标）目标对象的 Pod 的 CPU/Mem 指标。 对于 CPU，仅支持 target 设置为 type: Utilization，来设置 CPU 的平均使用率。 对于 Mem，仅支持 target 设置为 type: AverageValue，来设置 Mem 的平均使用值。 type:Resourceresource:name:cputarget:type:UtilizationaverageUtilization:60 ContainerResource -（Resource 指标）目标对象的 Pod 的某个 Container 的 CPU/Mem 指标。 对于 CPU，仅支持 target 设置为 type: Utilization，来设置 CPU 的平均使用率。 对于 Mem，仅支持 target 设置为 type: AverageValue，来设置 Mem 的平均使用值。 type:ContainerResourcecontainerResource:name:cpucontainer:application# 针对 Pod 中的 application 容器target:type:UtilizationaverageUtilization:60 Pods -（Custom 指标）伸缩对象的 Pod 指标，例如 metrics-server 提供 Pod 的每秒收包指标。 仅支持 target 设置为 type: AverageValue。 type:Podspods:metric:name:packets-per-secondtarget:type:AverageValueaverageValue:1k Object -（Custom 指标）一些 Kubernetes 集群的对象的指标，例如 metrics-server 提供了 Ingress 的 QPS 指标。 仅支持 target 设置为 type: AverageValue 和 type:Value。 type:Objectobject:metric:name:requests-per-seconddescribedObject:apiVersion:networking.k8s.iokind:Ingressname:main-routetarget:type:Valuevalue:2k External -（External 指标）非 Kubernetes 原生对象的指标 ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"2.3 Behavior spec.behavior 字段用以控制 HPA Controller 进行缩扩容的行为。 behavior:scaleDown:stabilizationWindowSeconds:300policies:- type:Percentvalue:100periodSeconds:15scaleUp:stabilizationWindowSeconds:0policies:- type:Percentvalue:100periodSeconds:15- type:Podsvalue:4periodSeconds:15selectPolicy:Max 其中，scaleDown 与 scaleUp 具有相同的字段，一个用于控制缩容行为，另一个控制扩容行为。 2.3.1 缩扩策略 policies 字段用于定义一个或多个缩扩策略。当指定多个缩扩策略时，会选择算出缩扩副本数最多的策略。 下面的配置等于默认的缩扩行为： behavior:scaleDown:stabilizationWindowSeconds:300policies:- type:Percentvalue:100periodSeconds:15scaleUp:stabilizationWindowSeconds:0policies:- type:Percentvalue:100periodSeconds:15- type:Podsvalue:4periodSeconds:15selectPolicy:Max type 表示策略的控制维度，periodSeconds 表示计算该策略的时间区间，value 表示最大数值。 目前支持两种方式（type）的控制： Pods - 控制每次缩扩的最大允许数量 Percent - 控制每次缩扩的最大比例 以上述配置为例，第一个策略表示 60s 内最多允许缩容 4 个副本，第二个策略表示 60s 内最多缩容当前副本数的 10%。 2.3.2 缩扩策略选择 selectPolicy 字段用于控制每次选择缩扩策略的行为，支持： Max -（默认）多个策略下，选择副本变化最大的策略。 Min - 多个策略下，选择副本变化最小的策略。 Disabled - 不进行缩或者扩 2.3.3 稳定窗口 当 Metric 持续抖动时，可以使用稳定窗口来限制副本数上下振动。缩扩算法在执行缩扩时，会考虑过去计算过的期望状态。stabilizationWindowSeconds 用于指定考虑过去多久的期望状态。 scaleDown:stabilizationWindowSeconds:300 这里表示，过去 5min 的所有期望状态都会被计算。如果稳定窗口为 0 时，表明当前算法计算出缩容时会立即执行。 ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"3 Status ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"3.1 Condition status.conditions 字段展示了 HPA 是否能够执行缩扩，以及受到了哪些的限制。 Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True ReadyForNewScale the last scale time was sufficiently old as to warrant a new scale ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric http_requests ScalingLimited False DesiredWithinRange the desired replica count is within the acceptable range AbleToScale 表示 HPA 是否能够获取和更新缩扩信息，以及是否存在阻止缩扩的限制条件。 ScalingActive 表示 HPA 是否启用，以及是否能够完成缩扩计算。当为 False 时，通常为获取 Metric 出现稳定。 ScalingLimited 表明缩扩值是否达到了限制值（最大最小限制值）。通常这表明可能需要调整 maxReplicas 或 minReplicas。 ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["Kubernetes 学习"],"content":"参考 Doc: Horizontal Pod Autoscaling ","date":"2021-12-20","objectID":"/posts/cloud_computing/k8s_learning/12-hpa/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 12 - Horizontal Pod Autoscaler","uri":"/posts/cloud_computing/k8s_learning/12-hpa/"},{"categories":["AWS 学习"],"content":"ELB ALB NLB","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"1 LB 的基本模型 不同的 LB 在使用上是基本相同的，都依据以下的模型： Domain - LB 拥有着一个唯一的域名，Client 通过该域名来访问 LB。 LB 的域名是单点的，但是域名对应多个 IP（每个 AZ 一个 IP）。也就是说，LB 所在 Region 的每个 AZ 都有着 LB 的处理程序，客户端的请求会被某个 AZ 的 LB 实例进行处理。 Listener - LB 域名下扩展多个 Listener，用于根据请求的 Port 来进行转发。 通过 Listener，Client 请求一个 LB 时可以根据请求的 Port 由不同的 Target 进行处理。 Rule - 如果是 ALB，还可以给 Listener 添加多个 Rule，进一步根据请求信息（HTTP Path、HTTP Method 等）来进行转发。 Target Group - 每个 Listener 对应于一个 Target Group，包含多个用于处理请求的 Target。 Target - 处理请求的实例，最平常的就是一个 Instance。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:1:0","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"2 Domain LB 分为两种 Scheme： Internet-facing - 面向公网的 LB，具有 Public IP，需要部署在 Publich Subnet Internal - 内网 LB，仅仅具有 Private IP 无论哪种 LB，创建后都会分配一个唯一的 Domain，用作访问 LB 的入口。该 Domain 会被解析为其服务的多个 Subnet 的 Public/Private IP。 LB 的 Domain 格式为： \u003cLB-id\u003e-\u003chash\u003e.elb.\u003cregion\u003e.amazonaws.com ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:2:0","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"3 Listener Listener 将 Domain 访问根据 Protocol + Port 分为多个路由项。也就是说，访问 Domain 的请求根据 Protocol + Port 由不同的 Listener 来处理。 每个 Listener 需要指定： Protocol - 匹配的协议 对于 ALB，Protocol 支持 HTTP 与 HTTPs。对于 NLB，Protocol 支持 TCP、TCP_UDP、TLS 与 UDP。 Port - 匹配的 Port Default Action - 默认路由的 Target Group ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:3:0","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"3.1 Rule 对于 ALB，在 Listener 范围下还可以添加 Rule，进一步的根据信息添加路由项。例如，你可以创建一个 HTTP 80 端口的 Listener，然后创建不同的 HTTP Path 的路由项。 每条 Rule 支持如下的方式进行组合： Host Header HTTP Header - 请求的 Header HTTP Request Method - 请求的 Method Path - 请求的 Path Query string Source IP - 源 IP 地址 Rule 按照编号从小到大进行匹配，当匹配到了之后就执行路由，不再匹配。Listener 创建时指定的规则为 Default Action，是最后一个 Rule。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:3:1","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"4 Target Group 每个 Listener(Rule) 都需要绑定对应的 Target Group。Target Group 是一个抽象的概念，用于表明有哪些后端服务。 Target Group 有四种类型： Instance - EC2 Instance 选择与 LB 同 Subnet 的 EC2 Instance IP 地址 - 指定多个后端服务的 IPv4 地址 Lambda 函数 - 指定触发的 Lambda 函数（仅 ALB 支持） ALB - 后端由 ALB 处理 指定 Target Group 时，也需要指定请求后端服务的 Port。 Note Listener 的 Port 用于判断是否处理请求，Target Group 的 Port 是访问 Target 指定的端口。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:4:0","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"4.1 Health Check 创建 Target Group 时，需要指定 Health Check 机制。LB 会周期性对所有 Target 进行健康检查。 Health Check 的状态变化是平滑的： Healthy -\u003e Unhealthy: 连续 Health Check 失败次数达到阈值 “Healthy threshold” Unhealthy -\u003e Healthy: 连续 Health Check 成功次数达到阈值 “Healthy threshold” 当某个 Target 变为 Unhealthy 时，LB 就不会将流量路由到该 Target，直到重新变为 Healthy。 Health Check 支持以下协议： TCP - 对于 TCP 端口发起连接请求，连接成功就表明健康检查成功 HTTP - 发起一个 HTTP Path 的请求，检查其回复的 HTTP Code HTTP - 发起一个 HTTPs Path 的请求，检查其回复的 HTTP Code Note 对于 UDP NLB，也仅仅只能通过 TCP 来进行健康检查。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:4:1","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"5 Application Load Balancer ALB 是基于七层的代理，创建时需要指定其所在的 VPC，以及指定至少两个 AZ 的 Subnet。LB 会在每个 Subnet 创建一个 ENI，用于作为 LB 域名解析后的 IP 之一。 Note 使用 nslookup \u003clb-addr\u003e 解析 LB 域名时，得到的就是各个 Subnet 的 ENI 的 IP 地址。 因为 LB 使用 ENI 来实现，因此也可以为 LB 设置 Security Group，来控制出入 LB 的流量。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:5:0","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"5.1 LB 的状态 LB 可能处于以下状态之一： provisioning - 创建 LB 中 active - LB 已经就绪，并正常路由流量中 active_impaired - LB 正在路由流量，但是没有扩展所需的资源 failed - LB 无法创建 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:5:1","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"5.2 Access Log ELB 可以提供 Access Log，用于查看发送到 LB 的请求的详细信息。 Access Log 功能默认是关闭的，启用后 AWS 会将访问日志压缩，并存储到用户指定的 S3 bucket 路径。使用访问日志不需要额外的付费，只需要为 S3 的存储付费。 ELB 默认每 5min 保存一次 LB 的 Access Log，存储的文件名格式如下： \u003cbucket path\u003e/AWSLogs/\u003caws-account-id\u003e/elasticloadbalancing/\u003cregion\u003e/\u003cyear\u003e/\u003cmonth\u003e/\u003cday\u003e/\u003caccount_id\u003e_elasticloadbalancing_\u003cregion\u003e_\u003cload-balancer-id\u003e_\u003cend-time\u003e_\u003cip-address\u003e_\u003crandom-string\u003e.log.gz 每个访问请求的信息包括 请求时间、客户端 IP、延迟、服务器回复等等，完整的列表见：Access Log Entry。 看一个例子，每个信息之间通过空格隔开： http 2018-07-02T22:23:00.186641Z app/my-loadbalancer/50dc6c495c0c9188 192.168.131.39:2817 10.0.0.1:80 0.000 0.001 0.000 200 200 34 366 \"GET http://www.example.com:80/ HTTP/1.1\" \"curl/7.46.0\" - - arn:aws:elasticloadbalancing:us-east-2:123456789012:targetgroup/my-targets/73e2d6bc24d8a067 \"Root=1-58337262-36d228ad5d99923122bbe354\" \"-\" \"-\" 0 2018-07-02T22:22:48.364000Z \"forward\" \"-\" \"-\" 10.0.0.1:80 200 \"-\" \"-\" ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:5:2","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"5.3 Deletion Protection 为了防止 LB 意外被删除，可以开启 Deletion Protection 功能。开启后，需要先关闭 Deletion Protection 功能，然后才能删除 LB。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:5:3","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"5.4 Connection idle timeout 对于 Client 的访问，LB 会维护两个连接： Client 与 LB 之间的连接 LB 与 Target 之间的连接 对于这两个连接，一旦连接空闲（未发送或接收任何数据）时间超过 idle timeout 后，LB 就会关闭连接。默认的 idle timeout 为 60s。 对于 LB 与 Target 之间的连接，为了能够复用后端连接，推荐开启 HTTP keep alive。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:5:4","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"5.5 Desync mitigation mode 在 LB 这种模式下，HTTP 的访问是由 LB 异步发送给 Target 进行处理的。Desync mitigation mode 就是用于保护 Target 不受到 HTTP 异步的影响。 LB 会使用 http_desync_guardian 库，将每个请求进行威胁分级，根据 desync mitigation mode 来选择是否转发某个请求。 Classifications Monitor mode Defensive mode（默认） Strictest mode Compliant Allowed Allowed Allowed Acceptable Allowed Allowed Blocked Ambiguous Allowed Allowed Blocked Severe Allowed Blocked Blocked ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:5:5","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"5.6 HEAD 处理 ALB 支持转发请求时添加 X-Forwarded 相关的 HTTP HEAD，用于提供给 Target 一些无法看到的信息。 X-Forwarded-For - 包含 Client 的 IP 地址 因为 LB 负责与 Target 进行通信，因此 Target 无法从协议上看到客户端的源 IP 地址。ALB 会在发送给 Target 的请求的 HTTP HEAD 加上 X-Forwarded-For 以提供给 Target 客户端的 IP 地址。 进一步的，可以开启 “客户端端口保留功能”，X-Forwarded-For 还会加上 Client 的源端口。 X-Forwarded-Proto - 包含 Client 与 ALB 通信的协议 X-Forwarded-Port - 包含 Client 访问 ALB 的目标端口 当请求包含无效格式（符合正则表达式 [-A-Za-z0-9]+） 的 HTTP HEAD 时，默认 ALB 转发请求时会将其删除，用户可以选择保留无效的 HTTP HEAD。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:5:6","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"5.7 Sticky Sessions 默认下，ALB 会根据选择的负载均衡算法将请求路由到 Target 上。用户也可以选择使用 Sticky Sessions 功能，使得将 Session 绑定路由到某个 Target 上。也就是说，在 Session 期间，用户的所有请求都会发送到同一个 Target 来处理。 Note 要使用 Sticky Sessions，客户端必须开启 Cookie。 ALB 支持两种类型： 基于持续时间的 Cookie 当 ALB 第一次收到 Client 请求时，会根据选定算法路由给 Target，并根据相关信息生成名为 AWSALB 的加密的 Cookie。Cookie 有着 7 天的过期时间。 后续 ALB 收到 Client 请求时，如果 Client 包含 AWSALB Cookie，那么会将其解密，根据信息将其路由到同一个 Target。如果该 Target 已经不存在或者 unhealthy，那么会选择一个新的 Target，同时更新 Cookie 的值。 基于应用程序的 Cookie 基于应用程序的 Cookie 是在 基于持续时间 的基础上，让应用程序也知晓 Cookie 的存在。 当 ALB 第一次收到 Client 请求时，会根据选定算法路由给 Target，而 Target 返回回复需要包含一个 Application Cookie。ALB 收到 Application Cookie 后，会根据信息与该 Cookie 生成 AWSALB Cookie。同样，该 Cookie 有着 7 天的过期时间。Client 会收到 Application Cookie 与 AWSALB Cookie。 后续 ALB 收到 Client 请求时，LB 会解密 AWSALB Cookie，根据信息将其路由到同一个 Target，并且会传递 Application Cookie。Target 可以根据 Application Cookie 从而知晓这是一个存在 Session。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:5:7","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"6 Network Load Balancer NLB 是基于四层的负载均衡，支持 TCP 与 UDP 协议。 对于 TCP 流量，LB 基于协议、源 IP 地址、源端口、目标 IP 地址、目标端口和 TCP 序列号，使用哈希算法选择 Target。每个 TCP 连接在有效期内都会路由到同一个 Target。 对于 UDP 流量，LB 基于协议、源 IP 地址、源端口、目标 IP 地址、目标端口，使用哈希算法选择 Target。如果 UDP 数据包有着相同的源地址和目标地址，那么就会路由到同一个 Target。 创建 NLB 时需要指定其所在的 VPC，以及至少两个 AZ 的 Subnet。LB 会在每个 Subnet 创建一个 ENI，用于作为 LB 域名解析后的 IP 之一。 Note 使用 nslookup \u003clb-addr\u003e 解析 LB 域名时，得到的就是各个 Subnet 的 ENI 的 IP 地址。 因为 LB 使用 ENI 来实现，因此也可以为 LB 设置 Security Group，来控制出入 LB 的流量。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:6:0","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"6.1 LB 的状态 LB 可能处于以下状态之一： provisioning - 创建 LB 中 active - LB 已经就绪，并正常路由流量中 failed - LB 无法创建 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:6:1","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"6.2 Access Log ELB 可以提供 TLS 请求的 Access Log，用于查看发送到 LB 的请求的详细信息。 Note 仅仅当 LB 具有 TLS Listener，且它们仅仅包含 TLS 请求的信息时，才会创建访问日志。 Access Log 功能默认是关闭的，启用后 AWS 会将访问日志压缩，并存储到用户指定的 S3 bucket 路径。使用访问日志不需要额外的付费，只需要为 S3 的存储付费。 ELB 默认每 5min 保存一次 LB 的 Access Log，存储的文件名格式如下： \u003cbucket path\u003e/AWSLogs/\u003caws-account-id\u003e/elasticloadbalancing/\u003cregion\u003e/\u003cyear\u003e/\u003cmonth\u003e/\u003cday\u003e/\u003caccount_id\u003e_elasticloadbalancing_\u003cregion\u003e_\u003cload-balancer-id\u003e_\u003cend-time\u003e_\u003cip-address\u003e_\u003crandom-string\u003e.log.gz 每个访问请求的信息包括 请求时间、客户端 IP、延迟、服务器回复等等，完整的列表见：Access Log Entry。 看一个例子，每个信息之间通过空格隔开： tls 2.0 2018-12-20T02:59:40 net/my-network-loadbalancer/c6e77e28c25b2234 g3d4b5e8bb8464cd 72.21.218.154:51341 172.100.100.185:443 5 2 98 246 - arn:aws:acm:us-east-2:671290407336:certificate/2a108f19-aded-46b0-8493-c63eb1ef4a99 - ECDHE-RSA-AES128-SHA tlsv12 - my-network-loadbalancer-c6e77e28c25b2234.elb.us-east-2.amazonaws.com - - - ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:6:2","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"6.3 Deletion Protection 为了防止 LB 意外被删除，可以开启 Deletion Protection 功能。开启后，需要先关闭 Deletion Protection 功能，然后才能删除 LB。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:6:3","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"6.4 跨 AZ 负载均衡 默认情况下，每个 Subnet 的 LB 节点仅仅为当前 AZ 的 Target 路由流量。开启 跨 AZ 负载均衡 功能后，每个 LB 节点会为所有启用的 AZ 的 Target 之间路由流量。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:6:4","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"7 Gateway Load Balancer Gateway Load Balancer 作为一个透明网络的网关设备（网络中的所有流量的单一入口与出口点），所有的流量会经过注册的第三方虚拟设备的处理（例如安全设备对流量的检查）。GWLB 基于第三层（网络层）工作，监听所有端口的 IP 数据包，将流量路由到注册的 Target。 对于 TCP/UDP 流量，GWLB 使用 5 元组来路由流量。对于非 TCP/UDP 流量，使用 3 元组来路由流量。 GWLB 使用 GWLB Endpoint 来安全地跨 VPC 边界交换流量。进出 GWLB Endpoint 的流量使用 Route Table 进行配置。 举一个实际的例子： 从 Internet 到 Application 的流量（蓝色箭头）： 流量进入 Internet Gateway 进入 Public VPC； 根据 Route Table，流量发送给了 GWLB Endpoint； GWLB Endpoint 将流量传输到了 GWLB； 经过 Security Application 检查后，流量会发送回 GWLB Endpoint； 流量被发送到 Application； 从 Application 到 Internet 的流量（橙色箭头）： 根据 Route Table 的默认路由， Application 发出的流量发送到了 GWLB Endpoint； 流量将发送到 GWLB，并经过了 Security Application 的检查； 检查后，流量发送回 GWLB Endpoint； 根据 Route Table，流量发送到 Internet Gateway； 流量发送到 Internet； 使用 GWLB 时，通过对 Route Table 的配置，让 Application 所在的网段的流量都经过另一个 Subnet 的 GWLB Endpoint。例如： Destination Target 10.0.0.0/16 本地 10.0.1.0/24 vpc-endpoint-id 0.0.0.0/0 internet-gateway-id 对于 Application 的路由表，将默认路由指向另一个 Subnet 的 GWLB Endpoint，从而让所有流量经过 GWLB。例如： Destination Target 10.0.0.0/16 本地 0.0.0.0/0 vpc-endpoint-id ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:7:0","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"7.1 LB 的状态 LB 可能处于以下状态之一： provisioning - 创建 LB 中 active - LB 已经就绪，并正常路由流量中 failed - LB 无法创建 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:7:1","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"7.2 Deletion Protection 为了防止 LB 意外被删除，可以开启 Deletion Protection 功能。开启后，需要先关闭 Deletion Protection 功能，然后才能删除 LB。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:7:2","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"7.3 跨 AZ 负载均衡 默认情况下，每个 Subnet 的 LB 节点仅仅为当前 AZ 的 Target 路由流量。开启 跨 AZ 负载均衡 功能后，每个 LB 节点会为所有启用的 AZ 的 Target 之间路由流量。 ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:7:3","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"参考 Application Load Balancer Network Load Balancer Gateway Load Balancer ","date":"2021-12-04","objectID":"/posts/cloud_computing/aws_learning/7-elb/:8:0","tags":["aws"],"title":"AWS 学习 - 7 - Elastic Load Balancing","uri":"/posts/cloud_computing/aws_learning/7-elb/"},{"categories":["AWS 学习"],"content":"AWS IAM 相关概念","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"使用 AWS 时，不同的员工或者应用程序有着不同的 AWS 资源需求，通过 IAM 对其访问权限进行控制。 IAM 本质上是为了完成两个功能： Authentication - 身份认证，操作资源前确认请求者是 “谁”； Authorization - 授权，操作者是否有权限操作资源； 这个 “谁” 被称为 Entity，后面看到的 User、User Group、Role 都属于 Entity。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:0:0","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"1 与 AWS 交互 有四种访问 AWS 服务的方式，所有的方式本质上都是通过请求 AWS API 来访问，不同方式发送时带有的凭证方式不同。 Console - Username + Password 登陆网页 CLI - Access Key + Secret Key（后续简称为 AK/SK） SDK - AK/SK AWS Service - Assume Role 所有的 API 请求会发送到各个 Region 的 AWS API 终端节点，所有请求都会经过 IAM 的身份认证与授权检查，以及 CloudTrail 的记录。 所有访问方式发送 API 时都会带有一个凭证，其内容包括： 请求主体； 签名 - 由 SK 作为私钥，对 请求主体 + 元信息 + 时间戳 等信息进行加密，得到签名； 该凭证实现的作用： 身份验证 - SK 签名实现 - AWS 服务端会查找对应的 AK 进行解密，从而验证是否是对应的 SK 加密的； 防止篡改 - 摘要信息实现 - AK 解密后得到摘要信息，通过对比摘要信息来检查请求主体是否被篡改； 防止重放攻击 - 时间戳实现 - 请求中会带有 时间戳 信息，通过时间戳信息来判断请求有效期（默认 5min），防止重放攻击； AK/SK 的作用 可以看到，AK 作用类似于公钥，SK 作用类似于私钥。 当然，所有的凭证生成对于用户都是透明的，用户只需要配置好 AK/SK 即可访问 AWS 服务。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:1:0","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"2 Root User / IAM User / User Group ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:2:0","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"2.1 Root User 创建 AWS Account 时，会自动创建一个 Root User，其权限不受控制。 对 Root User，因为其权限太大，所以日常使用不要使用，同时推荐对其开启 MFA，登陆时要求两步验证。 Root User 权限收到控制的情况 有个例外，当使用 AWS Organization 限制时，某个 Account 的 Root User 会受到 Service Control Policy 限制。 最佳实现是：最开始创建 Account 时，先用 Root User 创建 Admin IAM User（作为顶级管理员），然后使用 Admin IAM User 去创建各个 IAM User。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:2:1","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"2.2 IAM User IAM User 访问 AWS 服务收到 IAM 的限制，通过 IAM Policy 来限制其访问权限，以满足最小权限原则（只有用户至少需要的权限）。 每个 IAM User 有着独立的 Username/Password 与 AK/SK。每个 IAM User 最多有两套 AK/SK，以方便轮换密钥：先停用当前使用的密钥，切换使用新的密钥，后删除旧的密钥。 后面将 IAM User 直接简称为了 User。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:2:2","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"2.3 User Group 为了更加轻松的管理 User 与他们拥有的权限，可以将 User 添加到 User Group 中。 当将 IAM Policy 附加到某个 Group 中，该 Group 内的所有 User 都具有了相应的权限。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:2:3","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"2.4 Service Control Policy 使用 AWS Organization 管理多个 Account 时，Organization 可以通过 Service Control Policy 控制一个或一组 Account 的访问权限，Account 下所有 Entity 操作都会受到该 Policy（包括 Root User）控制 Service Control Policy 就是 Policy，会作用于 IAM 进行 Policy 评估时（后续会看到）。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:2:4","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"3 Policy Policy 定义了权限这个概念，通过将 Policy 附加到 User/Role，就可以限制其对 AWS 资源的访问权限。 Note Policy 仅仅是静态的权限定义，能够限制 “谁” 是靠附加 Policy 实现的（除了 Resource-based Policy）。 其格式类似于： { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"ec2:*\", \"Effect\": \"Allow\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"iam:CreateServiceLinkedRole\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"iam:AWSServiceName\": [ \"autoscaling.amazonaws.com\", \"ec2scheduled.amazonaws.com\", \"elasticloadbalancing.amazonaws.com\", \"spot.amazonaws.com\", \"spotfleet.amazonaws.com\", \"transitgateway.amazonaws.com\" ] } } } ] } Statement - 各个语句数组，多个规则之间为 OR 逻辑； Effect - 描述该语句是 Allow/Deny； Action/NotAction - 语句针对哪些 API 进行控制，支持通配符（例如 ec2:start*）； Resource/NotResource - 语句针对哪些资源进行控制，使用 ARN 地址； Condition - 语句生效的额外匹配条件，多个条件之前为 AND 逻辑； Principal - （仅用于 Resource-based Policy）语句作用于 User/Service； Note 注意 Resource 与 Principal，Resource 指操作的目标，Principal 指谁进行操作。理论来说，Principal 可以由 Condition 来实现。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:3:0","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"3.1 Policy 的类型 根据 Policy 的管理方式，分为： AWS 托管 - 由 AWS 进行维护与更新，包含大多数常用的权限语句； 客户托管 - 由客户自定义与维护，支持版本控制； 内联 - （很少使用）针对于 User/Role 直接绑定的 Policy，不可复用； 根据 Policy 应用主体，分为： Identity-based Policy 基于身份的策略 - 定义 Policy，并附加到 User/Role 来实现对其权限的控制； Resource-based Policy 基于资源的策略 - 与特定资源绑定的 Policy，配置后限制 “谁” 能访问该资源; 例如 S3 Bucket Policy 配置后限制哪些 Entity 可以访问这个存储桶，Lambda Policy 限制哪些 Entity 可以调用这个 Lambda。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:3:1","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"4 Role User 有着长期使用的 Username/Password 与 AK/SK，并不适合临时提供给他人使用。而 Role 就是用于解决临时的授权操作，主要用于： AWS 服务 Assume Role 后，能够拥有访问 AWS 资源的权限； User（当前账户 User 或者其他账户 User）Assume Role 后，能够拥有访问 AWS 资源的权限； 使用 Role 的基本过程为： 创建 Role； 将 Policy 附加到 Role 上； User/Service 进行 Assume Role 操作； Assume Role 后，会在一定时间后过期，也支持手动撤销（Revoke）所有已经分配（不影响后续使用）。 创建一个 Role 并分配了 Policy 后，可以将 Role 授予某个 User 或者 Resource。这样 User/Resource 会丢弃已有的权限，仅拥有 Role 定义的资源访问权限了。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:4:0","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"4.1 Trust Relationship 可以看到，User/Service 需要执行 Assume Role 操作来作为一个 Role，而一个 Role 的 Trust Relationship 就是控制 “谁” 能够 Assume 这个 Role。 Trust Relationship 本质就是一个 Policy，其控制的是 sts:AssumeRole/sts:AssumeRoleWithWebIdentity API，从而实现了对 Assume 的控制。 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] } ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:4:1","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"4.2 Session Policy 执行 Assume Role 行为时，可以传递一个 Session Policy，作为本次 Assume Role 期间额外添加的 Policy。通过 Session Policy 实现一次性的约束。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:4:2","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"4.3 Assume Role Assume Role 本质上是调用 sts:AssumeRole/sts:AssumeRoleWithWebIdentity 相关 API，调用时会检查 Trust Relationship（实际上就是 IAM 检查 Policy）。 Assume 成功后，User/Service 会得到一个 AK/SK + Token 等信息，同时带有着超时时间。这些信息作为一个临时凭证，提供访问 AWS 资源的权限。而该临时凭证由 AWS STS 来颁发。 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:4:3","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"5 Permission Boundary 默认下，当 Admin User 修改 Entity 权限（Policy）时，是可能提供过大的权限。这样操作，仅仅拥有管理 IAM 的 User 通过创建权限很大的 User 可以做到任何操作。 Permission Boundary 本质上就是为某个 Entity 指定一个全局的 Policy。当创建 User/Role 时，可以选择是否要带上 Permission Boundary（可选项）。 强制要求使用 Permission Boundary：即使 Permission Boundary 是可选项，我们可以通过对拥有创建 User/Role 权限的 User 的 Policy 进行条件约束，要求其调用 iam:CreateUser 等 API 时，必须要求带上 Permission Boundary。 { \"Effect\": \"Allow\", \"Action\": [ // -\u003e 针对创建 User 等相关的 API \"iam:CreateUser\" // ... ] \"Condition\": { \"StringEqua1s\": { \"iam:permissionsBoundary\": \"arn:aws:iam::ACCOUNT_ID:policy/PolicyX\"，// -\u003e 要求 Permission Boundary 必须指定了某个 Policy } } } ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:5:0","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"6 Policy 评估 总结一下前面提到的所有 IAM 会评估的 Policy： Identity-based Policy Resource-based Policy Session Policy Permission Boundary Service Control Policy 每个 IAM 检查 API 请求时，所有的设置了的 Policy 都会被评估，整体可以见下图： 看着流程很复杂，但是可见总结一下其判断逻辑： 显式拒绝 - 任何存在的 Policy，只要判断出 Deny，但是结果就是 Deny； 基于资源策略显式允许 - 不存在显示拒绝情况下，如果 Resource-based Policy 是 Allow，那么结果是允许（不参考其他的 Policy）； 其他策略交集取允许 - 其他策略要所有策略都需要显式的 Allow； 隐式拒绝 - 所有策略没有显式 Allow，那么结果为 Deny； Note 所有策略评估都是针对于使用了策略情况下，如果没有应用策略那么不会对结果有影响。 以伪代码方式看： if anyDeny { return Deny // 显式拒绝 } if ResourceBasedAllow { return Allow // 基于资源策略显式允许 } if AllAllow { return allow // 交集取允许 } return Deny // 隐式拒绝 ","date":"2021-12-01","objectID":"/posts/cloud_computing/aws_learning/6-iam/:6:0","tags":["aws"],"title":"AWS 学习 - 6 - IAM","uri":"/posts/cloud_computing/aws_learning/6-iam/"},{"categories":["AWS 学习"],"content":"1 VPC Peering VPC Peering 用于连接两个不同地址范围的 VPC，VPC 可以处于不同的 Region，甚至不同的 AWS 账户。 当 VPC Peering 连接两个 VPC 后，VPC 内部的资源（Instance、RDS 等）可以直接访问到对端 VPC 的资源。 但是，VPC Peering 是不具有传递性的。以上图来说，VPC1 是无法通过 VPC2 与 VPC3 通信的。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:1:0","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"1.1 使用 VPC Peering 使用 VPC Peering 连接 VPC 需要的工作： 由源端 VPC 发起 VPC Peering，连接到对端 VPC。 发起的称为 Requester VPC。 对端 VPC 接受 VPC Peering。 对端的称为 Accepter VPC。 配置 Route Table，将对方 VPC 的数据包发往 VPC Peering。 Destination Target Status Propagated 172.31.0.0/16 pcx-1353251 Active No 配置链路上的 Network ACL 与 Instance 的 Security Group，不影响流量。 VPC 地址范围不能重叠 因为要通过 IP 地址寻址表明了，VPC 之间的地址范围不能重叠。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:1:1","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"1.2 VPC Peering 的状态流转 从发起请求开始，VPC Peering 会经过各个阶段： ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:1:2","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"2 Transit Gateway ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:2:0","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"3 Endpoint 与 Private Link 前面在 Route Table 看到，Route Table 控制将 Subnet 内的流量转发到 AWS 资源。但是 AWS 有着许多的服务，服务类型也不断出现着，特定的 Rule.Target 无法满足增长。 因此，你可以看到 Rule.Target 有一类 vpc-endpoint-id，这就是表明路由到一个 Endpoint。 Endpoint 就是各式各样的 Service 的连接点的抽象。通过 Endpoint，Instance 就可以直接使用 Private IP 访问各种 Service 了。 Service Network Interface 可以将其理解为 Endpoint 就是 Service 的 “Network Interface”，有了 Endpoint 就可以将 Subnet 的流量发送给 Service。 目前，Endpoint 分为三类： Gateway Endpoints Interface Endpoints Gateway Load Balancer Endpoints 其中，第一种流量会通过公网，而后两种不会通过公网，其实现技术称之为 Private Link。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:3:0","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"3.1 Gateway Endpoint 使用 Gateway Endpoint 只能连接特定的 AWS 服务：S3 与 DynamoDB。或者说，当你创建 Endpoint 是连接的是 S3 或者 DynamoDB 服务，可以选择使用 Gateway Endpoint。 使用 Gateway Endpoint 的步骤： 创建 Endpoint，创建时选择目标服务为 Gateway 类型的 DynamoDB 或 S3。 配置 Route Table，是流量能够指向对应 Endpoint 的 vpce-id。 Destination Target Status Propagated pl-6ea54007 vpc-a2984bc5 Active No Instance 通过 Service 域名来访问 Endpoint。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:3:1","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"3.2 Interface Endpoint 大多数 AWS Service 可以通过 Interface Endpoint 进行连接。创建 Interface Endpoint 相当于创建一个 Network Interface，有着 Private IP 与 Private DNS。 使用 Interface Endpoint 的步骤： 创建 Endpoint，创建时选择需要的目标 AWS 服务。 Instance 通过 Private IP 或者 Private DNS 来访问 Service。 支持 S3 与 DynamoDB Interface Endpoint 是支持连接 S3 与 DynamoDB Service 的。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:3:2","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"3.3 Gateway Load Balancer Endpoint Gateway Load Balancer Endpoint 能够将流量路由到 Gateway Load Balancer，然后由 Load Balancer 负载均衡到下游的 Service。 创建 Endpoint 的称为 Service Consumer，下游的 Service 称为 Service Provider。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:3:3","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"4 Endpoint Service 你也可以将自己的程序作为一个 Endpoint Service，使得别人可以通过 Interface Endpoint 来访问你的服务。 Endpoint Service 的入口是一个 LB：Network Load Balancer 或 Gateway Load Balancer。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:4:0","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"参考 AWS PrivateLink and VPC endpoints ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/:5:0","tags":["aws"],"title":"AWS 学习 - 5 - VPC 与 Service 通信","uri":"/posts/cloud_computing/aws_learning/5-vpc-and-aws/"},{"categories":["AWS 学习"],"content":"Internet Gateway、NAT Device","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/4-vpc-and-internet/","tags":["aws"],"title":"AWS 学习 - 4 - VPC 与 Internet 通信","uri":"/posts/cloud_computing/aws_learning/4-vpc-and-internet/"},{"categories":["AWS 学习"],"content":"1 Internet Gateway Internet Gateway 用于将 VPC 的流量转发给外网，并将外网的主动访问转发给指定的 Instance。 使用 Internet Gateway 的必要操作： 创建 Internet Gateway 并关联到 VPC 上。 Instance 拥有公网 IP 地址：Public IP、Elastic IP 或者 IPv6。 Route Table 能够将 Instance 的流量转发到 Internet Gateway。 Security Group 与 Network ACL 不会拦截 Instance 的流量。 Internet Gateway 内部会维护一份 Public IP 与 Private IP 的映射关系，其原理可以简单理解为： 对于 Outbound 流量，Internet Gateway 会将数据包源地址从 Private IP 变为对应的 Public IP/Elastic IP。 对于 Inbound 流量，Internet Gateway 会根据数据包源地址，匹配 Public IP/Elastic IP，然后将目的地址变为 Private IP 转发给 Instance。 Instance 只能看到 Private IP 要明确，所谓的 Public IP 与 Elastic IP 都是针对于 Internet Gateway 设置的，因为 Instance 根本就无法知晓自己有着公网 IP 地址。 感觉就好像在路由器上设置了内网 IP 到外网 IP 的一对一映射。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/4-vpc-and-internet/:1:0","tags":["aws"],"title":"AWS 学习 - 4 - VPC 与 Internet 通信","uri":"/posts/cloud_computing/aws_learning/4-vpc-and-internet/"},{"categories":["AWS 学习"],"content":"2 Egress-only Internet Gateway 当 Instance 有着 IPv6 地址时，能够通过 Internet Gateway 访问外网。同时，IPv6 也是暴露在公网的，外网可以主动通过 IPv6 访问 Instance。 使用 Egress-only Internet Gateway 可以阻止外网主动访问 IPv6。这表明你的 Instance 仅仅只能作为一个 Client，而不能作为 Server。 Egress-only Internet Gateway 当然是有状态的，能将 response 转发给对应的 Instance。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/4-vpc-and-internet/:2:0","tags":["aws"],"title":"AWS 学习 - 4 - VPC 与 Internet 通信","uri":"/posts/cloud_computing/aws_learning/4-vpc-and-internet/"},{"categories":["AWS 学习"],"content":"3 NAT Device NAT Device 允许 Private Subnet 中的 Instance 访问 Internet，NAT Device 会将数据包的源地址 IP 替换为 NAT Device 的 Elastic IP。 NAT Device 是有状态的，当向实例发送回复时，会自动进行目标地址转换，并转发给指定的 Instance。 ","date":"2021-11-29","objectID":"/posts/cloud_computing/aws_learning/4-vpc-and-internet/:3:0","tags":["aws"],"title":"AWS 学习 - 4 - VPC 与 Internet 通信","uri":"/posts/cloud_computing/aws_learning/4-vpc-and-internet/"},{"categories":["Kubernetes 学习"],"content":"CSI 概念与实现","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"1 设计原理 在 源码阅读 - Volume 实现 中看到，所有的存储插件实现的都是操作 Volume 的方法： Provision Volume Attach Volume Mount Device Setup Volume CSI 整体体系的架构如下图 Sidecar 容器 是一组 Kubernetes 社区维护的标准容器，以减少实现 CSI 的重复代码。 CSI Driver 就是我们需要编写的 CSI 插件了，一个 CSI 插件只有一个二进制文件，以 gRPC 方式对外提供三个服务： CSI Identity CSI Controller CSI Node ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"1.1 Sidecar Containers Sidecar 容器会包含 Watch 资源的工作，并执行 CSI Driver 的回调函数，然后更新相关的资源。 通常，这些容器用于与 CSI Driver 的容器捆绑，作为一个 Pod 部署在一起。 目前开发的 CSI Sidecar 容器包括： node-driver-registrar 容器调用 CSI Node 的 NodeGetInfo() 接口，将 CSI Node 注册到 Kubelet。 external-provisioner 容器会 Watch PersistentVolumeClaim 资源，并调用 CSI Controller 的 CreateVolume() 接口来创建一个 Volume。 external-attacher 容器会 Watch VolumeAttachment 资源，并调用 CSI Controller 的 ControllerPublishVolume() 与 ControllerUnpublishVolume() 接口，将 Volume Attach 到指定的 Node。 Note external-attacher 仅仅用于 Attach 操作，而 Volume 的 MountDevice 与 Setup 操作，都是由 Kubelet 直接调用 CSI Node 插件的接口实现的。 external-snapshotter snapshot controller 容器会 Watch VolumeSnapshot 与 VolumeSnapshotContent 资源，snapshotter 容器会 Watch VolumeSnapshotConent 资源，并负责调用 CSI Controller 插件的 CreateSnapshot()、DeleteSnapshot() 与 ListSnapshot() 接口。 external-resizer 容器 Watch PersistentVolumeClaim 资源的修改事件，当用户请求扩容 PVC 时，调用 CSI Controller 的 ControllerExpandVolume() 接口扩容 Volume。 livenessprobe 容器会监控 CSI 插件的监控状态，并通过 Liveness Probe 提供给 Kubernetes。 Note 查看官方文档 Kubernetes CSI Sidecar Containers 来了解完整的 Sidecar 容器。 ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:1:1","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"1.2 CSI Driver CSI Driver 具体实现了对 Volume 的所有操作，本质上需要实现一个 gRPC Server，由 Kubernetes 调用具体的接口。 Note 具体的协议定义见 csi.proto 1.2.1 CSI Identity CSI Identity 服务用于提供插件的一些信息。 service Identity { // 返回插件的 name 与 version rpc GetPluginInfo(GetPluginInfoRequest) returns (GetPluginInfoResponse) {} // 得到插件拥有的功能 rpc GetPluginCapabilities(GetPluginCapabilitiesRequest) returns (GetPluginCapabilitiesResponse) {} // 查询插件是否正在运行中 rpc Probe (ProbeRequest) returns (ProbeResponse) {}} 1.2.2 CSI Controller CSI Controller 服务定义了对 Volume 的管理接口，包括：Provision/Delete、Attach/Detach、Snapshot 等操作。这些操作都是属于 Controller 的操作，在 Master 节点上执行。 service Controller { // 创建一个 Volume rpc CreateVolume (CreateVolumeRequest) returns (CreateVolumeResponse) {} // 删除一个 Volume rpc DeleteVolume (DeleteVolumeRequest) returns (DeleteVolumeResponse) {} // Attach Volume 到指定的 Node rpc ControllerPublishVolume (ControllerPublishVolumeRequest) returns (ControllerPublishVolumeResponse) {} // Detach Volume rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest) returns (ControllerUnpublishVolumeResponse) {} // 检查一个 Volume 的能力 rpc ValidateVolumeCapabilities (ValidateVolumeCapabilitiesRequest) returns (ValidateVolumeCapabilitiesResponse) {} // 查询已经创建的所有的 Volume rpc ListVolumes (ListVolumesRequest) returns (ListVolumesResponse) {} // 查询存储池的容量 rpc GetCapacity (GetCapacityRequest) returns (GetCapacityResponse) {} // 查询 Controller 支持的功能 rpc ControllerGetCapabilities (ControllerGetCapabilitiesRequest) returns (ControllerGetCapabilitiesResponse) {} // 创建一个 Snapshot rpc CreateSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {} // 删除一个 Snapshot rpc DeleteSnapshot (DeleteSnapshotRequest) returns (DeleteSnapshotResponse) {} // 查询所有创建的 Snapshot rpc ListSnapshots (ListSnapshotsRequest) returns (ListSnapshotsResponse) {} // 扩容一个 Volume rpc ControllerExpandVolume (ControllerExpandVolumeRequest) returns (ControllerExpandVolumeResponse) {} // 查询一个 Volume rpc ControllerGetVolume (ControllerGetVolumeRequest) returns (ControllerGetVolumeResponse) { option (alpha_method) = true; }} 1.2.3 CSI Node CSI Node 提供在 Node 上的操作，包括 MountDevice 与 Setup 等操作。 service Node { // 将 Volume 挂载到一个全局目录，即 MountDevice 操作 rpc NodeStageVolume (NodeStageVolumeRequest) returns (NodeStageVolumeResponse) {} // 将 Volume 取消挂载全局目录 rpc NodeUnstageVolume (NodeUnstageVolumeRequest) returns (NodeUnstageVolumeResponse) {} // 将 Volume 挂载到 Pod 目录，即 Setup 操作 rpc NodePublishVolume (NodePublishVolumeRequest) returns (NodePublishVolumeResponse) {} // 将 Volume 取消挂载 Pod 目录 rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest) returns (NodeUnpublishVolumeResponse) {} // 返回 Volume 信息 rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest) returns (NodeGetVolumeStatsResponse) {} // 扩容 Volume rpc NodeExpandVolume(NodeExpandVolumeRequest) returns (NodeExpandVolumeResponse) {} // 返回 Node 插件功能，例如是否只是 stage/unstage rpc NodeGetCapabilities (NodeGetCapabilitiesRequest) returns (NodeGetCapabilitiesResponse) {} // 返回插件对应的节点信息 rpc NodeGetInfo (NodeGetInfoRequest) returns (NodeGetInfoResponse) {}} ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:1:2","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"2 CSI 相关资源对象 ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"2.1 CSIDriver CSIDriver 用于让 Kubernetes 发现该 CSI 插件，并知晓其拥有的功能。 apiVersion:storage.k8s.io/v1kind:CSIDrivermetadata:name:mycsidriver.example.comspec:attachRequired:truepodInfoOnMount:truefsGroupPolicy:File# added in Kubernetes 1.19, this field is GA as of Kubernetes 1.23volumeLifecycleModes:# added in Kubernetes 1.16, this field is beta- Persistent- EphemeraltokenRequests:# added in Kubernetes 1.20. See status at https://kubernetes-csi.github.io/docs/token-requests.html#status- audience:\"gcp\"- audience:\"\"# empty string means defaulting to the `--api-audiences` of kube-apiserverexpirationSeconds:3600requiresRepublish:true# added in Kubernetes 1.20. See status at https://kubernetes-csi.github.io/docs/token-requests.html#status name - CSI Driver 的名字 attachRequired - 表示 Volume 是否需要 Attach 操作（Kubernetes 是否需要创建 VolumeAttachment 资源） podInfoOnMount - 表明当调用 Mount 操作时，是否需要传递额外的 Pod 信息给 CSI Driver fsGroupPolicy - 表示 CSI Driver 是否支持：当 Mount Volume 时，修改 Volume 的所有权与权限 volumeLifecycleModes - Volume 支持的模式 tokenRequests - Kubelet 调用 CSI Node 的 NodePublishVolume() 接口时是否需要提供 Pod 的 ServiceAccount requiresRepublish - 为 true 时，Kubelet 会周期性调用 CSI Node 的 NodePublishVolume() 接口 ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"2.2 CSINode CSI Driver 可以生成特定的关于 Node 的信息，将其保存到 CSINode 对象中，而不用将信息保存到 Kubernetes Node 对象中。Kubernetes 会创建 CSINode 对象，并根据已经注册的 CSIDriver 来填充 CSINode 信息。 kind:CSINodemetadata:name:kube-node-5ownerReferences:- apiVersion:v1kind:Nodename:kube-node-5uid:f442c38a-f0f3-4a79-8153-3e5bcd7c91a4spec:drivers:- name:hostpath.csi.k8s.ionodeID:kube-node-5topologyKeys:- topology.hostpath.csi/node drivers - 该 Node 上运行的所有 CSI Driver name - CSI Driver 的名字 nodeID - Driver 提供的该 Node ID topologyKeys - 针对于该 CSI 的 Topology Key ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"2.3 PersistentVolumeClaim 当用户部署对应 CSI 插件的 PVC 时，由 CSI Controller 来负责创建对应的 PV 并绑定到指定的 PVC。等待 PVC 与 PV 相互绑定后，Kubernetes 就继续进行 Attach 等流程了。 PV 定义中使用 spec.csi 字段来表明该 PV 是 CSI 提供的。 apiVersion:v1kind:PersistentVolumemetadata:name:pv0003spec:# ...csi:driver:hostpath.csi.k8s.iovolumeHandle:5a760aab-d579-11eb-ba5c-fa03df0e077dreadonly:falsefsType:FilesystemvolumeAttributes:kind:standardstorage.kubernetes.io/csiProvisionerIdentity:1624246373313-8081-hostpath.csi.k8s.io-kube-node-5# controllerPublishSecretRef: []# nodeStageSecretRef: []# nodePublishSecretRef: [] driver - CSI Driver 的名字 volumeHandle - 插件提供的 Volume ID，用于 Kubernetes 与插件共同标识该 Volume readonly - 表明 Volume 是否是只读 fsType - Volume 类型 volumeAttributes - Volume 的一些元信息，将通过 volume_context 传递给 CSI 插件 controllerPublishSecretRef - 调用 CSI Controller 的 ControllerPublishVolume() 与 ControllerUnpublishVolume() 时传递的 Secret 对象 nodeStageSecretRef - 调用 CSI Node 的 NodeStageVolume() 时传递的 Secret 对象 nodePublishSecretRef - 调用 CSI Node 的 NodePublishVolume() 时传递的 Secret 对象 ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"2.4 VolumeAttachment 当 Kubernetes 需要 Attach Volume 时，其 Kubernetes 会创建 VolumeAttachment 资源表示需要进行 Attach，而 CSI Controller 需要监听 VolumeAttachment 资源然后进行实际的 Attach 操作。 apiVersion:storage/v1kind:VolumeAttachmentmetadata:name:pv0003spec:attacher:www.example.csi.comnodeName:node-1source:persistentVolumeName:pvc0003inlineVolumeSpec:# pvc specstatus:attached:trueattachmentMetadata:{}# attachError:# time:# message:# detachError:# time:# message: spec attacher - 需要执行 Attach 操作的 CSI Driver nodeName - 目标 Node source persistentVolumeName - 指定被 Attach 的 PVC inlineVolumeSpec - 使用内联的 PVC status attached - 表明是否成功 Attach attachmentMetadata - Attach 成功后填充的一些元信息，以提供给后续的操作 attachError - 上一次 Attach 失败信息 detachError - 上一次 Detach 失败信息 ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:2:4","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"3 CSI 实现 在 Kubernetes 实现中，CSI VolumePlugin 实现了 VolumePlugin 接口，提供了 Attach 与 SetUp 等操作，其本质就是创建资源或者调用特定 CSI 插件的接口，类似于一个中间层。 CSI 插件不需要注册到 Controller 层中，相互之间完全围绕着 Kubernetes 资源进行交互。 ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"3.1 Controller 层实现 从上面可以看到，Controller 层主要执行了 Create/Delete、Attach/Detach 和 Snapshot 操作。Create/Delete 操作是由 CSI Controller 插件监听并执行，因此不需要 Kubernetes 中进行代码实现。 而 Attach/Detach 和 Snapshot 操作就是在 VolumePlugin 接口实现上，创建对应的 VolumeAttachment 和 VolumeSnapshot 等资源，然后等待 CSI Controller 执行操作。 type csiPlugin struct { host volume.VolumeHost csiDriverLister storagelisters.CSIDriverLister serviceAccountTokenGetter func(namespace, name string, tr *authenticationv1.TokenRequest) (*authenticationv1.TokenRequest, error) volumeAttachmentLister storagelisters.VolumeAttachmentLister } // ProbeVolumePlugins returns implemented plugins func ProbeVolumePlugins() []volume.VolumePlugin { p := \u0026csiPlugin{ host: nil, } return []volume.VolumePlugin{p} } 3.1.1 Attacher csiPlugin 实现了 CanAttach() 接口，用于让 Kubernetes 知晓是否需要执行 Attach 操作。 // CanAttach 判断 Volume 是否需要 Attach func (p *csiPlugin) CanAttach(spec *volume.Spec) (bool, error) { // inline Volume 不支持 inlineEnabled := utilfeature.DefaultFeatureGate.Enabled(features.CSIInlineVolume) if inlineEnabled { volumeLifecycleMode, err := p.getVolumeLifecycleMode(spec) if err != nil { return false, err } if volumeLifecycleMode == storage.VolumeLifecycleEphemeral { klog.V(5).Info(log(\"plugin.CanAttach = false, ephemeral mode detected for spec %v\", spec.Name())) return false, nil } } // 取得 PV 定义与 CSI Driver name pvSrc, err := getCSISourceFromSpec(spec) if err != nil { return false, err } driverName := pvSrc.Driver // 检查该 CSI Driver 是否需要 Attach skipAttach, err := p.skipAttach(driverName) if err != nil { return false, err } return !skipAttach, nil } func (p *csiPlugin) skipAttach(driver string) (bool, error) { // ... // 得到对应的 CSIDriver 资源 csiDriver, err := p.csiDriverLister.Get(driver) if err != nil { if apierrors.IsNotFound(err) { // CSIDriver 对象不存在，默认为需要 Attach return false, nil } return false, err } // 通过 spec.attachRequired 判断是否需要 Attach if csiDriver.Spec.AttachRequired != nil \u0026\u0026 *csiDriver.Spec.AttachRequired == false { return true, nil } return false, nil } ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"3.2 Kubelet 层的实现 3.2.1 Plugin Watcher - 注册插件 与 Controller 层不同，Kubelet 需要调用 CSI Node 的接口，因此需要一个注册机制，让 CSI 插件注册到 Kubelet 中。 Plugin Manager 会创建并运行一个 Plugin Watcher，Plugin Watcher 会监控一个 socket 根目录及其子目录。一旦目录或子目录中创建了 socket 文件，就认定其是一个 “插件注册” 事件，将其 socket 文件路径加入到 desiredStateOfWorld，等待后续的协调处理。 根目录路径 插件注册的根目录路径为 \u003ckubelet-root\u003e/plugins_registry，默认为 /var/lib/kubelet/plugins_registry。 // Watcher 监控 Plugin 的注册 type Watcher struct { path string // 根目录 fs utilfs.Filesystem fsWatcher *fsnotify.Watcher desiredStateOfWorld cache.DesiredStateOfWorld } Watcher.Start() 经过初始的扫描与处理后，会启动一个 Goroutine 监控文件系统事件，并处理。 func (w *Watcher) Start(stopCh \u003c-chan struct{}) error { // 创建根目录 if err := w.init(); err != nil { return err } // 创建 fs watcher fsWatcher, err := fsnotify.NewWatcher() if err != nil { return fmt.Errorf(\"failed to start plugin fsWatcher, err: %v\", err) } w.fsWatcher = fsWatcher // 扫描当前根目录 if err := w.traversePluginDir(w.path); err != nil { klog.ErrorS(err, \"Failed to traverse plugin socket path\", \"path\", w.path) } // 启动监控文件系统的 Groutine go func(fsWatcher *fsnotify.Watcher) { for { select { case event := \u003c-fsWatcher.Events: if event.Op\u0026fsnotify.Create == fsnotify.Create { // 处理文件创建事件 err := w.handleCreateEvent(event) } else if event.Op\u0026fsnotify.Remove == fsnotify.Remove { // 处理文件删除事件 w.handleDeleteEvent(event) } continue case err := \u003c-fsWatcher.Errors: if err != nil { klog.ErrorS(err, \"FsWatcher received error\") } continue case \u003c-stopCh: w.fsWatcher.Close() return } } }(fsWatcher) return nil } func (w *Watcher) traversePluginDir(dir string) error { // 添加根目录的监控 err := w.fsWatcher.Add(dir) // 遍历根目录处理 return w.fs.Walk(dir, func(path string, info os.FileInfo, err error) error { // ... // do not call fsWatcher.Add twice on the root dir to avoid potential problems. if path == dir { return nil } switch mode := info.Mode(); { case mode.IsDir(): // 根目录下的子目录页加入监控 err := w.fsWatcher.Add(path) case mode\u0026os.ModeSocket != 0: // socket 文件进行处理 event := fsnotify.Event{ Name: path, Op: fsnotify.Create, } w.handleCreateEvent(event) } return nil }) } handleCreateEvent() 与 handleDeleteEvent() 就是将 socket 文件路径 添加/移除 到 desiredStateOfWorld。 func (w *Watcher) handleCreateEvent(event fsnotify.Event) error { // stat file fi, err := os.Stat(event.Name) // ... if !fi.IsDir() { isSocket, err := util.IsUnixDomainSocket(util.NormalizePath(event.Name)) // 忽略其他类型文件 if !isSocket { return nil } // 如果是 socket 文件，注册到 desiredStateOfWorld return w.handlePluginRegistration(event.Name) } // 如果是子目录，继续监控与处理 return w.traversePluginDir(event.Name) } func (w *Watcher) handlePluginRegistration(socketPath string) error { err := w.desiredStateOfWorld.AddOrUpdatePlugin(socketPath) if err != nil { return fmt.Errorf(\"error adding socket path %s or updating timestamp to desired state cache: %v\", socketPath, err) } return nil } func (w *Watcher) handleDeleteEvent(event fsnotify.Event) { socketPath := event.Name w.desiredStateOfWorld.RemovePlugin(socketPath) } 3.2.2 Plugin Manager - 插件管理 Kubelet 中实现了 Plugin Manager 进行插件的记录与管理，其还是依靠 actualStateOfWorld 与 desiredStateOfWorld 协调，根据 Add/Remove 事件调用注册的 RegisterPlugin() 与 DeRegisterPlugin() 回调。 其核心逻辑由 Reconciler 实现。 // Reconciler runs a periodic loop to reconcile the desired state of the world // with the actual state of the world by triggering register and unregister // operations. Also provides a means to add a handler for a plugin type. type Reconciler interface { // 运行 Reconciler Run(stopCh \u003c-chan struct{}) // 注册 Plugin 事件的回调 AddHandler(pluginType string, pluginHandler cache.PluginHandler) } type reconciler struct { operationExecutor operationexecutor.OperationExecutor loopSleepDuration time.Duration desiredStateOfWorld cache.DesiredStateOfWorld actualStateOfWorld cache.ActualStateOfWorld handlers map[string]cache.PluginHandler // Plugin 事件回调 sync.RWMutex } 周期性的协调流程依旧是对比 actualStateOfWorld 与 desiredStateOfWorld 进行处理，并调用对应注册的 PluginHandler。 func (rc *reconciler) reconcile() { // 遍历 actualStateOfWorld for _, registeredPlugin := range rc.actualStateOfWorl","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:3:2","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"3.3 CSI Plugin 实现 从 源码阅读 - Volume 实现 中我们知晓了，Kubelet 底层还是使用某个 VolumePlugin 的实现的接口进行 Volume 操作。而在 CSI 情况下，其调用的就是 CSI VolumePlugin。 CSI VolumePlugin 本质上就是一个 Plugin Manager，其会根据 Volume 类型调用指定的 CSI Plugin 的接口。我们以 CSI Mounter 为例，可以看到其最后调用了 CSI Node 的 NodePublishVolume 接口。 type csiPlugin struct { host volume.VolumeHost csiDriverLister storagelisters.CSIDriverLister serviceAccountTokenGetter func(namespace, name string, tr *authenticationv1.TokenRequest) (*authenticationv1.TokenRequest, error) volumeAttachmentLister storagelisters.VolumeAttachmentLister } func (c *csiMountMgr) SetUpAt(dir string, mounterArgs volume.MounterArgs) error { // 得到 CSI Node csi, err := c.csiClientGetter.Get() // 解析 Volume 配置 // 创建 Pod 的目录 parentDir := filepath.Dir(dir) if err := os.MkdirAll(parentDir, 0750); err != nil { return errors.New(log(\"mounter.SetUpAt failed to create dir %#v: %v\", parentDir, err)) } // 读取 Secret nodePublishSecrets = map[string]string{} if secretRef != nil { nodePublishSecrets, err = getCredentialsFromSecret(c.k8s, secretRef) } // CSI Driver 的 `podInfoOnMount` 功能实现 podInfoEnabled, err := c.plugin.podInfoEnabled(string(c.driverName)) if podInfoEnabled { volAttribs = mergeMap(volAttribs, getPodInfoAttrs(c.pod, c.volumeLifecycleMode)) } // CSI Driver 的 `tokenRequest` 功能实现 serviceAccountTokenAttrs, err := c.podServiceAccountTokenAttrs() volAttribs = mergeMap(volAttribs, serviceAccountTokenAttrs) // CSI Driver 的 `fsGroupPolicy` 功能实现 driverSupportsCSIVolumeMountGroup := false var nodePublishFSGroupArg *int64 if utilfeature.DefaultFeatureGate.Enabled(features.DelegateFSGroupToCSIDriver) { driverSupportsCSIVolumeMountGroup, err = csi.NodeSupportsVolumeMountGroup(ctx) if driverSupportsCSIVolumeMountGroup { nodePublishFSGroupArg = mounterArgs.FsGroup } } // 调用 CSI Node 接口 err = csi.NodePublishVolume( ctx, volumeHandle, readOnly, deviceMountPath, dir, accessMode, publishContext, volAttribs, nodePublishSecrets, fsType, mountOptions, nodePublishFSGroupArg, ) // ... return nil } 那最后的问题就是，CSI Plugin 是如何注册到 CSI VolumePlugin 中的？CSI VolumePlugin 实现了前面所说的 PluginHandler 接口，并在 Kubelet 启动时注册到了 Plugin Manager 中。 因此当新的 CSI Plugin 创建 socket 文件后，Kubelet 的 Plugin Manager 监听到创建事件，然后回调 CSI VolumePlugin 的 RegisterPlugin() 的接口，记录下来。 // RegisterPlugin is called when a plugin can be registered func (h *RegistrationHandler) RegisterPlugin(pluginName string, endpoint string, versions []string) error { // 记录下 CSI Plugin csiDrivers.Set(pluginName, Driver{ endpoint: endpoint, highestSupportedVersion: highestSupportedVersion, }) // ... return nil } ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:3:3","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"3.4 总结 ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:3:4","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"4 实现一个 CSI 插件 ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"参考 《Kubernetes CSI Developer Document》 CSI Spec ","date":"2021-11-20","objectID":"/posts/cloud_computing/k8s_learning/11-csi/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - 11 - CSI","uri":"/posts/cloud_computing/k8s_learning/11-csi/"},{"categories":["Kubernetes 学习"],"content":"Kubernetes Volume 的背后原理","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"1 概述 以一个云存储作为 Volume 为例，其架构图如下： Deploy PVC - 用户 Deploy PVC 对象。 Provision Volume - PV Controller 基于 StorageClass 创建一个 Volume（无法绑定已有 PV 的情况下），并创建 PV 对象。 Bind PV - PV Controller 将 PV 与 PVC 绑定。 Attach Volume - AttachDetach Controller Attach Volume 到 Node 上，Node 上看来是一个块设备。 Mount Device - Kubelet 将块设备 Mount 到一个全局的目录。 Setup Volume - Kubelet 准备 Pod 特定的目录，该目录后续用于挂载到各个容器中的目录。 上述过程是一个完成的过程，不同的 Volume 类型可能只会执行其中的几步，例如一些 “特殊” 的 Volume（例如 hostpath、secret、configmap 等），仅仅实现了 SetUp 步骤。 对应的，销毁流程就是创建流程的相反操作： TearDown Volume - Kubelet 移除为 Pod 准备的目录。 Unmount Device - Unmount 全局目录。 Detach Volume - 从 Node 上 Detach Volume。 Delete - 特定条件下，删除该 Volume。 Note 上述步骤很重要，是后续所有组件执行的操作。 ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:1:0","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"2 Volume Plugin 无论是控制面的 Controller 还是 Kubelet，涉及到实际的 Volume 操作（Create/Delete、Attach/Detach 等）时，都会调用对应的 VolumePlugin 接口。VolumePlugin 为管理组件提供了 概述 中的具体操作接口。 目前 Kubernetes 内置了许多的 Volume Plugin，可以分为三类： 内置 Volume - 包括 ConfigMap、HostPath、以及各个云厂商的 Volume，其代码是原生内置在 Controller Manager 中的； CSI - Container Storage Interface，在 Controller Mananger 中是一个独立的 Volume Plugin，由该 Plugin 来调用注册的 CSI Plugin； FlexVolume - 通过可执行文件实现的动态注册插件 ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:2:0","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"2.1 基础接口 先了解下最基本执行各个操作的接口，这些接口都是用于特定 Pod 执行特定的 Volume 操作（Pod 与 Volume 都是确定的了），由 VolumePlugin 的方法得到。 可以看到，各个接口对应了在 概述 所对应的步骤。 Mounter CanMount() - SetUp() 调用前检查所需的组件（例如插件程序）是否准备好 SetUp() 与 SetUpAt() - 为 Pod 准备好对应的 Volume 目录 Unmounter TearDown() 与 TearDownAt() - 移除为 Pod 准备的 Volume 目录 CustomBlockVolumeMapper SetUpDevice() - 为 Pod 准备好 Device MapPodDevice() - 将 Device 映射给 Pod CustomBlockVolumeUnmapper TearDownDevice() - 移除为 Pod 准备好的 Device UnmapPodDevice() - 取消 Device 映射 Provisioner Provision() - 创建一个 Volume Deleter Delete() - 删除一个 Volume Attacher Attach() - Attach Volume 到 Node 上 VolumesAreAttached() - 返回 Node 上的 attached Volume Detacher Detach() - Detach Volume DeviceMounter MountDevice() - 将 attached Volume 挂载到一个全局目录 DeviceUnmounter UnmountDevice() - 取消 attached Volume 的全局目录挂载 // Volume represents a directory used by pods or hosts on a node. All method // implementations of methods in the volume interface must be idempotent. type Volume interface { // GetPath 返回 volume 需要的挂载路径 GetPath() string // MetricsProvider 提供统计接口 MetricsProvider } // BlockVolume interface provides methods to generate global map path // and pod device map path. type BlockVolume interface { // GetGlobalMapPath returns a global map path which contains // bind mount associated to a block device. // ex. plugins/kubernetes.io/{PluginName}/{DefaultKubeletVolumeDevicesDirName}/{volumePluginDependentPath}/{pod uuid} GetGlobalMapPath(spec *Spec) (string, error) // GetPodDeviceMapPath returns a pod device map path // and name of a symbolic link associated to a block device. // ex. pods/{podUid}/{DefaultKubeletVolumeDevicesDirName}/{escapeQualifiedPluginName}/, {volumeName} GetPodDeviceMapPath() (string, string) SupportsMetrics() bool MetricsProvider } // Mounter 接口为 Pod 提供 Volume 对应的目录 type Mounter interface { Volume // CanMount 优先于 Setup 调用，用于检查当前环境是否包含能够 Mount 的组件 CanMount() error // SetUp 为 Pod 提供 Volume 的目录，目录路径由自己决定 SetUp(mounterArgs MounterArgs) error // SetUpAt 为 Pod 提供 Volume 的目录，目录路径由 dir 指定 SetUpAt(dir string, mounterArgs MounterArgs) error // GetAttributes 返回 Volume 的属性 GetAttributes() Attributes } // Unmounter interface provides methods to cleanup/unmount the volumes. type Unmounter interface { Volume // TearDown 取消为 Pod 提供的目录 TearDown() error // TearDown 取消为 Pod 提供的指定目录 TearDownAt(dir string) error } // BlockVolumeMapper interface is a mapper interface for block volume. type BlockVolumeMapper interface { BlockVolume } // CustomBlockVolumeMapper interface provides custom methods to set up/map the volume. type CustomBlockVolumeMapper interface { BlockVolumeMapper // SetUpDevice 准备为 Pod 提供的块设备 SetUpDevice() (stagingPath string, err error) // MapPodDevice maps the block device to a path and return the path. MapPodDevice() (publishPath string, err error) // GetStagingPath returns path that was used for staging the volume // it is mainly used by CSI plugins GetStagingPath() string } // BlockVolumeUnmapper interface is an unmapper interface for block volume. type BlockVolumeUnmapper interface { BlockVolume } // CustomBlockVolumeUnmapper interface provides custom methods to cleanup/unmap the volumes. type CustomBlockVolumeUnmapper interface { BlockVolumeUnmapper // TearDownDevice 移除为 Pod 提供的块设备 TearDownDevice(mapPath string, devicePath string) error // UnmapPodDevice removes traces of the MapPodDevice procedure. UnmapPodDevice() error } type Provisioner interface { // Provision 创建一个 Volume Provision(selectedNode *v1.Node, allowedTopologies []v1.TopologySelectorTerm) (*v1.PersistentVolume, error) } type Deleter interface { Volume // Deleter 移除一个 Volume Delete() error } type Attacher interface { DeviceMounter // Attach Volume 到指定的 Node 上 Attach(spec *Spec, nodeName types.NodeName) (string, error) // VolumesAreAttached 查询 Node 上的 attached Volume VolumesAreAttached(specs []*Spec, nodeName types.NodeName) (map[*Spec]bool, error) // WaitForAttach 等待 attach 结束 WaitForAttach(spec *Spec, devicePath string, pod *v1.Pod, timeout time.Duration) (string, error) } type Device","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:2:1","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"2.2 VolumePlugin 接口 VolumePlugin 是 Volume Plugin 的最基本的 interface。 type Spec struct { Volume *v1.Volume PersistentVolume *v1.PersistentVolume ReadOnly bool InlineVolumeSpecForCSIMigration bool Migrated bool } // VolumePlugin is an interface to volume plugins that can be used on a // kubernetes node (e.g. by kubelet) to instantiate and manage volumes. type VolumePlugin interface { // Init 初始化 plugin Init(host VolumeHost) error // Name 返回 plugin name // 命名方式为 \"example.com/volume\"，\"kubernetes.io\" 由 kubernete 内置 Volume 预留使用 GetPluginName() string // GetVolumeName 基于 spec 返回一个唯一的 volume name 或者 id // 对于 Attach 与 Detach 操作会传递该 name 来让 plugin 识别 volume GetVolumeName(spec *Spec) (string, error) // CanSupport 返回 plugin 是否支持该 spec CanSupport(spec *Spec) bool // RequiresRemount 表明 plugin 是否支持 volume 的重新挂载 RequiresRemount(spec *Spec) bool // NewMounter 创建一个 Mounter NewMounter(spec *Spec, podRef *v1.Pod, opts VolumeOptions) (Mounter, error) // NewUnmounter 创建一个 UnMounter NewUnmounter(name string, podUID types.UID) (Unmounter, error) // ConstructVolumeSpec 由 volume name 得到一个 spec ConstructVolumeSpec(volumeName, volumePath string) (*Spec, error) // SupportsMountOption 表明是否支持挂载选项 SupportsMountOption() bool // SupportsBulkVolumeVerification checks if volume plugin type is capable // of enabling bulk polling of all nodes. This can speed up verification of // attached volumes by quite a bit, but underlying pluging must support it. SupportsBulkVolumeVerification() bool } Mounter 的作用 Mounter 并不是进行块设备的挂载的（这是 DeviceMounter 的工作），而是为特定的 Pod.Volume 准备一个目录，而该目录后续由 kubelet 挂载给 Pod 的各个容器。 VolumePlugin 是一个最基本的 interface，在 VolumePlugin 之上还根据功能有着各样的 Plugin interface。 PersistentVolumePlugin - Volume 支持保存持久数据 RecyclableVolumePlugin - 支持 Recycle Volume DeletableVolumePlugin - 支持 Delete Volume ProvisionableVolumePlugin - 支持 Provision Volume DeviceMountableVolumePlugin - 需要先挂载 Device，才可以将 Volume 挂载给 Pod AttachableVolumePlugin - 需要先 Attach 到 Node，再挂载 Device，才可以将 Volume 挂载给 Pod ExpandableVolumePlugin - Volume 支持从控制面触发 Expand NodeExpandableVolumePlugin - Volume 支持 Node 触发 Expand VolumePluginWithAttachLimits - 限制 Node 能够 Attach Volume 的数量 BlockVolumePlugin - 支持 Block Volume // PersistentVolumePlugin is an extended interface of VolumePlugin and is used // by volumes that want to provide long term persistence of data type PersistentVolumePlugin interface { VolumePlugin // GetAccessModes describes the ways a given volume can be accessed/mounted. GetAccessModes() []v1.PersistentVolumeAccessMode } // RecyclableVolumePlugin is an extended interface of VolumePlugin and is used // by persistent volumes that want to be recycled before being made available // again to new claims type RecyclableVolumePlugin interface { VolumePlugin // Recycle knows how to reclaim this // resource after the volume's release from a PersistentVolumeClaim. // Recycle will use the provided recorder to write any events that might be // interesting to user. It's expected that caller will pass these events to // the PV being recycled. Recycle(pvName string, spec *Spec, eventRecorder recyclerclient.RecycleEventRecorder) error } // DeletableVolumePlugin is an extended interface of VolumePlugin and is used // by persistent volumes that want to be deleted from the cluster after their // release from a PersistentVolumeClaim. type DeletableVolumePlugin interface { VolumePlugin // NewDeleter creates a new volume.Deleter which knows how to delete this // resource in accordance with the underlying storage provider after the // volume's release from a claim NewDeleter(spec *Spec) (Deleter, error) } // ProvisionableVolumePlugin is an extended interface of VolumePlugin and is // used to create volumes for the cluster. type ProvisionableVolumePlugin interface { VolumePlugin // NewProvisioner creates a new volume.Provisioner which knows how to // create PersistentVolumes in accordance with the plugin's underlying // storage provider NewProvisioner(options VolumeOptions) (Provisioner, error) } // AttachableVolumePlugi","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:2:2","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"2.3 DynamicPluginProber 接口 大部分内置的插件都以代码方式实现了 VolumePlugin 接口，以静态方式注册。而 FlexVolume 是由 DynamicPluginProber 接口进行 Probe 后，以动态方式注册。 type ProbeEvent struct { Plugin VolumePlugin // VolumePlugin that was added/updated/removed. if ProbeEvent.Op is 'ProbeRemove', Plugin should be nil PluginName string Op ProbeOperation // The operation to the plugin } type DynamicPluginProber interface { Init() error // If an error occurs, events are undefined. Probe() (events []ProbeEvent, err error) } 调用 Probe() 接口会去插件目录查找插件的可执行文件，并创建 flexVolumePlugin 类，flexVolumePlugin 即实现了 VolumePlugin 接口，可以注册到 VolumePluginMgr 中。 插件目录 Probe() 接口查找的目录格式为 \u003cplugindir\u003e/\u003cvendor~driver\u003e/\u003cdriver\u003e，\u003cplugindir\u003e 在 Kubelet 由参数 --volume-plugin-dir 指定，在 Controller Manager 由参数 --flex-volume-plugin-dir。 默认为路径 /usr/libexec/kubernetes/kubelet-plugins/volume/exec。 DynamicPluginProber 周期性进行 Probe() 操作，注册新发现的 flexVolumePlugin，移除不存在的 flexVolumePlugin。 当调用 flexVolumePlugin 的操作接口（NewMounter、NewAttacher 等），会转发调用各个可执行文件的命令。例如： $ ./uds Usage: flexvoldrv [command] Available Commands: help Help about any command init Flex volume init command. mount Flex volume unmount command. unmount Flex volume unmount command. version Print version ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:2:3","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"2.4 VolumePluginMgr 类 所有的 VolumePlugin 会注册到 VolumePluginMgr 对象中管理。 type VolumePluginMgr struct { mutex sync.RWMutex plugins map[string]VolumePlugin // 静态注册的 VolumePlugin prober DynamicPluginProber probedPlugins map[string]VolumePlugin // probe 探测出的 VolumePlugin loggedDeprecationWarnings sets.String Host VolumeHost } 而大多数 Controller 使用时，就会先通过各个 Find 函数来查找可用的 VolumePlugin，然后调用 VolumePlugin 的方法进行实际的操作。 // FindPluginBySpec 查找能够支持 spec 的 VolumePlugin func (pm *VolumePluginMgr) FindPluginBySpec(spec *Spec) (VolumePlugin, error) { pm.mutex.RLock() defer pm.mutex.RUnlock() if spec == nil { return nil, fmt.Errorf(\"could not find plugin because volume spec is nil\") } // 调用静态注册的 VolumePlugin.CanSupport() 函数筛选 matches := []VolumePlugin{} for _, v := range pm.plugins { if v.CanSupport(spec) { matches = append(matches, v) } } // 进行一次 Probe Plugin pm.refreshProbedPlugins() // 调用动态注册的 VolumePlugin.CanSupport() 函数筛选 for _, plugin := range pm.probedPlugins { if plugin.CanSupport(spec) { matches = append(matches, plugin) } } // 没有任何 Volume Plugin 能够处理 if len(matches) == 0 { return nil, fmt.Errorf(\"no volume plugin matched\") } // 多个 Volume Plugin 能够处理 if len(matches) \u003e 1 { matchedPluginNames := []string{} for _, plugin := range matches { matchedPluginNames = append(matchedPluginNames, plugin.GetPluginName()) } return nil, fmt.Errorf(\"multiple volume plugins matched: %s\", strings.Join(matchedPluginNames, \",\")) } // 返回唯一的 Volume Plugin // Issue warning if the matched provider is deprecated pm.logDeprecation(matches[0].GetPluginName()) return matches[0], nil } // FindPluginByName 通过 plugin name 查找 Volume Plugin func (pm *VolumePluginMgr) FindPluginByName(name string) (VolumePlugin, error) { pm.mutex.RLock() defer pm.mutex.RUnlock() // 查找静态注册的 VolumePlugin // Once we can get rid of legacy names we can reduce this to a map lookup. matches := []VolumePlugin{} if v, found := pm.plugins[name]; found { matches = append(matches, v) } // 进行一次 Probe Plugin pm.refreshProbedPlugins() // 查找动态注册的 VolumePlugin if plugin, found := pm.probedPlugins[name]; found { matches = append(matches, plugin) } // 没有任何 VolumePlugin 或者多个 VolumePlugin 视为错误 if len(matches) == 0 { return nil, fmt.Errorf(\"no volume plugin matched name: %s\", name) } if len(matches) \u003e 1 { matchedPluginNames := []string{} for _, plugin := range matches { matchedPluginNames = append(matchedPluginNames, plugin.GetPluginName()) } return nil, fmt.Errorf(\"multiple volume plugins matched: %s\", strings.Join(matchedPluginNames, \",\")) } // 返回唯一的 VolumePlugin // Issue warning if the matched provider is deprecated pm.logDeprecation(matches[0].GetPluginName()) return matches[0], nil } func (pm *VolumePluginMgr) FindPersistentPluginBySpec(spec *Spec) (PersistentVolumePlugin, error) { // ... } // 其他各个类型的 VolumePlugin ... ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:2:4","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"3 PV Controller PV Controller 主要负责 PV 与 PVC 的绑定，并负责了 PV 的创建与销毁。 PV Controller 就是一个控制器，监听着 PV 与 PVC 的事件，并对应进行 PV 与 PVC 的协调处理。 ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:3:0","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"3.1 触发逻辑 Controller 基本都是通过 Informer 实现，周期性与事件触发。 // NewController 创建一个 PV Controller 对象 func NewController(p ControllerParameters) (*PersistentVolumeController, error) { // 构建 Controller 对象，基本的参数都来自于 ControllerParameters controller := \u0026PersistentVolumeController{ volumes: newPersistentVolumeOrderedIndex(), claims: cache.NewStore(cache.DeletionHandlingMetaNamespaceKeyFunc), kubeClient: p.KubeClient, eventRecorder: eventRecorder, runningOperations: goroutinemap.NewGoRoutineMap(true /* exponentialBackOffOnError */), cloud: p.Cloud, enableDynamicProvisioning: p.EnableDynamicProvisioning, clusterName: p.ClusterName, createProvisionedPVRetryCount: createProvisionedPVRetryCount, createProvisionedPVInterval: createProvisionedPVInterval, claimQueue: workqueue.NewNamed(\"claims\"), volumeQueue: workqueue.NewNamed(\"volumes\"), resyncPeriod: p.SyncPeriod, operationTimestamps: metrics.NewOperationStartTimeCache(), } // 初始化所有的 Volume Plugin if err := controller.volumePluginMgr.InitPlugins(p.VolumePlugins, nil /* prober */, controller); err != nil { return nil, fmt.Errorf(\"could not initialize volume plugins for PersistentVolume Controller: %w\", err) } // 监听 PV 事件，注册回调 p.VolumeInformer.Informer().AddEventHandler( cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { controller.enqueueWork(controller.volumeQueue, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { controller.enqueueWork(controller.volumeQueue, newObj) }, DeleteFunc: func(obj interface{}) { controller.enqueueWork(controller.volumeQueue, obj) }, }, ) controller.volumeLister = p.VolumeInformer.Lister() controller.volumeListerSynced = p.VolumeInformer.Informer().HasSynced // 监听 PVC 事件，注册回调 p.ClaimInformer.Informer().AddEventHandler( cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { controller.enqueueWork(controller.claimQueue, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { controller.enqueueWork(controller.claimQueue, newObj) }, DeleteFunc: func(obj interface{}) { controller.enqueueWork(controller.claimQueue, obj) }, }, ) controller.claimLister = p.ClaimInformer.Lister() controller.claimListerSynced = p.ClaimInformer.Informer().HasSynced // 构建对象其他属性 // ... return controller, nil } // enqueueWork enqueueWork 将对象的 key 放入队列 func (ctrl *PersistentVolumeController) enqueueWork(queue workqueue.Interface, obj interface{}) { // Beware of \"xxx deleted\" events if unknown, ok := obj.(cache.DeletedFinalStateUnknown); ok \u0026\u0026 unknown.Obj != nil { obj = unknown.Obj } objName, err := controller.KeyFunc(obj) if err != nil { return } queue.Add(objName) } 可以看到，PV/PVC 的事件回调都放入了对应的 controller.volumeQueue/controller.claimQueue 队列。 在 PV Controller 的 Run 函数时，启动了三个协程进行处理，分别进行： resync 处理 volumeQueue 处理 claimQueue。 // Run 启动 Controller 的控制循环 func (ctrl *PersistentVolumeController) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() defer ctrl.claimQueue.ShutDown() defer ctrl.volumeQueue.ShutDown() // 等待 cache 填充 if !cache.WaitForNamedCacheSync(\"persistent volume\", stopCh, ctrl.volumeListerSynced, ctrl.claimListerSynced, ctrl.classListerSynced, ctrl.podListerSynced, ctrl.NodeListerSynced) { return } // 初始化 ctrl.Store ctrl.initializeCaches(ctrl.volumeLister, ctrl.claimLister) // 执行各个工作函数 go wait.Until(ctrl.resync, ctrl.resyncPeriod, stopCh) go wait.Until(ctrl.volumeWorker, time.Second, stopCh) go wait.Until(ctrl.claimWorker, time.Second, stopCh) \u003c-stopCh } 先看下 resync，其作用就是为了比 Informer 更短周期的进行周期性的控制循环。因为 Shared Informer 的触发周期是所有使用者相同的，所以自己实现了一个 resync 函数。 // resync supplements short resync period of shared informers - we don't want // all consumers of PV/PVC shared informer to have a short resync period, // therefore we do our own. func (ctrl *PersistentVolumeController) resync() { // List 所有 PVC，放入队列 pvcs, err := ctrl.claimLister.List(labels.NewSelector()) if err != nil { return } for _, pvc := range pvcs { ctrl.enqueueWork(ctrl.claimQueue, pvc) } // List 所有 PV，放入队列 pvs, err := ctrl.volumeLister.List(labels.NewSelector()) if err != nil { return } for _, pv := rang","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:3:1","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"3.2 PV 的控制循环 PV 的控制循环主要负责了 PV 的状态更新，以及 PV 的回收。而 PV 创建与绑定相关都是由后续的 PVC 的控制循环处理。 PV 控制循环大概可以分为几个步骤： 检查 PV 的绑定情况： PV 还未绑定 PVC (PV spec.claimRef == nil)，更新 PV status.phase 为 “Available”。 PV 绑定了 PVC (PV spec.claimRef == nil)： 绑定的 PVC 还未创建 (PV spec.claimRef.UID == “\")，更新 PV status.phase 为 “Available”，后续不执行。 绑定的 PV 创建过了 (PV spec.claimRef.UID != “\")，检查实际的 PV 是否存在。 会查询三个地方，一个查询成功即表明存在 Controller 自己维护的 Store Informer.Cache APiServer 如果 PV 已经绑定了 PVC，但是 PVC 不存在，说明 PVC 已经被删除了 更新 PV status.phase 为 “Released”。 根据 PV spec.persistentVolumeReclaimPolicy，执行对 PV 的操作： “Retain” - 不执行任何操作 “Recycle” - 调用 CSI 插件回收 Volume，解除 PV 与 PVC 的绑定 “Delete” - 调用 CSI 插件删除 Volume，并删除 PV 对象 如果 PV 已经绑定了 PVC，但是 PVC 还未绑定 PV 检查 PVC 与 PV 的 VolumeMode 是否匹配 触发一次 syncClaim 如果 PV 与 PVC 相互绑定，那么更新 PV status.phase 为 “Bound”。 如果 PV 与 PVC 绑定不对称，那么解除绑定，允许的情况下还会删除 PV。 func (ctrl *PersistentVolumeController) syncVolume(volume *v1.PersistentVolume) error { // .. if volume.Spec.ClaimRef == nil { // PV Spec 没有指定绑定 PVC，那么更新 Volume Phase 为 \"Available\" if _, err := ctrl.updateVolumePhase(volume, v1.VolumeAvailable, \"\"); err != nil { return err } return nil } else /* pv.Spec.ClaimRef != nil */ { // PV Spec 指定了绑定了 PVC // 绑定的 PVC 还未存在（没有 UID），表明给 PV 是预留的，更新 Volume Phase 为 \"Available\" if volume.Spec.ClaimRef.UID == \"\" { if _, err := ctrl.updateVolumePhase(volume, v1.VolumeAvailable, \"\"); err != nil { // Nothing was saved; we will fall back into the same // condition in the next call to this method return err } return nil } // 到这里，说明绑定的 PVC 已经存在 // Get the PVC by _name_ var claim *v1.PersistentVolumeClaim claimName := claimrefToClaimKey(volume.Spec.ClaimRef) // 查询 PVC 是否还存在 obj, found, err := ctrl.claims.GetByKey(claimName) if err != nil { return err } if !found { // 如果 PVC 在自己的 store 不存在，进行 double check // If the PV was created by an external PV provisioner or // bound by external PV binder (e.g. kube-scheduler), it's // possible under heavy load that the corresponding PVC is not synced to // controller local cache yet. So we need to double-check PVC in // 1) informer cache // 2) apiserver if not found in informer cache // to make sure we will not reclaim a PV wrongly. // Note that only non-released and non-failed volumes will be // updated to Released state when PVC does not exist. if volume.Status.Phase != v1.VolumeReleased \u0026\u0026 volume.Status.Phase != v1.VolumeFailed { obj, err = ctrl.claimLister.PersistentVolumeClaims(volume.Spec.ClaimRef.Namespace).Get(volume.Spec.ClaimRef.Name) if err != nil \u0026\u0026 !apierrors.IsNotFound(err) { return err } found = !apierrors.IsNotFound(err) if !found { obj, err = ctrl.kubeClient.CoreV1().PersistentVolumeClaims(volume.Spec.ClaimRef.Namespace).Get(context.TODO(), volume.Spec.ClaimRef.Name, metav1.GetOptions{}) if err != nil \u0026\u0026 !apierrors.IsNotFound(err) { return err } found = !apierrors.IsNotFound(err) } } } if !found { klog.V(4).Infof(\"synchronizing PersistentVolume[%s]: claim %s not found\", volume.Name, claimrefToClaimKey(volume.Spec.ClaimRef)) // Fall through with claim = nil } else { var ok bool claim, ok = obj.(*v1.PersistentVolumeClaim) if !ok { return fmt.Errorf(\"cannot convert object from volume cache to volume %q!?: %#v\", claim.Spec.VolumeName, obj) } klog.V(4).Infof(\"synchronizing PersistentVolume[%s]: claim %s found: %s\", volume.Name, claimrefToClaimKey(volume.Spec.ClaimRef), getClaimStatusForLogging(claim)) } if claim != nil \u0026\u0026 claim.UID != volume.Spec.ClaimRef.UID { claim = nil } if claim == nil { // 如果最终检查的该 PVC 还是不存在，那么 PVC 已经被删除了 // 更新 Volume Phase 为 \"Released\" if volume.Status.Phase != v1.VolumeReleased \u0026\u0026 volume.Status.Phase != v1.VolumeFailed { if volume, err = ctrl.updateVolumePhase(volume, v1.VolumeReleased, \"\"); err != nil { // Nothing was saved; we will fall back into the same condition // in the next call to this method return err } } // 根据 ReclaimPolicy 决定是否销毁 PV if err = ctrl.reclaimVolume(volume); err != nil { return err } // 如果 ReclaimPolicy 为保留，后续操作不执行 if volume.Spec.PersistentVolumeReclaimPolicy == v1.PersistentVolumeReclaimRetain { ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:3:2","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"3.3 PVC 的控制循环 PVC 的控制循环负责 PVC 与 PV 的绑定，如果 PVC 指定 StorageClass 还可能创建一个 PV。 PVC 控制循环分为两种情况： PVC 已经由 Controller 绑定了 PV（判断 PVC annotation “pv.kubernetes.io/bind-completed”），执行 syncBoundClaim() 方法。 PVC 还未由 Controller 绑定了 PV（判断 PVC annotation “pv.kubernetes.io/bind-completed”），执行 syncUnboundClaim() 方法。 先看简单的 syncBoundClaim() 逻辑，主要就是检查 PV 与 PVC 的绑定关系是否正常。 PVC spec.volumeName == “\"，说明绑定关系丢失了，更新 PVC status.phase 为 “Lost”。 查询对应绑定的 PV，如果 PV 不存在，说明 PV 丢失，更新 PVC status.phase 为 “Lost”。 对应的 PV 存在，但是 PV 未绑定该 PVC（PV spec.claimRef == nil），那么执行绑定操作。 对应的 PV 存在，但是 PV 绑定的不是该 PVC，说明绑定关系丢失了，更新 PVC status.phase 为 “Lost”。 func (ctrl *PersistentVolumeController) syncBoundClaim(claim *v1.PersistentVolumeClaim) error { // Annotation 表明 PVC 已经绑定了 PV，但是 PVC Spec.VolumeName 为空，说明 PV 丢失了，更新 Phase 为 \"Lost\" if claim.Spec.VolumeName == \"\" { if _, err := ctrl.updateClaimStatusWithEvent(claim, v1.ClaimLost, nil, v1.EventTypeWarning, \"ClaimLost\", \"Bound claim has lost reference to PersistentVolume. Data on the volume is lost!\"); err != nil { return err } return nil } // 查询 PVC 绑定的 PV obj, found, err := ctrl.volumes.store.GetByKey(claim.Spec.VolumeName) if err != nil { return err } if !found { // 对应的 PV 不存在，表明 PV 丢失了，更新 Phase 为 \"Lost\" if _, err = ctrl.updateClaimStatusWithEvent(claim, v1.ClaimLost, nil, v1.EventTypeWarning, \"ClaimLost\", \"Bound claim has lost its PersistentVolume. Data on the volume is lost!\"); err != nil { return err } return nil } else { volume, ok := obj.(*v1.PersistentVolume) if !ok { return fmt.Errorf(\"cannot convert object from volume cache to volume %q!?: %#v\", claim.Spec.VolumeName, obj) } if volume.Spec.ClaimRef == nil { // 对应 PV 还未绑定，那么将对应 PV 绑定到自身 PVC if err = ctrl.bind(volume, claim); err != nil { return err } return nil } else if volume.Spec.ClaimRef.UID == claim.UID { // PV 与 PVC 都相互绑定了，还是调用一个 bind // NOTE: syncPV can handle this so it can be left out. // NOTE: bind() call here will do nothing in most cases as // everything should be already set. if err = ctrl.bind(volume, claim); err != nil { return err } return nil } else { // 对应的 PV 绑定的不是该 PVC，更新 Phase 为 \"Lost\" if _, err = ctrl.updateClaimStatusWithEvent(claim, v1.ClaimLost, nil, v1.EventTypeWarning, \"ClaimMisbound\", \"Two claims are bound to the same volume, this one is bound incorrectly\"); err != nil { return err } return nil } } } syncUnboundClaim() 主要负责 PV 与 PVC 的绑定关系，如果未绑定会查找合适的 PV。 PV spec.volumeName == “\"，尝试绑定一个合适的 PV 无法查找到合适的 PV 如果 PVC 指定了 StorageClass，那么尝试创建一个 PV。 其他情况，PVC status.phase 为 “Pending”。 找到了合适的 PV，执行绑定操作 PV spec.volumeName != “\"，说明是用户部署时指定了 PV，那么执行绑定操作。 func (ctrl *PersistentVolumeController) syncUnboundClaim(claim *v1.PersistentVolumeClaim) error { if claim.Spec.VolumeName == \"\" { // 确实未绑定 PV // 确认 BindingMode 是否为 \"WaitForFirstConsumer\" delayBinding, err := pvutil.IsDelayBindingMode(claim, ctrl.classLister) if err != nil { return err } // 查找合适的 PV volume, err := ctrl.volumes.findBestMatchForClaim(claim, delayBinding) if err != nil { return fmt.Errorf(\"error finding PV for claim %q: %w\", claimToClaimKey(claim), err) } if volume == nil { // 没找到合适的 PV switch { case delayBinding \u0026\u0026 !pvutil.IsDelayBindingProvisioning(claim): // 延迟绑定，并且还未调度到 Node（通过 PVC annotation \"volume.kubernetes.io/selected-node\" 判断） if err = ctrl.emitEventForUnboundDelayBindingClaim(claim); err != nil { return err } case storagehelpers.GetPersistentVolumeClaimClass(claim) != \"\": // PVC 指定了 StorageClass，那么尝试创建一个 PV if err = ctrl.provisionClaim(claim); err != nil { return err } return nil default: ctrl.eventRecorder.Event(claim, v1.EventTypeNormal, events.FailedBinding, \"no persistent volumes available for this claim and no storage class is set\") } // 更新 PVC 状态为 Pending if _, err = ctrl.updateClaimStatus(claim, v1.ClaimPending, nil); err != nil { return err } return nil } else /* pv != nil */ { // 找了一个合适的 PV，执行绑定操作 claimKey := claimToClaimKey(claim) if err = ctrl.bind(volume, claim); err != nil { return err } return nil } } else /* pvc.Spec.VolumeName != nil */ { // PVC 已经指定绑定的 PV，说明是用户指定的 // 查找对应 PV","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:3:3","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"4 AttachDetach Controller 前面 PV Controller 负责了 PV 的创建与销毁，以及 PV 与 PVC 的绑定与解绑。当 PVC 绑定后，并被 Pod 使用后，由 AttachDetach Controller 负责将其 Attach 到 Node 上，以及负责从 Node 上 Detach。 Note Attach 表示将 Volume 提供到 Node 上，以便后续 Node 上的 kubelet 进行处理。 整体 AttachDetach Controller 还是基于 Controller 模式，但是其核心是依赖于两个 map 的数据进行处理：actualStateOfWorld 与 desiredStateOfWorld。处理流程就是，根据当前状态，执行操作，向期望状态靠拢。 因此，AttachDetach Controller 的核心逻辑可以分为三个部分： 更新 desiredStateOfWorld 更新 actualStateOfWorld Reconcile ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:4:0","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"4.1 更新 desiredStateOfWorld desiredStateOfWorld 记录 Volume 的期望状态，即哪些 Volume 需要 Attach 到哪些 Node。 所有的期望状态来自于 Pod 的 Spec 定义，状态更新来自于： 监听 Pod/PVC 的事件， 周期性遍历所有的 Pod 总之最终都会收敛到同一个函数 ProcessPodVolumes() 进行处理。 func ProcessPodVolumes(pod *v1.Pod, addVolumes bool, desiredStateOfWorld cache.DesiredStateOfWorld, volumePluginMgr *volume.VolumePluginMgr, pvcLister corelisters.PersistentVolumeClaimLister, pvLister corelisters.PersistentVolumeLister, csiMigratedPluginManager csimigration.PluginManager, csiTranslator csimigration.InTreeToCSITranslator) { if pod == nil { return } // 如果 Pod 没有使用 Volume，啥也不干 if len(pod.Spec.Volumes) \u003c= 0 { klog.V(10).Infof(\"Skipping processing of pod %q/%q: it has no volumes.\", pod.Namespace, pod.Name) return } // 查询是否调度到了一个存在的 Node nodeName := types.NodeName(pod.Spec.NodeName) if nodeName == \"\" { return } else if !desiredStateOfWorld.NodeExists(nodeName) { // If the node the pod is scheduled to does not exist in the desired // state of the world data structure, that indicates the node is not // yet managed by the controller. Therefore, ignore the pod. return } // 处理 Pod Spec 中的 Volume for _, podVolume := range pod.Spec.Volumes { // 创建 Volume 对象 volumeSpec, err := CreateVolumeSpec(podVolume, pod, nodeName, volumePluginMgr, pvcLister, pvLister, csiMigratedPluginManager, csiTranslator) if err != nil { klog.V(10).Infof( \"Error processing volume %q for pod %q/%q: %v\", podVolume.Name, pod.Namespace, pod.Name, err) continue } // 找到对应的 CSI Plugin attachableVolumePlugin, err := volumePluginMgr.FindAttachablePluginBySpec(volumeSpec) if err != nil || attachableVolumePlugin == nil { klog.V(10).Infof( \"Skipping volume %q for pod %q/%q: it does not implement attacher interface. err=%v\", podVolume.Name, pod.Namespace, pod.Name, err) continue } uniquePodName := util.GetUniquePodName(pod) if addVolumes { // 添加 Volume 到 desiredStateOfWorld _, err := desiredStateOfWorld.AddPod( uniquePodName, pod, volumeSpec, nodeName) } else { // 移除 Volume 从 desiredStateOfWorld // Remove volume from desired state of world uniqueVolumeName, err := util.GetUniqueVolumeNameFromSpec( attachableVolumePlugin, volumeSpec) if err != nil { continue } desiredStateOfWorld.DeletePod( uniquePodName, uniqueVolumeName, nodeName) } } return } ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:4:1","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"4.2 更新 actualStateOfWorld actualStateOfWorld 记录 Volume 的当前状态，即 Node 上的 Attached Volume。 其状态更新本质来自于： 执行 Volume 操作后，更新 actualStateOfWorld； 启动时与后续周期性执行 sync，更新 actualStateOfWorld； 监听 Node 的 Add/Update/Delete 事件，更新 actualStateOfWorld； 最终操作会收敛到函数 processVolumesInUse()。 // nodeAdd 处理 Node 添加事件 func (adc *attachDetachController) nodeAdd(obj interface{}) { node, ok := obj.(*v1.Node) // 转为 Node 的更新处理 nodeName := types.NodeName(node.Name) adc.nodeUpdate(nil, obj) // 更新 actualStateOfWorld 的 Node adc.actualStateOfWorld.SetNodeStatusUpdateNeeded(nodeName) } // nodeUpdate 处理 Node 更新事件 func (adc *attachDetachController) nodeUpdate(oldObj, newObj interface{}) { node, ok := newObj.(*v1.Node) // Node 添加到 desiredStateOfWorld nodeName := types.NodeName(node.Name) adc.addNodeToDswp(node, nodeName) adc.processVolumesInUse(nodeName, node.Status.VolumesInUse) } // nodeDelete 处理 Node Delete 事件 func (adc *attachDetachController) nodeDelete(obj interface{}) { node, ok := obj.(*v1.Node) // 从 desiredStateOfWorld 删除 Node nodeName := types.NodeName(node.Name) err := adc.desiredStateOfWorld.DeleteNode(nodeName) adc.processVolumesInUse(nodeName, node.Status.VolumesInUse) } func (adc *attachDetachController) processVolumesInUse( nodeName types.NodeName, volumesInUse []v1.UniqueVolumeName) { klog.V(4).Infof(\"processVolumesInUse for node %q\", nodeName) // 获取 Node 上已经 Attached Volume for _, attachedVolume := range adc.actualStateOfWorld.GetAttachedVolumesForNode(nodeName) { mounted := false // 检查 Volume 是否是 Attached for _, volumeInUse := range volumesInUse { if attachedVolume.VolumeName == volumeInUse { mounted = true break } } // 记录到 actualStateOfWorld err := adc.actualStateOfWorld.SetVolumeMountedByNode(attachedVolume.VolumeName, nodeName, mounted) if err != nil { klog.Warningf( \"SetVolumeMountedByNode(%q, %q, %v) returned an error: %v\", attachedVolume.VolumeName, nodeName, mounted, err) } } } 启动时与周期调用 Reconcile 时，都会执行一部分 sync 逻辑，其中会遍历所有 Node 的 Volume，进行检查并更新 actualStateOfWorld。 func (rc *reconciler) sync() { defer rc.updateSyncTime() rc.syncStates() } func (rc *reconciler) updateSyncTime() { rc.timeOfLastSync = time.Now() } func (rc *reconciler) syncStates() { volumesPerNode := rc.actualStateOfWorld.GetAttachedVolumesPerNode() rc.attacherDetacher.VerifyVolumesAreAttached(volumesPerNode, rc.actualStateOfWorld) } ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:4:2","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"4.3 Reconcile 核心的 Reconcile 中主要执行了两个操作： Detach Volume - Attached Volume 存在于 actualStateOfWorld，但是不存在于 desiredStateOfWorld Attach Volume - Volume 存在于 desiredStateOfWorld，但是 actualStateOfWorld 中状态为 Unattached func (rc *reconciler) reconcile() { for _, attachedVolume := range rc.actualStateOfWorld.GetAttachedVolumes() { if !rc.desiredStateOfWorld.VolumeExists( attachedVolume.VolumeName, attachedVolume.NodeName) { // 如果 Attached Volume 存在于 actualStateOfWorld，但是不存在于 desiredStateOfWorld // 表明 Volume 需要 detach // ... attachState := rc.actualStateOfWorld.GetAttachState(attachedVolume.VolumeName, attachedVolume.NodeName) if attachState == cache.AttachStateDetached { // 如果 attachState 已经是 Detached 了，跳过处理 continue } // ... // 将 Volume 标记为 detached err = rc.actualStateOfWorld.RemoveVolumeFromReportAsAttached(attachedVolume.VolumeName, attachedVolume.NodeName) // 更新 Node Status err = rc.nodeStatusUpdater.UpdateNodeStatuses() if err != nil { continue } // 执行 detach Volume klog.V(5).Infof(attachedVolume.GenerateMsgDetailed(\"Starting attacherDetacher.DetachVolume\", \"\")) verifySafeToDetach := !timeout err = rc.attacherDetacher.DetachVolume(attachedVolume.AttachedVolume, verifySafeToDetach, rc.actualStateOfWorld) // ... } } // attach Volume rc.attachDesiredVolumes() // Update Node Status err := rc.nodeStatusUpdater.UpdateNodeStatuses() } func (rc *reconciler) attachDesiredVolumes() { // Ensure volumes that should be attached are attached. for _, volumeToAttach := range rc.desiredStateOfWorld.GetVolumesToAttach() { // 遍历 desiredStateOfWorld // ... attachState := rc.actualStateOfWorld.GetAttachState(volumeToAttach.VolumeName, volumeToAttach.NodeName) if attachState == cache.AttachStateAttached { // Volume/Node exists, touch it to reset detachRequestedTime rc.actualStateOfWorld.ResetDetachRequestTime(volumeToAttach.VolumeName, volumeToAttach.NodeName) continue } // ... err := rc.attacherDetacher.AttachVolume(volumeToAttach.VolumeToAttach, rc.actualStateOfWorld) } } ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:4:3","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"5 Kubelet 的处理 Kubelet 中对 Volume 的处理也是基于 AttachDetach Controller 的方式，依赖于两个 map 的数据进行周期性协调：actualStateOfWorld 与 desiredStateOfWorld。 我们不再说明 actualStateOfWorld 与 desiredStateOfWorld 的填充，只看下其 Reconcile 流程。 ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:5:0","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"5.1 Reconcile reconcile() 整体逻辑就是三步： unmountVolumes() mountAttachVolumes() unmountDetachDevices() func (rc *reconciler) reconcile() { // Unmounts are triggered before mounts so that a volume that was // referenced by a pod that was deleted and is now referenced by another // pod is unmounted from the first pod before being mounted to the new // pod. rc.unmountVolumes() // Next we mount required volumes. This function could also trigger // attach if kubelet is responsible for attaching volumes. // If underlying PVC was resized while in-use then this function also handles volume // resizing. rc.mountAttachVolumes() // Ensure devices that should be detached/unmounted are detached/unmounted. rc.unmountDetachDevices() } 5.1.1 unmountVolumes unmountVolumes 负责 unmount 需要的 volume，通过判断 mounted volume 是否存在于 actualStateOfWorld。 func (rc *reconciler) unmountVolumes() { // 遍历当前节点所有的 mounted volume for _, mountedVolume := range rc.actualStateOfWorld.GetAllMountedVolumes() { // 检查该 volume 与 pod 是否存在于 desiredStateOfWorld if !rc.desiredStateOfWorld.PodExistsInVolume(mountedVolume.PodName, mountedVolume.VolumeName) { // 进行 unmount volume 操作（底层调用的是 VolumePlugin.TearDown） err := rc.operationExecutor.UnmountVolume( mountedVolume.MountedVolume, rc.actualStateOfWorld, rc.kubeletPodsDir) if err != nil \u0026\u0026 !nestedpendingoperations.IsAlreadyExists(err) \u0026\u0026 !exponentialbackoff.IsExponentialBackoff(err) { // Ignore nestedpendingoperations.IsAlreadyExists and exponentialbackoff.IsExponentialBackoff errors, they are expected. // Log all other errors. klog.ErrorS(err, mountedVolume.GenerateErrorDetailed(fmt.Sprintf(\"operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled %v)\", rc.controllerAttachDetachEnabled), err).Error()) } if err == nil { klog.InfoS(mountedVolume.GenerateMsgDetailed(\"operationExecutor.UnmountVolume started\", \"\")) } } } } operationExecutor.UnmountVolume() 是对 unmount 进行了封装，其底层会查找支持的 VolumePlugin，并调用 VolumePlugin.TearDown() 接口。 5.1.2 mountAttachVolumes mountAttachVolumes 负责准备 volume。有两种可能的情况： 如果 volume 还未 attach，而 kubelet 允许执行 attach，那么调用 Attach 操作。 如果 volume 还未 mount，那么执行 MountDevice 与 SetUp 操作。 func (rc *reconciler) mountAttachVolumes() { // 遍历 desiredStateOfWorld // Ensure volumes that should be attached/mounted are attached/mounted. for _, volumeToMount := range rc.desiredStateOfWorld.GetVolumesToMount() { // 查询 actualStateOfWorld 对应的 volume 状态 volMounted, devicePath, err := rc.actualStateOfWorld.PodExistsInVolume(volumeToMount.PodName, volumeToMount.VolumeName) volumeToMount.DevicePath = devicePath // 根据 volMounted 与 err，执行操作 if cache.IsVolumeNotAttachedError(err) { // 如果 err 表明 volume 还未被 attache if rc.controllerAttachDetachEnabled || !volumeToMount.PluginIsAttachable { // volume 未被 attach，而 kubelet 不允许执行 attach 操作，那么等待 Contrller 进行 attach err := rc.operationExecutor.VerifyControllerAttachedVolume( volumeToMount.VolumeToMount, rc.nodeName, rc.actualStateOfWorld) // log } else { // volume 未被 attach，而 kubelet 允许进行 attach 操作，那么由 kubelet 调用 attach 操作 volumeToAttach := operationexecutor.VolumeToAttach{ VolumeName: volumeToMount.VolumeName, VolumeSpec: volumeToMount.VolumeSpec, NodeName: rc.nodeName, } err := rc.operationExecutor.AttachVolume(volumeToAttach, rc.actualStateOfWorld) // log } } else if !volMounted || cache.IsRemountRequiredError(err) { // 如果 volume 还未被 mount，或者 err 需要重新 mount // 调用 MountVolume，底层会进行 MountDevice + SetUp 操作 remountingLogStr := \"\" isRemount := cache.IsRemountRequiredError(err) if isRemount { remountingLogStr = \"Volume is already mounted to pod, but remount was requested.\" } err := rc.operationExecutor.MountVolume( rc.waitForAttachTimeout, volumeToMount.VolumeToMount, rc.actualStateOfWorld, isRemount) // log } else if cache.IsFSResizeRequiredError(err) \u0026\u0026 utilfeature.DefaultFeatureGate.Enabled(features.ExpandInUsePersistentVolumes) { // 如果 volume 为需要扩容 // 调用 ExpandInUseVolume 进行扩容 klog.V(4).InfoS(volumeToMount.GenerateMsgDetailed(\"Starting operationExecutor.ExpandInUseVolume\", \"\"), \"pod\", klog.KObj(vol","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:5:1","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Kubernetes 学习"],"content":"参考 Blog：详解 Kubernetes Volume 的实现原理 Blog：kubelet volume manager 源码分析 ","date":"2021-11-11","objectID":"/posts/cloud_computing/k8s_learning/volume-implementation/:6:0","tags":["k8s","云计算"],"title":"源码阅读 - Volume 实现","uri":"/posts/cloud_computing/k8s_learning/volume-implementation/"},{"categories":["Security"],"content":"Cookie、Session、JWT","date":"2021-10-25","objectID":"/posts/security/credentials/","tags":["security"],"title":"凭证 Credential","uri":"/posts/security/credentials/"},{"categories":["Security"],"content":"在授权过程中，基本每种授权模式目的就是为了得到一个令牌，令牌就是 凭证Credential。目前主流有两种令牌实现方式：Cookie-Session 和 JWT。 ","date":"2021-10-25","objectID":"/posts/security/credentials/:0:0","tags":["security"],"title":"凭证 Credential","uri":"/posts/security/credentials/"},{"categories":["Security"],"content":"1 Cookie-Session HTTP 是无状态协议，实现了简单的 Web 资源交换功能。但是一些情况下也需要一些手段，让服务器接收 HTTP 请求时能够得用户知相关的上下文。为此，RFC 6265 定义了 HTTP 的状态管理机制，在 HTTP 协议中增加了 Set-Cookie 指令。 Set-Cookie 指令的含义是：以键值对的方式向客户端发送一组信息，该信息将在此后每次 HTTP 请求中，以名为 Cookie 的 Header 发送给服务端。服务端就可以根据 Cookie 字段来区分客户端。 Set-Cookie: id=icyfenix; Expires=Wed, 21 Feb 2020 07:28:00 GMT; Secure; HttpOnly 收到该指令后，客户端再次访问时会带有 Cookie Header，例如： GET /index.html HTTP/2.0 Host: icyfenix.cn Cookie: id=icyfenix 这样，每次请求传到服务器的 Cookie，服务端就能够识别出是哪个用户。因为 Cookie 是 HTTP Header，所以通常 Cookie 的值为一个 session id。服务端根据 session id 在保存的信息中检索出用户对应的上下文信息。再加上一些超时自动清理的管理措施。 这种服务端的状态机制就称为 Session。Cookie-Session 就是目前最传统也最常用的客户端服务端状态管理机制。 优点： 安全性优势，因为状态信息都存储于服务端； 服务端管理状态信息，能够实现灵活的管理； 缺点： 由于 Session 存储在服务器内存中，Cookie-Session 当遇到多节点环境时，各个服务器之间的状态信息同步就是个问题，类似于分布式的状态问题； 通常，Cookie-Session 遇到多节点的解决方案有： 负载均衡 - 每个特定用户请求分配到特定的某个服务器； 状态同步 - 每个服务器将 Session 变动同步到其他所有服务器； 单点 - 将 Session 集中放在一个单点服务器，所有服务器无状态，访问单点 Session 服务器读取状态； ","date":"2021-10-25","objectID":"/posts/security/credentials/:1:0","tags":["security"],"title":"凭证 Credential","uri":"/posts/security/credentials/"},{"categories":["Security"],"content":"2 JWT 与 Cookie-Session 保存状态信息的思路相反，当服务器存在多个时，一个状态信息对应客户端只有一个，可以把状态信息保存在客户端，每次随着请求发送给服务端。但是，这个思路的缺点是无法携带大量信息，而且有着泄漏和篡改的风险。 JWT（JSON Web Token）是目前广泛使用的令牌格式，能够确保传输信息不被中间人篡改。 Note JWT 默认是不加密的，因为仅仅解决防止篡改问题，不解决泄漏问题。 JWT 最常见的使用方式是作为名为 Authorization 的 Header 发送服务端。其值前缀定义为 Bearer，类似于： GET /restful/products/1 HTTP/1.1 Host: icyfenix.cn Connection: keep-alive Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX25hbWUiOiJpY3lmZW5peCIsInNjb3BlIjpbIkFMTCJdLCJleHAiOjE1ODQ5NDg5NDcsImF1dGhvcml0aWVzIjpbIlJPTEVfVVNFUiIsIlJPTEVfQURNSU4iXSwianRpIjoiOWQ3NzU4NmEtM2Y0Zi00Y2JiLTk5MjQtZmUyZjc3ZGZhMzNkIiwiY2xpZW50X2lkIjoiYm9va3N0b3JlX2Zyb250ZW5kIiwidXNlcm5hbWUiOiJpY3lmZW5peCJ9.539WMzbjv63wBtx4ytYYw_Fo1ECG_9vsgAn8bheflL8 ","date":"2021-10-25","objectID":"/posts/security/credentials/:2:0","tags":["security"],"title":"凭证 Credential","uri":"/posts/security/credentials/"},{"categories":["Security"],"content":"2.1 JWT 的格式 JWT 传输时是以 Base64 编码后传输，其原文格式以 JSON 格式。总结划分为三个部分，每个部分间用点号 “.” 分隔。 第一部分是 Header： { \"alg\": \"HS256\", \"typ\": \"JWT\" } alg - 签名的算法 typ - 令牌类型 第二部分是 Payload，即令牌真正传输的信息，用户随意定制。 { \"username\": \"icyfenix\", \"authorities\": [ \"ROLE_USER\", \"ROLE_ADMIN\" ], \"scope\": [ \"ALL\" ], \"exp\": 1584948947, \"jti\": \"9d77586a-3f4f-4cbb-9924-fe2f77dfa33d\", \"client_id\": \"bookstore_frontend\" } JWT RFC 中推荐了七项内容： iss（Issuer）- 签发人； exp（Expiration Time）- 令牌过期时间； sub（Subject）- 主题； aud（Audience）- 令牌受众； nbf（Not Before）- 令牌生效时间； iat（Issued At）- 令牌签发时间； jti（JWT ID）- 令牌 ID； 第三部分是 Signature，通过 Header 中定义算法，通过特定密钥（服务器保管）对前面两部分进行加密计算。服务端在接收到 JWT 后，只要内容被篡改过，其签名就无法通过检查。 优点： 服务端无状态 缺点： 令牌难以主动失效。令牌签发后，理论上和认证服务器没有关联了，难以主动让某个令牌失效。通常，会使用 Session 机制在服务端维护一个黑名单，过滤失效的令牌。 容易收到重放攻击。 只能携带优先的数据。 必须考虑令牌在客户端如何存储。 ","date":"2021-10-25","objectID":"/posts/security/credentials/:2:1","tags":["security"],"title":"凭证 Credential","uri":"/posts/security/credentials/"},{"categories":["Security"],"content":"RBAC、OAuth2","date":"2021-10-24","objectID":"/posts/security/authorization/","tags":["security"],"title":"授权 Authorization","uri":"/posts/security/authorization/"},{"categories":["Security"],"content":"认证解决了对用户身份认证的问题，授用于解决用户有着哪些权限的问题。涉及两个独立的问题： 确保授权的过程可靠 - 常用 OAuth2 和 SAML 2.0 确保授权的结果可控 - 常用 DAC、ABAC 和 RBAC。 ","date":"2021-10-24","objectID":"/posts/security/authorization/:0:0","tags":["security"],"title":"授权 Authorization","uri":"/posts/security/authorization/"},{"categories":["Security"],"content":"1 RBAC ","date":"2021-10-24","objectID":"/posts/security/authorization/:1:0","tags":["security"],"title":"授权 Authorization","uri":"/posts/security/authorization/"},{"categories":["Security"],"content":"2 OAuth 2 OAuth 2 是在 RFC 6749 中定义的国际标准，用于解决面向第三方应用（Third-Party Application）的认证授权协议。当一个第三方应用要访问某个系统时，最简单的方法就是将账号密码告知第三方应用，但是这样显然导致了密码泄露，并且还无法很好的回收权限。这也正是 OAuth 2 要解决的问题。 OAuth 2 有许多方式，不过共同特征就是以 令牌Token 代替用户密码作为凭证。 哪怕令牌泄露，也不会导致密码泄露； 令牌可以设定访问资源的范围，以及时效性； 每个应用都有独立的令牌，其中一个泄露不会导致其他应用收到影响； ","date":"2021-10-24","objectID":"/posts/security/authorization/:2:0","tags":["security"],"title":"授权 Authorization","uri":"/posts/security/authorization/"},{"categories":["Security"],"content":"2.1 基本概念 第三方应用 Third-Party Application - 需要得到授权，访问某个资源的应用； 授权服务器 Authorization Server - 根据用户意愿授权第三方应用的服务器； 资源服务器 Resource Server - 保存第三方应用需要访问资源的服务器； 资源所有者 Resource Owner - 拥有授权权限的人； 操作代理 User Agent - 用户用来访问服务器的工具，例如浏览器、HTTPClient； 基本的访问流程如下图，不同方式在其中步骤中有所延展： ","date":"2021-10-24","objectID":"/posts/security/authorization/:2:1","tags":["security"],"title":"授权 Authorization","uri":"/posts/security/authorization/"},{"categories":["Security"],"content":"2.2 授权码模式 授权码模式Authorization Code 是最严格的方式，考虑了几乎所有的敏感信息泄漏的预防和后果。 开始授权前，第三方应用先要到授权服务器注册，从授权服务器中获得应用对应的 ClientID 和 ClientSecret。 定向到授权服务器 - 用户访问时，第三方应用将用户定向到授权服务器的授权页面，向授权服务器提供 ClientID，以及用户如果同意授权后要跳转的 URI。 用户授权 - 授权服务器根据 ClientID 确定第三方应用身份，用户决定是否同意授权。 得到授权码 - 办法用户同意授权后，授权服务器将定向到第 1 步中的回调 URI，并附带上授权码的获取地址作为参数。 获取令牌 - 第三方应用访问授权码地址得到授权码，将授权码与自己的 ClientSecret 作为参数，访问授权服务器来换取令牌。 颁发令牌 - 授权服务器检查授权码和 ClientSecret，无误后颁发令牌。令牌包括访问令牌与刷新令牌。 资源服务器检查访问令牌与授权权限，向第三方应用提供资源。 ","date":"2021-10-24","objectID":"/posts/security/authorization/:2:2","tags":["security"],"title":"授权 Authorization","uri":"/posts/security/authorization/"},{"categories":["Security"],"content":"2.3 隐式授权模式 隐式授权模式Implicit 省略了通过授权码换取令牌的步骤（合并了上述第 3 4 5 步），并且明确禁止发放刷新令牌。 可以看到，授权服务器不会验证第三方应用的身份，在得到用户授权后，直接返回访问令牌给第三方引用。显然，这样减低了安全性，但是无需注册提升了灵活性。 另外，RFC 6749 中明确说明令牌必须是通过 URI 的 Fragment 带回的。 ","date":"2021-10-24","objectID":"/posts/security/authorization/:2:3","tags":["security"],"title":"授权 Authorization","uri":"/posts/security/authorization/"},{"categories":["Security"],"content":"2.4 密码模式 密码模式Resource Owner Password Credentials 整合了认证和授权的过程。第三方应用直接拿着账户和密码向授权服务器换取令牌。 ","date":"2021-10-24","objectID":"/posts/security/authorization/:2:4","tags":["security"],"title":"授权 Authorization","uri":"/posts/security/authorization/"},{"categories":["Security"],"content":"2.5 客户端模式 客户端模式Client Credentials 指第三方应用以用户名义，直接向授权服务器申请资源许可。该模式通常用于管理操作或者自动处理的场景中，即常用于服务间认证授权。 ","date":"2021-10-24","objectID":"/posts/security/authorization/:2:5","tags":["security"],"title":"授权 Authorization","uri":"/posts/security/authorization/"},{"categories":["Security"],"content":"认证基本概念，基本认证机制","date":"2021-10-23","objectID":"/posts/security/authentication/","tags":["security"],"title":"认证 Authentication","uri":"/posts/security/authentication/"},{"categories":["Security"],"content":" 文章内容来自于《凤凰架构》，推荐阅读。 认证Authentication 用来解决系统对用户身份认证的问题，例如用户密码就是最简单的认证机制。用户不仅仅是人，也可能指外部的代码。 从网络资源传输流程上，有着主要三个认证方式： 通信信道上的认证：建立网络连接时，进行身份的认证。典型是基于 SSL/TLS 的传输层认证。 通信协议上的认证：获取资源时，由协议进行身份的认证。典型是基于 HTTP 协议的认证。 通信内容上的认证：获取资源时，由业务进行身份认证。典型是基于 Web 内容的认证。 ","date":"2021-10-23","objectID":"/posts/security/authentication/:0:0","tags":["security"],"title":"认证 Authentication","uri":"/posts/security/authentication/"},{"categories":["Security"],"content":"1 传输层的认证 ","date":"2021-10-23","objectID":"/posts/security/authentication/:1:0","tags":["security"],"title":"认证 Authentication","uri":"/posts/security/authentication/"},{"categories":["Security"],"content":"1.1 摘要、加密与签名 摘要digest 通过哈希算法的易变性与不可逆性，用于判断数据的真伪。 易变性 - 如果输入端发生任何一点变动，哈希结果都会产生极大变化。通过对比摘要，可以判断收到的数据与源数据是否是一致的。 不可逆行 - 从摘要无法还原出源数据，保证公开的摘要不会泄漏源数据。 加密encryption 与摘要的本质区别是，加密是可逆的，通过解密可以得到原数据。根据加密与解密是否使用同一个密钥，可以分为 对称加密算法 和 非对称加密算法。 对称加密算法 - 加密和解密时使用相同的密钥 性能比非对称加密高，但是面临着密钥传输难题（无法保证密钥在传输过程中不泄露）。 非对称加密算法 - 密钥分为公钥与私钥，私钥由用户保管，公钥可以传输给对端。公钥加密数据，只有私钥能解密。私钥加密数据，只有公钥能解密。 因为加解密使用不同密钥，公钥就可以通过不安全的信道传输，解决了密钥传输难题。但是，非对称加密性能比对称加密低几个数量级。 目前在加密方面，现在一版本会结合对称与非对称加密的优点：用非对称加密传输对称加密的密钥，然后采用对称加密来进行通信。 因为非对称加密的加密算法，可以提供两种功能： 公钥加密，私钥解密 - 这就是加密。A 将公钥传输给 B，B 使用公钥加密数据传输给 A，A 使用私钥解密。整个过程是即使公钥或者加密数据泄漏，通信还是安全的。 私钥加密，公钥解密 - 这就是签名。因为私钥加密后，只有公钥可以解密，那么公钥持有者就可以通过解密来验证私钥是否正确。 签名signature 就是类似于客户端通过证书里的公钥，来检查服务端的私钥是否正确。 ","date":"2021-10-23","objectID":"/posts/security/authentication/:1:1","tags":["security"],"title":"认证 Authentication","uri":"/posts/security/authentication/"},{"categories":["Security"],"content":"1.2 数字证书 前面看到，“签名” 的方法是让公钥持有者验证私钥持有者的身份，而关键就在于公钥持有者保存的公钥是否是受信任的。 内部服务，公钥可以来自于自签名，那么公钥肯定是受信任的，因为是开发者自行保存的。 公共服务，公钥需要一个权威的公证人进行一次签名，例如证书机制中的 CA 存在。所有的用户都信任公证人，那么就可以认为公证人签名后的公钥是可以信任的。 第二种方式引出了公钥可信分发的标准，称为 公开密钥基础设置Public Key Infrastructure，简称 PKI。其中，数字证书认证中心 CA 就是权威的机构，可以签发证书。 证书Certificate 是权威 CA 对特定公钥的签名证书，可以理解 CA 将某个公钥使用了 CA 的私钥进行加密。 在认证时，用户使用内置在机器的根证书对证书进行认证，本质上就是使用根证书中的公钥解密证书。认证（解密）后，就可以认为证书是合法的了，而后续就可以使用证书中的公钥进行通信了。 Note 可以看到，证书就是 CA 对公钥签名，而 CA 的证书（公钥）内置在用户机器上。 PKI 中的证书格式是 X.509 格式，除了公钥内容外还包含了用户信息： 版本号 Version - 证书使用哪个版本的 X.509 标准，目前版本为 3。 Version: 3 (0x2) 序列号 Serial Number - 给证书分配的唯一标识符。 Serial Number: 04:00:00:00:00:01:15:4b:5a:c3:94 签名算法标识符 Signature Algorithm ID - 签名证书算法标识，包含非对称加密算法与摘要算法。 Signature Algorithm: sha1WithRSAEncryption 认证机构的数字签名 Certificate Signature - 证书颁发者使用私钥对证书的签名。 认证机构 Issuer Name - 证书颁发者的名字。 Issuer: C=BE, O=GlobalSign nv-sa, CN=GlobalSign Organization Validation CA - SHA256 - G2 有效期限 Validity Period - 证书起始日期和时间，以及终止日期和时间。 Validity Not Before: Nov 21 08:00:00 2020 GMT Not After : Nov 22 07:59:59 2021 GMT 主题信息 Subject - 证书持有人的唯一标识符。 Subject: C=CN, ST=GuangDong, L=Zhuhai, O=Awosome-Fenix, CN=*.icyfenix.cn 公钥信息 Public Key - 证书持有者的公钥，算法标识符和其他密钥参数。 ","date":"2021-10-23","objectID":"/posts/security/authentication/:1:2","tags":["security"],"title":"认证 Authentication","uri":"/posts/security/authentication/"},{"categories":["Security"],"content":"2 HTTP 认证 RFC 7235 中定义了 HTTP 协议的通用认证框架：在未授权用户访问保护区域的资源时，服务器应返回 401 Unauthorized 的状态码，同时在回复 Header 中附带以下两个 Header 之一，告知客户端采取何种方式进行认证。 WWW-Authenticate: \u003c认证方案\u003e realm=\u003c保护区域描述信息\u003e Proxy-Authenticate: \u003c认证方案\u003e realm=\u003c保护区域描述信息\u003e 客户端收到回复后，遵循服务器指定的认证方案，在请求的 Header 中加入身份凭证信息。服务端验证通过后返回资源，否则将返回 403 Forbidden 错误。请求 Header 中包含以下 Header 之一： Authorization: \u003c认证方案\u003e \u003c凭证内容\u003e Proxy-Authorization: \u003c认证方案\u003e \u003c凭证内容\u003e 以简单的家有路由器为例，登陆路由器时，浏览器会收到服务端基于 HTTP Basic 认证的回复： HTTP/1.1 401 Unauthorized Date: Mon, 24 Feb 2020 16:50:53 GMT WWW-Authenticate: Basic realm=\"example from icyfenix.cn\" 接着浏览器会弹出 HTTP Basic 认证对话框，要求用户输入账户与密码。用户输入后，浏览器将信息编码然后发送给服务端： GET /admin HTTP/1.1 Authorization: Basic aWN5ZmVuaXg6MTIzNDU2 服务端收到请求后，检查用户账户密码是否正确。 除了 Basic 认证外，还定义了许多用于实际生产环境的认证方案，例如： Digest - RFC 7616，HTTP 摘要认证，视为 Basic 认证的改良版本。 Bearer - RFC 6750，基于 OAuth2 来完成认证。 HOBA - RFC 7486，一种基于自签名证书的认证方案。 HTTP 认证框架提供了基本的框架，允许自行扩展，并不要求使用 RFC 规范来定义，只要客户端与服务端能够识别并处理私有的认证方案即可。 ","date":"2021-10-23","objectID":"/posts/security/authentication/:2:0","tags":["security"],"title":"认证 Authentication","uri":"/posts/security/authentication/"},{"categories":["Security"],"content":"3 Web 认证 对于非浏览器的用户认证场景中，通常是希望后端系统来实现认证方式，而不是直接使用 HTTP 协议的认证方案。这些依靠内容而不是传输协议来实现的认证方式，统称为 “Web 认证”。 Web 认证没有什么标准，直到 2019 年 3 月，WebAuthn 出现。WebAuthn 不采用传统的密码登陆，而是直接采用生物识别（指纹、人脸、虹膜、声纹）或者实体密钥（USB、蓝牙、NFC 连接的物理密钥容器）作为身份凭证。 ","date":"2021-10-23","objectID":"/posts/security/authentication/:3:0","tags":["security"],"title":"认证 Authentication","uri":"/posts/security/authentication/"},{"categories":["网络"],"content":"客户端缓存，CDN，服务端缓存","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":" 文章内容来自于《凤凰架构》，推荐阅读。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:0:0","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"1 HTTP 客户端缓存 HTTP 设计之初就是确定客户端与服务端 “无状态” 的交互原则，所以不可避免地导致携带重复数据导致网络性能降低。对此，HTTP 协议的解决方案是客户端缓存。 在 HTTP 的演进中，形成了 “状态缓存” “强制缓存” “协商缓存” 的机制。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:1:0","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"1.1 状态缓存 状态缓存 是指不通过服务器，客户端直接根据缓存的信息对目标网站判断。HTTP 301 永久重定向 就是状态缓存。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:1:1","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"1.2 强制缓存 强制缓存 的基本原理很直接：一定时间内，请求的资源内容和状态不一定会改变，因此客户端可以无需发送请求，在时间内一直持有和使用该资源的本地缓存。 根据约定，强制缓存在浏览器的地址输入、页面链接跳转、新开窗口、前进和后退中均可以生效。但是，用户主动刷新页面时应当失效。 HTTP 协议有两类 Header 实现了强制缓存机制：Expires、Cache-Control。 1.2.1 Expires Expires 的值为一个 Deadline，表示：当服务器回复时带有该 Header，意味着服务器承诺该资源在 Deadline 之间不会发生变动，客户端可以直接缓存并使用该回复。 HTTP/1.1 200 OK Expires: Web, 8 Apr 2020 07:28:00 GMT Expires 是最初的 HTTP 缓存机制，其设计非常简单，但是无法处理缓存提前失效等问题。 1.2.2 Cache-Control Cache-Control 语义比 Expires 更加丰富，两者同时存在时，规定必须以 Cache-Control 为准。 HTTP/1.1 200 OK Cache-Control: max-age=60 Cache-Control 在客户端请求 Header 或者服务端回复 Header 中都可以存在，其值可以为一系列参数： max-age 和 s-maxage max-age=\u003cn\u003e - 相对于请求时间 n 秒内，缓存是有效的。 s-maxage=\u003cn\u003e - “共享”缓存的有效时间，即允许被 CDN、代理等持有的缓存时间，由于提示 CDN 这类服务器缓存的有效事件。 public 和 private 指明资源是否设计用户私有资源。如果是 public，可以被代理、CDN 等缓存资源；如果是 private，只能有用户客户端进行缓存。 no-cache 和 no-store no-cache 指该资源不应该被缓存，强制会话相同 URL 资源也必须请求获取，令强制缓存完全失效，但不影响协商缓存。 no-store 不强制会话中相同 URL 资源重复获取，但是禁止浏览器、CDN 等缓存资源。 no-transform 禁止以任何形式修改资源，不允许对 Content-Encoding、Content-Range、Content-Type 任何形式的修改。例如，禁止 CDN、代理自动压缩图片或文本。 min-fresh 和 only-if-cached min-fresh=\u003cn\u003e - 建议服务器返回一个不少于 n 秒的缓存资源。 only-if-cached 表示服务器要求客户端必须使用缓存资源，如果缓存不能名字，则返回 503/Service Unvailable 错误。 must-revalidate 和 proxy-revalidate must-revalidate 表示缓存资源过期后，一定要从服务器获取。 proxy-revalidate 与 must-revalidate 语义相同，专用于提示 CDN、代理。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:1:2","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"1.3 协商缓存 强制缓存是基于时效性，客户端与服务端协商好缓存的时间，不需要进行检查判断缓存是否失效。协商缓存 是基于检查的缓存机制，每次请求会检查缓存是否失效，从而决定是使用缓存资源还是进行请求。 强制缓存与协商缓存不是互斥的 强制缓存与协商缓存不是互斥的，例如当强制缓存时间范围内，直接返回资源不检查；当强制缓存失效，或者被禁止（no-cache/no-store）时，使用协商缓存，先检查后决定。 协商缓存检查变动存在两种方式： 基于资源的修改时间 基于资源的唯一标识 根据约定，协商缓存不仅仅在强制缓存的场景中有效，在用户主动刷新页面（F5）时也有效，只有用户强制刷新（Ctrl+F5）或明确禁用时才失效。 1.3.1 Last-Modified 和 If-Modified-Since Last-Modified 是服务器回复的 Header，用户告知客户端资源的最后修改时间。当客户端再次需要请求该资源时，通过 If-Modified-Since 把收到的最后修改时间发送给服务器。 服务器收到该请求后，发现资源没有修改过，那么返回 304/Not Modified 回复，无须包含资源内容，表示让客户端使用缓存资源。 HTTP/1.1 304 Not Modified Cache-Control: public, max-age=600 Last-Modified: Wed, 8 Apr 2020 15:31:30 GMT 服务器收到该请求后，发现资源变动后，那么返回 200/OK 回复，并且包含完整的资源内容。 HTTP/1.1 200 OK Cache-Control: public, max-age=600 Last-Modified: Wed, 8 Apr 2020 15:31:30 GMT Content 1.3.2 ETag 和 If-None-Match Etag 是服务器回复的 Header，告诉客户端该资源的唯一标识。当客户端再次请求该资源时，会通过 If-None-Match 把收到的唯一标识发送回服务端。 服务器收到该请求后，发现资源的唯一标识与请求中的一致，那么返回 304/Not Modified 回复，无须包含资源内容，表示让客户端使用缓存资源。 HTTP/1.1 304 Not Modified Cache-Control: public, max-age=600 ETag: \"28c3f612-ceb0-4ddc-ae35-791ca840c5fa\" 服务器收到该请求后，发现资源唯一标识有变动，就返回 200/OK 与完成资源内容。 HTTP/1.1 200 OK Cache-Control: public, max-age=600 ETag: \"28c3f612-ceb0-4ddc-ae35-791ca840c5fa\" Content 可以看到，ETag 通过判断资源唯一表示来判断资源是否变动过，其一致性比 Last-Modified 更高（可能资源修改过但是内容每发生变化，这样 Last-Modified 其实是失效的）。 但是，因为每次需要计算资源的唯一标识，因此其性能也是最差的。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:1:3","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"1.4 多版本资源缓存 前面所说的都是针对于 “单个资源” 的缓存，但是一个 URL 可能对应着不同版本的资源。例如资源的不同语言版本，不同的编码方式（Content-Type）等。 对于一个 URL 能够获取多个资源的场景，缓存也需要明确的表示根据什么内容来缓存资源，使用 Vary Header。 HTTP/1.1 200 OK Vary: Accept, User-Agent 以上回复就是告知客户端，根据 Accept 与 User-Agent 来缓存资源，不同 Accept 和 User-Agent 的组合缓存不同的资源。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:1:4","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"2 CDN 内容分发网络（Content Distribution Network，CDN）是将客户端缓存、域名解析等综合运用的方式。CDN 的工作过程，主要涉及到路由解析、内容分发、负载均衡和 CDN 应用四个方面。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:2:0","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"2.1 路由解析 CDN 将用户对某个域名的请求解析到 CDN 的服务器上处理，这个过程就是依靠 DNS 服务器实现的。 将你的域名（icyfenix.cn）在 CDN 服务器上注册为 “源站”，注册后就得到了一个 CNAME（icyfenix.cn.cdn.dnsv1.com）。 处理你的域名的 DNS 服务商上个将得到的 CNAME 注册为一条 CNAME 记录。 用户访问时，DNS 服务商解析出 CNAME 返回给本地 DNS。 本地 DNS 查询 CNAME，CDN 服务商的 DNS 服务器返回合适的 CDN 服务器 IP 地址。 用户访问 CDN 服务器。CDN 服务器如果没有缓存请求的资源，就会请求源站，并缓存。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:2:1","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"2.2 内容分发 可以看到，CDN 服务器透明的接管了用户发出的访问资源，另一个问题就是 CDN 如何获得源站的资源。 CDN 获取源站资源的过程称为 “内容分发”。主要有两种分发方式： 主动分发（Push）- 分发由源站主动发起，将内容从源站提送到各个 CDN 缓存服务器上。 主动分发需要源站调用 CDN 服务的 API，因此对于源站不是透明的。 主动分发一般用于网站要预载大量资源的场景，例如大型活动前缓存网站的资源。 被动回源（Pull）- 被动回源指，当某个资源首次被请求时，CDN 服务器没有缓存该资源，那么就会立即从源站获取。 被动回源中，CDN 服务器相当于普通用户（但是网络特别的快）访问源站，因此对于源站也是透明的。 在 CDN 服务器缓存源站资源后，还需要考虑如何管理（更新）缓存的资源。这个问题目前没有统一的标准可言，目前最常见的做法是 “超时被动失效” 与 “手动主动失效” 相结合。 超时被动失效 - 缓存资源有一定的过期时间，超过时间后 CDN 在下次请求时重新回源一次。 手工主动失效 - CDN 服务商提供缓存失效/更新接口，由源站主动更新缓存资源。例如网站可以在更新时通过 CD 来自动调用接口实现缓存更新。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:2:2","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"2.3 CDN 的应用 CDN 最初是为了快速分发静态资源设计的，而目前 CDN 能够做更多的事。 加速静态资源分发 安全防御：CDN 可以看做网站的堡垒机，源站可以只对 CDN 提供服务，由 CDN 为外界用户提供服务。这样攻击就不容易直接攻击源站。 协议升级：不少 CDN 提供商对阵 SSL 证书服务，这样就可以实现源站基于 HTTP 协议，而对外用户提供 HTTPS 协议。类似，也可以源站提供 HTTP/1.x 协议，CDN 对外提供 HTTP/2 或 HTTP/3 协议等等。 状态缓存：CDN 不仅可以缓存网站资源，还可以缓存源站的 HTTP Status。例如可以通过 CDN 缓存源站的 301/302 来让客户端直接跳转。 修改资源：CDN 可以在返回资源时修改资源的任意内容。例如可以对资源自动压缩并修改 Content-Encoding，以节省带宽。 访问控制：CDN 可以实现 IP 黑/白 名单的功能，根据 IP 来进行流量控制等。 注入功能：通过修改返回资源的内容，CDN 可以在不修改源站的情况下，为源站注入各种功能。 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:2:3","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["网络"],"content":"3 服务端缓存 ","date":"2021-10-11","objectID":"/posts/net/cache-mechanism/:3:0","tags":["网络"],"title":"Cache 机制","uri":"/posts/net/cache-mechanism/"},{"categories":["Kubernetes 实践"],"content":"跨 Kubernetes 集群网络连通","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":" Kubernetes 实践 主要记录一些自己的一些部署实践。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:0:0","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"1 目标 最近在看 TiDB 的 Multi Kubernetes 集群部署方案，TiDB 本身是灵活扩容的分布式数据库，因此部署前需要确保 Multi Kubernetes 之间服务的连通性。 Kubernetes 中，IP 是虚拟以及不持久的，各个服务之间是靠 Domain域名 相互访问的。因此，Multi Kubernetes 之间的连通性关键就在于： 物理网络连通 - 所有方案最基本的，就是底层的网络可以联通。 DNS 解析 - 因为服务之间靠 Domain 访问，因此访问另一个集群的服务时，DNS 要能够解析为另一个集群的 Pod IP。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:1:0","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"2 物理网络连通 Multi Kubernetes 之间物理网络的连通不同场景有着不同的方案。 使用 EKS 时，不同 VPC 下的 EKS 集群都过 VPC Peering 连接，实现物理网络的连通性。 使用 GKE 时，GKE 集群之间网络就是联通的。 其他自己部署的 Multi Kubernetes 集群，可能需要 Tunnel 来构建物理网络。可以参考下项目 submariner-io/submariner。 物理网络连通的最终的效果就是，一个 Kubernetes 集群的 Pod，是能够 “访问” 到另一个 Kubernetes 集群的 Pod 的。 Note 最理想的网络互通，应该是多个 Kubernetes 集群处于同一个网络，不过这应该就是一个 Kubernetes 集群了，而不是两个了。 在这之下，两个 Kubernetes 集群之间能够 “直接”（不需要通过 NodePort 映射）相互访问。就像两个不同的网段，中间有路由器来连通这两个网段。 而这个 “路由器” 有着多种的实现了：例如最直接的，Cluster1 网络 -\u003e Node1 -\u003e Node2 -\u003e Cluster2 网络，中间通过 NodePort 来转发。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:2:0","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"3 DNS ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:3:0","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"3.1 DNS 解析流程 无论是 Pod DNS 还是 Service DNS，其解析都由 CoreDNS 负责。所以一个 Cluster 要访问另一个 Cluster 的服务时，其期望的步骤为： Cluster1 Pod -\u003e Cluster1 CoreDNS Cluster1 Pod 发送 DNS 请求到 Cluster1 CoreDNS。 Cluster1 CoreDNS -\u003e Cluster2 CoreDNS Cluster1 CoreDNS 判断请求的 Domain 是 Cluster2 的，因此转发给 Cluster2 CoreDNS 解析。 因为物理网络是连通的，所以转发是可以成功的。 Cluster2 CoreDNS -\u003e Cluster1 CoreDNS Cluster2 CoreDNS 接受到请求后，解析 Domain，回复对应 Cluster2 Pod IP 给 Cluster1 CoreDNS。 因为物理网络是连通的，所以回复是可以成功的。 Cluster1 CoreDNS -\u003e Cluster1 Pod Cluster1 CoreDNS 将解析结果回复给 Cluster1 Pod。 Cluster1 Pod -\u003e Cluster2 Pod 因为物理网络是连通的，所以访问是可以成功的。 当然，我们 CoreDNS 能够包含所有 Cluster 的记录，那么可能可以加速解析的流程。目前不清楚可不可以这样配置，或者有没有这样的实现。 Note 注意，上述中直接访问跨 Kubernetes 访问 Service 是可能无法联通的，猜想两种情况： Service IP 与 Pod 网络不是一个平面的，那么如果 Cluster1 Pod 发出的包无法被路由到 Cluster2 中（除非配置好路由规则，例如 AWS GCP 的 Route）。 Service IP 与 Pod 网络是一个平面的，Cluster1 Pod 发出的数据包能够路由到 Cluster2 中，由 Cluster2 中的 kube-proxy 根据 Service IP 成功转发了。 所以，问题的关键还是在于数据包能否路由到 Cluster2 网络中。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:3:1","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"3.2 配置 CoreDNS 基于上面的流程，有两个关键的问题： 如何判断请求的 Domain 是 Cluster2 的？ 如何配置 CoreDNS 转发请求？ 这些都需要通过 CoreDNS 配置来实现，先看参考文档 Customizing DNS Service 知晓如何配置 CoreDNS。 从 K8s 学习 - 2 - Service 我们知晓，Kubernetes 中的 Domain 分为 Pod 与 Service。 Pod Domain \u003cpod_ip\u003e.\u003cnamespace\u003e.pod.\u003cclust-domain\u003e \u003cpod_ip\u003e.\u003cdepolyment/daemonset_name\u003e.svc.\u003ccluster_domain\u003e \u003chostname\u003e.\u003csubdomain\u003e.\u003cnamespace\u003e.svc.\u003ccluster_domain\u003e Service Domain \u003cservice\u003e.\u003cnamespace\u003e.svc.\u003ccluster_domain\u003e 因为 DNS 解析是将 Domain 从后向前按照域解析的，所以最简单的方式就是：将 Multi Kubernetes 的 Cluster Domain 设置为不同的，然后配置 CoreDNS 转发。 apiVersion:v1kind:ConfigMapmetadata:name:corednsnamespace:kube-systemdata:Corefile:|.:53 { # default.. } \u003ctarget-cluster-domain\u003e:53 { # \u003c-- 指向对端的 Kubernetes Cluster Domain errors cache 30 forward . \u003ctarget-coredns-ip\u003e } 除了自己集群的域名解析外，对于 “\u003ctarget-cluster-domain\u003e” 的域名解析都转发给了对应集群的 CoreDNS。 改变 Kubernetes 集群的 Cluster Domain 方式：部署 Cluster 时，通过 kubelet 参数 -cluster-domain=\u003cdefault-local-domain\u003e 配置。 不过，改变 Cluster Domain 方式需要在部署 Cluster 时配置，如果你已经部署了 Cluster，这可能就不太方便了。这种情况下，Multi Kubernetes Cluster Domain 都是 “cluster.local”，所以我们要在更前面的域上来区分，自然是 “\u003cnamespace\u003e\"。 例如，你的 Pod 要访问另一个 Cluster 的 Pod： apiVersion:v1kind:ConfigMapmetadata:name:corednsnamespace:kube-systemdata:Corefile:|.:53 { # default.. } \u003ctarget-namespace\u003e.svc.cluster.local:53 { # \u003c-- 指向对端 Kubernetes 服务的 namespace errors cache 30 forward . \u003ctarget-coredns-ip\u003e } 可以想到，这种访问匹配的范围更小，更重要的是，这使得 Multi Cluster 之间的 namespace 不应该重复。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:3:2","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"4 AWS 上的实践 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:4:0","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"4.1 创建 EKS 集群 先建立两个 EKS 集群，需要注意两个集群 VPC CIDR 不要不覆盖。 $ eksctl create cluster --name test-cluster --with-oidc --ssh-access --managed --region us-west-2 --instance-types m5.xlarge --nodes 3 $ eksctl create cluster --name test-cluster-2 --with-oidc --ssh-access --managed --region us-west-2 --instance-types m5.xlarge --nodes 3 --vpc-cidr 10.0.0.0/16 查询下两个 EKS 集群信息，后面需要用到： $ eksctl get cluster test-cluster NAME VERSION STATUS CREATED VPC SUBNETS SECURITYGROUPS test-cluster 1.20 ACTIVE 2021-09-30T11:53:19Z vpc-0409bd99d5e2b6f5b subnet-00d1a0abdaf60021b,subnet-0a9fca73d25257d90,subnet-0aee90db41909f538,subnet-0c81d647b53fe567a,subnet-0dcd3f3da03b5e767,subnet-0f80939acbdda6357 sg-00985dc11204cfeda $ eksctl get cluster test-cluster-2 NAME VERSION STATUS CREATED VPC SUBNETS SECURITYGROUPS test-cluster-2 1.20 ACTIVE 2021-09-30T12:19:39Z vpc-0e35ec388cb70e7fc subnet-012fa5481e326aafc,subnet-0586d236d85ed6893,subnet-0d5695aa8970fea2d,subnet-0d61b543c82d94d14,subnet-0d740e2366cf2778f,subnet-0def1a48342d72d32 sg-0d914515ffd1173b5 我们创建了同 Region 下不同 VPC 两个 EKS 集群，不同 Region 下后续操作也是一样的。 后续操作中，各个资源的 1、2 分别代表对应集群的资源。k1 与 k2 分别代表对应集群的 kubectl 操作（通过 –kubeconfig 参数）。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:4:1","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"4.2 构建网络 EKS 中 Pod 是 Host Network，与 Node 在同一个网络中。AWS VPC 中各个 Subnet 网络是互通的，因此我们只需要构建 VPC 之间的网络互通。 需要创建与配置以下 AWS 资源： VPC Peering - 连接 VPC 之间的网络 Route Table - 能够将流量正确的转发到另一个 VPC ACL - 流量到达 Subnet 能够通过 ACL Security Group - 流量到达每个 Instance 能够通过 Security Group Note 要使用 VPC Peering 连通 VPC，这些资源都是需要配置的。 4.2.1 创建 VPC Peering Create VPC Peering，构建 VPC1 连通 VPC2。 $ aws ec2 create-vpc-peering-connection \\ --vpc-id vpc-0409bd99d5e2b6f5b \\ --peer-vpc-id vpc-0e35ec388cb70e7fc { \"VpcPeeringConnection\": { \"AccepterVpcInfo\": { \"OwnerId\": \"385595570414\", \"VpcId\": \"vpc-0e35ec388cb70e7fc\", \"Region\": \"us-west-2\" }, \"ExpirationTime\": \"2021-10-07T12:53:36+00:00\", \"RequesterVpcInfo\": { \"CidrBlock\": \"192.168.0.0/16\", \"CidrBlockSet\": [ { \"CidrBlock\": \"192.168.0.0/16\" } ], \"OwnerId\": \"385595570414\", \"PeeringOptions\": { \"AllowDnsResolutionFromRemoteVpc\": false, \"AllowEgressFromLocalClassicLinkToRemoteVpc\": false, \"AllowEgressFromLocalVpcToRemoteClassicLink\": false }, \"VpcId\": \"vpc-0409bd99d5e2b6f5b\", \"Region\": \"us-west-2\" }, \"Status\": { \"Code\": \"pending-acceptance\", \"Message\": \"Pending Acceptance by 385595570414\" }, \"Tags\": [], \"VpcPeeringConnectionId\": \"pcx-094a1149b32ef0c05\" } } Accept VPC Peering，完成 VPC Peering 构建。 $ aws ec2 accept-vpc-peering-connection \\ --vpc-peering-connection-id pcx-094a1149b32ef0c05 { \"VpcPeeringConnection\": { \"AccepterVpcInfo\": { \"CidrBlock\": \"10.0.0.0/16\", \"CidrBlockSet\": [ { \"CidrBlock\": \"10.0.0.0/16\" } ], \"OwnerId\": \"385595570414\", \"PeeringOptions\": { \"AllowDnsResolutionFromRemoteVpc\": false, \"AllowEgressFromLocalClassicLinkToRemoteVpc\": false, \"AllowEgressFromLocalVpcToRemoteClassicLink\": false }, \"VpcId\": \"vpc-0e35ec388cb70e7fc\", \"Region\": \"us-west-2\" }, \"RequesterVpcInfo\": { \"CidrBlock\": \"192.168.0.0/16\", \"CidrBlockSet\": [ { \"CidrBlock\": \"192.168.0.0/16\" } ], \"OwnerId\": \"385595570414\", \"PeeringOptions\": { \"AllowDnsResolutionFromRemoteVpc\": false, \"AllowEgressFromLocalClassicLinkToRemoteVpc\": false, \"AllowEgressFromLocalVpcToRemoteClassicLink\": false }, \"VpcId\": \"vpc-0409bd99d5e2b6f5b\", \"Region\": \"us-west-2\" }, \"Status\": { \"Code\": \"provisioning\", \"Message\": \"Provisioning\" }, \"Tags\": [], \"VpcPeeringConnectionId\": \"pcx-094a1149b32ef0c05\" } } 4.2.2 Route Table 添加 Route 项 为了 VPC 之间的连通性，需要向各个 VPC 的 Route Table 添加 Route 项，转发数据包到 VPC Peering。 先查询两个 VPC 对应的 Route Table ID。 $ aws ec2 describe-route-tables --filters Name=vpc-id,Values=vpc-0409bd99d5e2b6f5b $ aws ec2 describe-route-tables --filters Name=vpc-id,Values=vpc-0e35ec388cb70e7fc 添加 Route 项，匹配另一个集群网段，路由到 VPC Peering。 $ aws ec2 create-route \\ --route-table-id rtb-058ae03476f0eb62d \\ --destination-cidr-block 10.0.0.0/16 \\ --vpc-peering-connection-id pcx-094a1149b32ef0c05 { \"Return\": true } $ aws ec2 create-route \\ --route-table-id rtb-03e1023998c139fea \\ --destination-cidr-block 192.168.0.0/16 \\ --vpc-peering-connection-id pcx-094a1149b32ef0c05 { \"Return\": true } 4.2.3 配置 ACL 与 Security Group 接着配置 ACL 与 Security Group，让数据包不被防火墙所拦截。 eksctl 创建 VPC 时没有配置 ACL，因此默认的 ACL 允许所有的进出流量，所以不需要配置。 Instance 的默认 Security Group 是禁止所有的，eksctl 添加了 Node 节点的白名单，我们需要加入另一个 Cluster 网段的 Rule。 EKS 会让其下 Instance 共用一个 Security Group（eksctl get cluster 时得到），所以我们只需要配置一个 Security Group 即可。 $ aws ec2 authorize-security-group-ingress \\ --group-id sg-01cde04b3896f32df \\ --protocol all \\ --cidr 10.0.0.0/16 $ aws ec2 authorize-security-group-ingress \\ --group-id sg-06cbf88ed5d56bd18 \\ --protocol all \\ --cidr 192.168.0.0/16 Note 这里的 Security Group ID 部署 eksctl get cluster 命令得到的，eksctl get cluster 得到是控制面的 Security Group。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:4:2","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"4.3 Kubernetes DNS 配置 到这里，我们的物理连通性构建好了，接下来就是对于 Cluster 中的 DNS 配置了。因为 EKS 集群部署没有修改 Cluster Domain 的方式（也可能是我没找到），如 3.2 配置 CoreDNS 所说，我们使用 namespace 来区分 Domain。 我们先要知道 Cluster1 与 Cluster2 中各个 CoreDNS Pod 的 IP，这样才能配置 CoreDNS 转发。 $ k1 get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coredns-86d9946576-72gcv 1/1 Running 0 112m 192.168.95.166 ip-192-168-85-158.us-west-2.compute.internal \u003cnone\u003e \u003cnone\u003e coredns-86d9946576-tk9nk 1/1 Running 0 112m 192.168.4.66 ip-192-168-24-153.us-west-2.compute.internal \u003cnone\u003e \u003cnone\u003e $ k2 get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coredns-86d9946576-79zvs 1/1 Running 0 87m 10.0.76.0 ip-10-0-77-8.us-west-2.compute.internal \u003cnone\u003e \u003cnone\u003e coredns-86d9946576-tqcbh 1/1 Running 0 87m 10.0.91.141 ip-10-0-77-8.us-west-2.compute.internal \u003cnone\u003e \u003cnone\u003e 接着配置 CoreDNS1 与 CoreDNS2，通过对应的 ConfigMap 来配置 Corefile。配置后，CoreDNS 会自动重新加载配置。 $ k1 edit -n kube-system cm corednsapiVersion:v1kind:ConfigMap# ...data:Corefile:|.:53 { # default... } cluster-2.svc.cluster.local:53 { log errors cache 30 forward . 10.0.76.0 10.0.91.141 }$ k2 edit -n kube-system cm corednsapiVersion:v1kind:ConfigMap# ...data:Corefile:|.:53 { # default... } cluster-1.svc.cluster.local:53 { log errors cache 30 forward . 10.0.76.0 10.0.91.141 } 这配置表明了，在创建服务时部署在 Cluster1 的 “cluster-1” namespace 与 Cluster2 的 “cluster-2” namespace 时，域名解析才能正常。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:4:3","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"4.4 测试 OK，一切都完成了，我们来简单的测试下。我们在 Cluster1 部署一个 busybox Pod，用于等会访问 Cluster2 Service 使用。 $ k1 apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/master/hack/testdata/recursive/pod/pod/busybox.yaml 在 Cluster2 部署一个 Service，Service 将消息转发到百度。需要注意，一定要部署在 “cluster-2” namespace。 apiVersion:v1kind:Servicemetadata:name:proxy-to-baidunamespace:cluster-2spec:ports:- name:baiduport:8888protocol:TCPtype:ExternalNameexternalName:www.baidu.com# \u003c-- 转发 测试下域名解析，可以看到域名解析到了百度的域名了，说明跨 Kubernetes 联通了！ $ k1 exec busybox1 -- nslookup proxy-to-baidu.cluster-2.svc.cluster.local Server: 10.100.0.10 Address: 10.100.0.10:53 proxy-to-baidu.cluster-2.svc.cluster.local canonical name = www.baidu.com www.baidu.com canonical name = www.a.shifen.com www.a.shifen.com canonical name = www.wshifen.com ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:4:4","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"5 GCP 上的实践 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:5:0","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"5.1 创建 VPC Network GCP 与 AWS 不同，其 VPC 是一个虚拟的全球的概念，其下的 Subnet 是天然的跨 Region 联通的。因此，物理网络我们不需要过多的配置使其联通。 先创建一个自定义子网的 VPC： $ gcloud compute networks create ${network_name} --subnet-mode=custom 在新创建的 VPC 网络下创建两个个 Subnet，注意各个 Subnet 的 IP 地址范围不能相互重叠。每个 Subnet 还包含了后续 Kubernetes 要使用的 Pod 与 Service 的 IP 地址范围。 $ gcloud compute networks subnets create ${subnet_1} \\ --region=${region_1} \\ --network=${network_name} \\ --range=10.0.0.0/16 \\ --secondary-range pods=10.10.0.0/16,services=10.100.0.0/16 $ gcloud compute networks subnets create ${subnet_1} \\ --region=${region_1} \\ --network=${network_name} \\ --range=10.2.0.0/16 \\ --secondary-range pods=10.12.0.0/16,services=10.102.0.0/16 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:5:1","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"5.2 创建 GKE 集群 创建两个 Region 级别的 GKE 集群： $ gcloud beta container clusters create ${cluster_1} \\ --region ${region_1} --num-nodes 1 \\ --network ${network_name} --subnetwork ${subnet_1} \\ --cluster-dns clouddns --cluster-dns-scope vpc \\ --cluster-dns-domain ${cluster_domain_1} --enable-ip-alias \\ --cluster-secondary-range-name=pods --services-secondary-range-name=services $ gcloud beta container clusters create ${cluster_2} \\ --region ${region_2} --num-nodes 1 \\ --network ${network_name} --subnetwork ${subnet_2} \\ --cluster-dns clouddns --cluster-dns-scope vpc \\ --cluster-dns-domain ${cluster_domain_2} --enable-ip-alias \\ --cluster-secondary-range-name=pods --services-secondary-range-name=services 一些重要的参数： --network 与 --subnetwork 指定了加入的 Subnet； -cluster-dns clouddns 和 --cluster-dns-scope vpc 参数使用了 Cloud DNS 服务，以实现跨 Kubernetes 的 DNS 解析； --cluster-dns-domain ${cluster_domain_1} 定义了域名解析使用的 Cluster Domain，也就是我们需要为 Pod 配置的； --cluster-secondary-range-name=pods --services-secondary-range-name=services 指定了 Pod 与 Service 使用的 IP 地址范围； ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:5:2","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"5.3 配置 Firewall 最后一步就是配置 GKE 集群节点应用的 Firewall。默认情况，创建 GKE 集群会创建默认本集群 Pod 访问的 Firewall（见文档 自动创建的防火墙规则），因此集群见 Pod 相互访问是不允许的。 先找到集群用于 Pod 见通信的 Firewall rules。 $ gcloud compute firewall-rules list --filter='name~gke-${cluster_1}-.*-all' NAME NETWORK DIRECTION PRIORITY ALLOW DENY DISABLED gke-${cluster_1}-b8b48366-all ${network} INGRESS 1000 tcp,udp,icmp,esp,ah,sctp False 更新该防火墙规则，设置 source range 为所有集群的 Pod 网络的 IP 地址范围： $ gcloud compute firewall-rules update gke-${cluster_1}-b8b48366-all --source-ranges 10.10.0.0/16,10.11.0.0/16 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:5:3","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"6 Kind 上的实践 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:6:0","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"6.1 创建 Kubernetes 集群 编写 Kind Cluster 配置文件。因为 Kind 支持配置 kubeadm 的配置，所以可以设置两个集群的 cluster domain。 $ cat cluster-1.yamlkind:ClusterapiVersion:kind.x-k8s.io/v1alpha4networking:podSubnet:10.10.0.0/16serviceSubnet:10.40.0.0/16nodes:- role:control-plane- role:worker- role:workerkubeadmConfigPatches:- |kind: ClusterConfiguration networking: dnsDomain: \"cluster-1.com\"$ cat cluster-2.yamlkind:ClusterapiVersion:kind.x-k8s.io/v1alpha4networking:podSubnet:10.20.0.0/16serviceSubnet:10.50.0.0/16nodes:- role:control-plane- role:worker- role:workerkubeadmConfigPatches:- |kind: ClusterConfiguration networking: dnsDomain: \"cluster-2.com\" 创建两个集群 cluster-1 cluster-2。 $ kind create cluster --config cluster-1.yaml --name cluster-1 $ kind create cluster --config cluster-2.yaml --name cluster-2 WARN 操作时，发现如果 network.podSubnet 设置为 10.0.0.0/16 之外的网段，Kind 的 CNI 无法部署成功。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:6:1","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"6.2 构建网络 整个网络数据包的流转为：Pod A -\u003e Node A -\u003e Node B -\u003e Pod B。其中 Pod -\u003e Node 之间的网络是连通的，所以我们需要解决的关键问题就是 Node A -\u003e Node B 的数据包转发。 Kind 创建的多个集群都是在一个 Docker Bridge Network，因此 Node 之间的网络是天然联通的。 因此，我们只需要在 Node 上添加路由项，使之能够正确转发发往对方集群的数据包。我们为所有 Node 添加指向另一个集群下的 Node 的路由，路由基于 Node 的 Pod CIDR。 # 找到 每个 node 的 IP 与 PodCIDR $ kubectl --context kind-cluster-1 get node -ojsonpath='{range .items[*]} {.metadata.name}{\"\\t\"} {.spec.podCIDR}{\"\\t\"} {.status.addresses[0].address}{\"\\n\"} {end}' cluster-1-control-plane 10.10.0.0/24 172.19.0.2 cluster-1-worker 10.10.2.0/24 172.19.0.4 cluster-1-worker2 10.10.1.0/24 172.19.0.3 $ kubectl --context kind-cluster-2 get node -ojsonpath='{range .items[*]} {.metadata.name}{\"\\t\"} {.spec.podCIDR}{\"\\t\"} {.status.addresses[0].address}{\"\\n\"} {end}' cluster-2-control-plane 10.20.0.0/24 172.19.0.7 cluster-2-worker 10.20.2.0/24 172.19.0.5 cluster-2-worker2 10.20.1.0/24 172.19.0.6 # 给节点添加路由项 $ for node in cluster-1-control-plane cluster-1-worker cluster-1-worker2 ; do docker exec ${node} ip route add 10.20.0.0/24 via 172.19.0.7; done $ for node in cluster-1-control-plane cluster-1-worker cluster-1-worker2 ; do docker exec ${node} ip route add 10.20.2.0/24 via 172.19.0.5; done $ for node in cluster-1-control-plane cluster-1-worker cluster-1-worker2 ; do docker exec ${node} ip route add 10.20.1.0/24 via 172.19.0.6; done $ for node in cluster-2-control-plane cluster-2-worker cluster-2-worker2 ; do docker exec ${node} ip route add 10.10.0.0/24 via 172.19.0.2; done $ for node in cluster-2-control-plane cluster-2-worker cluster-2-worker2 ; do docker exec ${node} ip route add 10.10.2.0/24 via 172.19.0.4; done $ for node in cluster-2-control-plane cluster-2-worker cluster-2-worker2 ; do docker exec ${node} ip route add 10.10.1.0/24 via 172.19.0.3; done Note 这里尝试过给宿主机上的 kind 的 Bridge 网卡添加路由来转发，而不用为每个 Node 设置路由。但是，这样设置好后发现 UDP ICMP 数据包是可以互通的，但是 TCP 连接无法成功。还不清楚原因。 下图总结了目前构建的网络： Pod A 发送数据包，走 default 路由，由 Bridge 网卡处理。 根据路由，将数据包转发给 Control Plane Node。 Control Plane Node 转发给 Node A，后发送到 Pod A。 Note Kind 网络中，某个集群 Pod 访问其他集群时，数据包会被 Node 进行 SNAT。因此，在某些需要使用源 IP（例如证书 host 配置）时，需要注意。 ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:6:2","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"6.3 Kubernetes DNS 配置 DNS 配置与 AWS 中的 DNS 配置类似，不过因为设置了两个集群的 cluster domain，所以可以直接使用 cluster domain 配置 CoreDNS。 先看两个集群的 CoreDNS Pod IP。 $ kubectl --context kind-cluster-1 get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coredns-558bd4d5db-7kjdt 1/1 Running 0 65m 10.10.0.3 cluster-1-control-plane \u003cnone\u003e \u003cnone\u003e coredns-558bd4d5db-lb46f 1/1 Running 0 65m 10.10.0.2 cluster-1-control-plane \u003cnone\u003e \u003cnone\u003e $ kubectl --context kind-cluster-2 get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coredns-558bd4d5db-b8c46 1/1 Running 0 64m 10.20.0.2 cluster-2-control-plane \u003cnone\u003e \u003cnone\u003e coredns-558bd4d5db-w9p8g 1/1 Running 0 64m 10.20.0.3 cluster-2-control-plane \u003cnone\u003e \u003cnone\u003e 接着配置 CoreDNS1 与 CoreDNS2，通过对应的 ConfigMap 来配置 Corefile。配置后，CoreDNS 会自动重新加载配置。 $ kubectl --context kind-cluster-1 edit -n kube-system cm corednsapiVersion:v1kind:ConfigMap# ...data:Corefile:|.:53 { # default... } cluster-2.com:53 { errors cache 30 forward . 10.20.0.2 10.20.0.3 # -\u003e 对方集群的 CoreDNS Pod IP } $ kubectl --context kind-cluster-2 edit -n kube-system cm corednsapiVersion:v1kind:ConfigMap# ...data:Corefile:|.:53 { # default... } cluster-1.com:53 { errors cache 30 forward . 10.10.0.2 10.10.0.3 # -\u003e 对方集群的 CoreDNS Pod IP } ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:6:3","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 实践"],"content":"参考 Doc: Customizing DNS Service Doc: DNS for Services and Pods Orchestrate CockroachDB Across Multiple Kubernetes Clusters ","date":"2021-09-30","objectID":"/posts/cloud_computing/k8s_practice/across-kubernetes/:7:0","tags":["k8s","云计算"],"title":"K8s 实践 - Network Across Cluster","uri":"/posts/cloud_computing/k8s_practice/across-kubernetes/"},{"categories":["Kubernetes 编程"],"content":"kube-scheduler 负责集群资源的调度，原生的 Kubernetes Scheduler 可以满足大多数情况的需求。不过针对于不同业务的特殊的调度需求，Kubernetes 提供了多种扩展调度器的方式。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:0:0","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1 Scheduler 扩展点 Scheduler Framework 在整个调度过程中有着一系列的扩展点，支持用户以 Plugin 的方式进行扩展。 整个的调度流程中的扩展点如下图： 所有的扩展点都是由 Plugin 实现的，允许配置开启或关闭，也可以添加自定义的 Plugin。这在后续的 KubeSchedulerConfiguration 中深入说明。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:0","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.1 QueueSort 该插件用于对 scheduling queue 中的 Pod 进行排序，一次只能存在一个 QueueSort Plugin。 一个 QueueSort Plugin 本质上就是提供了一个 Less(Pod1, Pod2) 函数。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:1","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.2 PreFilter 该插件用于对 Pod 的信息进行预处理，或者检查一些集群或 Pod 必须满足的前提条件。 如果 PreFilter Plugin 返回一个 error，那么调度流程直接终止。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:2","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.3 Filter 该插件用于过滤不能运行 Pod 的 Node。 对于每一个 Node，调度器将按照顺序执行 Filter Plugin。如果任意一个 Filter Plugin 将 Node 标记为不可行，那么剩下的 Filter Plugin 就不会再运行。 所有 Node 会并行进行 Filter。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:3","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.4 PostFilter 这些插件在 Filter 阶段之后被调用，只有在没有为 Pod 找到可运行的 Node 时才会运行。所有插件按照顺序运行。 如果任意一个 PostFilter Plugin 将 Node 标记为 Schedulable，剩下的 PostFilter Plugin 就不会再运行。 一个典型的 PostFilter Plugin 是抢占，它通过抢占其他 Pod 来调度 Pod。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:4","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.5 PreScore 这些插件用于执行 “预打分” 工作，为 Score Plugin 生成一个可共享的状态。 如果 PreScore 插件返回 error，调度流程终止。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:5","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.6 Score 这些插件用于为所有可选 Node 打分，调度器会为每个 Node 调用每个 Score Plugin。 打分有一个明确定义的整数范围，代表最低和最高分数。 在 NormalizeScore 阶段后，Scheduler 将根据配置的插件权重，合并所有插件的打分。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:6","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.7 NormalizeScore 这些插件用于在最终排序 Node 前修改每个 Node 的评分结果。 当 NormalizeScore Plugin 被调用时，将获得同一个 Plugin 中打分结果。在每个调度周期后，每个插件会被调用一次。 例如，支持 Plugin BlinkingLightScorer 根据每个 Node 有多少个 blinking lights 进行排名。 func ScoreNode(_ *v1.pod, n *v1.Node) (int, error) { return getBlinkingLightCount(n) } 但是，与 NodeScoreMax 相比，blinking lights 数量也可能很少。为解决这个问题，BlinkingLightScorer 也注册了这个扩展点。 func NormalizeScores(scores map[string]int) { highest := 0 for _, score := range scores { highest = max(highest, score) } for node, score := range scores { scores[node] = score*NodeScoreMax/highest } } 如果任何 NormalizeScore Plugin 返回 error，调度将终止。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:7","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.8 Reserve Reserve Plugin 有两个方法：Reserve 和 Unreserve，会带有 Reserve 和 Unreserve 调度阶段的信息。有状态的插件可以使用该扩展点来获得 Node 上为 Pod 预留的资源。 该事件发生在调度器将 Pod 绑定到 Node 之前，目的是避免调度器在等 Pod 与 Node 绑定过程中，有新的 Pod 调度到该 Node 上，发生实际使用资源超过可用资源的情况。（因为绑定 Pod 是异步的） 这是调度过程的最后一个步骤，Pod 进入 reserved 状态后，要么绑定失败触发 Unreserve，要么由 Postbind 扩展结束绑定过程。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:8","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.9 Permit Permit Plugin 在调度周期的最后调用，为了组织或延迟绑定后候选的 Node。 Permit Plugin 做以下的事情： approve 一旦所有 Permit Plugin approve 一个 Pod，那么就会进行绑定操作。 deny 如果任一 Permit Plugin deny 一个 POod，那么将其重新放入调度队列。这也会触发 Unreserve 阶段。 wait（with a timeout） 如果一个 Permit Plugin 返回 wait，那么该 Pod 会在 waiting Pod list 中等待，直到该 Pod 被 approve。 如果 wait timeout 触发，那么变为 deny Pod。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:9","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.10 PreBind 这些插件用于在 Pod 被绑定前执行一些工作。 例如，PreBind Plugin 可以创建 network volume，并将其绑定到目标 Node。 如果任一 PreBind Plugin 返回 error，Pod 会被拒绝，然后重新放入调度队列。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:10","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.11 Bind 这些插件用于将 Pod 绑定到 Node。 每个 Bind Plugin 会按照配置顺序调用，每个 Bind Plugin 可以选择是否处理该 Pod。如果某个 Bind Plugin 决定处理该 Pod，后续的 Bind Plugin 会跳过。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:11","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"1.12 PostBind Post-bind Plugin 在绑定成功后被调用，是绑定周期的最后一个阶段。常常用于清理相关资源。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:1:12","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"2 Scheduling Policy Scheduling Policy 是一种老版本配置调度器的方式，可以定制 Predicates 与 Priorities 使用的插件，也可以通过 Extender 对阶段进行扩展。 Scheduling Policy 可以为 yaml 或者 json 格式，一个示例如下： { \"kind\" : \"Policy\", \"apiVersion\" : \"v1\", \"predicates\": [ {\"name\": \"NoVolumeZoneConflict\"}, {\"name\": \"MaxEBSVolumeCount\"}, {\"name\": \"MaxAzureDiskVolumeCount\"}, {\"name\": \"NoDiskConflict\"}, {\"name\": \"GeneralPredicates\"}, {\"name\": \"PodToleratesNodeTaints\"}, {\"name\": \"CheckVolumeBinding\"}, {\"name\": \"MaxGCEPDVolumeCount\"}, {\"name\": \"MatchInterPodAffinity\"}, ], \"priorities\": [ {\"name\": \"SelectorSpreadPriority\", \"weight\": 1}, {\"name\": \"InterPodAffinityPriority\", \"weight\": 1}, {\"name\": \"LeastRequestedPriority\", \"weight\": 1}, {\"name\": \"BalancedResourceAllocation\", \"weight\": 1}, {\"name\": \"NodePreferAvoidPodsPriority\", \"weight\": 1}, {\"name\": \"NodeAffinityPriority\", \"weight\": 1}, {\"name\": \"TaintTolerationPriority\", \"weight\": 1} ], \"extenders\": [ { \"urlPrefix\": \"http://127.0.0.1:10262/scheduler\", \"filterVerb\": \"filter\", \"preemptVerb\": \"preempt\", \"weight\": 1, \"httpTimeout\": 30000000000, \"enableHttps\": false } ] } predicates 定制 Predicate 阶段使用的插件； priorities 定制 Priority 阶段使用的插件； extenders 定制扩展的 Webhook，调度器会在对应的阶段调用对应的 URL； 运行 kube-scheduler 时，我们可以通过 kube-scheduler --policy-config-file \u003cfilename\u003e 指定一个 Policy 文件，或者通过 kube-scheduler --policy-configmap \u003cConfigMap\u003e ----policy-configmap-namespace \u003cNamespace\u003e 指定一个 ConfigMap 获取 Policy。加上 1.22 之前 kube-scheduler 支持的 --scheduler-name 参数，很简单的定义出一个新的调度器。 $ kube-scheduler --port=10261 --leader-elect=true --lock-object-namespace=pingcap --lock-object-name=my-scheduler --scheduler-name=my-scheduler --policy-configmap=my-scheduler-policy --policy-configmap-namespace=pingcap 使用上述命令，我们运行了一个名为 my-scheduler 的调度器，通过 Pod 定义的 spec.schedulerName 可以指定使用该调度器。 Scheduling Policy 是旧版本的 1.22 后，kube-scheduler 已经不支持 --scheduler-name 参数了，因此已经无法使用 Scheduler Policy 来得到一个新的调度器了。 新版多调度器只能通过 Multiple Profiles 方式实现，而其与 Scheduler Policy 是不兼容的。所以如果你要考虑运行一个新的调度器，应该直接使用 Scheduling Profiles 方式。 当然，Schedule Policy 还是能够实现定制 default scheduler 的作用，不过 Scheduling Profiles 能实现的更多。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:2:0","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"3 Scheduling Profiles 无论哪种方式扩展调度器，都是围绕着通过 Scheduling Profiles 对 kube-scheduler 进行配置实现的。 运行 kube-scheduler 时，通过 kube-scheduler --config \u003cfilename\u003e 指定 Scheduling Profiles。其文件内容是 v1beta1 或者 v1beta2 版本的 KubeSchedulerConfiguration 结构。 最简单的配置如下： apiVersion:kubescheduler.config.k8s.io/v1beta2kind:KubeSchedulerConfigurationclientConnection:kubeconfig:/etc/srv/kubernetes/kube-scheduler/kubeconfig ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:3:0","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"3.1 配置 Plugin 在 KubeSchedulerConfiguration 定义中的 profiles 字段对 kube-scheduler 的插件进行配置。配置方式按照 Scheduler 扩展点 中所属的扩展点。 对于每个扩展点，你可以关闭某些默认开启的 Plugin，或者打开你自己定义的 Plugin。 例如下面示例中，关闭了 Score 扩展点的 NodeResourcesLeastAllocated Plugin，打开 Score 扩展点的 MyCustomPluginA Plugin 与 MyCustomPluginB Plugin。 apiVersion:kubescheduler.config.k8s.io/v1beta2kind:KubeSchedulerConfigurationprofiles:- plugins:score:disabled:- name:NodeResourcesLeastAllocatedenabled:- name:MyCustomPluginAweight:2- name:MyCustomPluginBweight:1 profiles[].plugins.score - 表明配置 score 扩展点的插件。 disabled - 关闭插件 enabled - 打开插件 插件是编译到程序中的 在 profiles.plugins 字段中配置的插件都是预编译到 scheduler 代码中。也就是说，你需要先通过 scheduler framework 编写自定义 Plugin 代码，编译成 scheduler 镜像，然后才能打开你定义的插件。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:3:1","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"3.2 Multiple Profiles 可以通过 profiles 字段配置多个 Profile，每个 Profile 就类似与一个 Scheduler 的 “分身”，可以进行不同扩展点 Plugin 的配置，并且有着独立的 schedulerName。 如下配置中，将运行两个 Scheduler：default-scheduler 运行所有默认 Plugin，no-scoring-scheduler 关闭了所有的 Score Plugin。 apiVersion:kubescheduler.config.k8s.io/v1beta2kind:KubeSchedulerConfigurationprofiles:- schedulerName:default-scheduler- schedulerName:no-scoring-schedulerplugins:preScore:disabled:- name:'*'score:disabled:- name:'*' 多个调度器运行时，通过 Pod 定义中的 .spec.schedulerName 字段来指定期望的调度器。 默认下，一个 Profile 的 scheduler name 为 “default-scheduler”，也就是 Kubernetes 中默认的调度器名字（Pod 没有指定 .spec.schedulerName 字段时，自动将其设置为 “default-scheduler”）。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:3:2","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"3.3 Extender 上面都是基于内置的 Plugin 进行配置，Scheduling Profile 也完全能够实现 Scheduling Policy 的 Webhook 扩展功能。 apiVersion:kubescheduler.config.k8s.io/v1beta1kind:KubeSchedulerConfigurationleaderElection:leaderElect:trueresourceNamespace:namespaceresourceName:my-schedulerprofiles:- schedulerName:my-schedulerextenders:- urlPrefix:http://127.0.0.1:10262/schedulerfilterVerb:filterpreemptVerb:preemptweight:1enableHTTPS:falsehttpTimeout:30s extenders 字段用于定制扩展的 Webhook，调度器会在对应的阶段调用对应的 URL。这对于所有的 profiles 都是生效的。 按照示例中的定义，我们定义一个新调度器只需要一个配置： $ kube-scheduler --config \u003cconfig-file\u003e ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:3:3","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"4 方案 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:4:0","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"4.1 如何配置 Scheduler 从前面所述知识看到，配置一个 Scheduler 有着几种方式： 基于 kube-scheduler 的代码框架，添加自定义的 Plugin，然后编译部署（可能需要在 Scheduling Profiles 配置开启 Plugin）； 如果我们需要很细致的定制调度，可以使用这种方式。不过，我们需要自己对 kube-scheduler 进行版本维护。 通过 Scheduling Policy 定制； 不应该是优先考虑的方式，除非 Kubernetes 版本不支持其他方式。 通过 Scheduling Profiles 定制内置的 Plugin； 如果 kube-scheduler 内置的 Plugin 能够满足需求，仅仅需要配置，那么可以使用这种方式。 通过 Scheduling Porfiles 中的 Extender 功能配置自定义的 Webhook。 需要简单的定制调度，可以使用这种方式。 第一种与第四种都可以让我们实现我们自己的调度，区别在于 Plugin 可以基于所有的扩展点实现，而 Extender 只能基于大的阶段进行定制。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:4:1","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"4.2 如何运行 Multiple Schedulers Multiple Schedulers 是基于配置 Scheduler 的基础上实现的，我们可以将一个定制的 Scheduler 命名，可以与 Kubernetes 原生的 Scheduler（名为 default-scheduler）同时运行。所以关键就是如何命名 Scheduler 了。 目前有两种方式来命名 Scheduler： 版本 \u003c 1.22 可以使用 kube-scheduler --scheduler-name \u003cname\u003e 给调度器命名； 版本 \u003e= 1.19 可以使用 Scheduling Profiles 配置文件中的 profiles[].schedulerName 给调度器命名，并且支持一个 kube-scheduler 程序运行多个调度器。 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:4:2","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"5 实现 Plugin ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:5:0","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"6 实现 Extender ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:6:0","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":"参考 Doc: Scheduling Policies Ooc: Scheduler Configuration Doc: Scheduling Framework Blog: 自定义 Kubernetes 调度器 ","date":"2021-09-22","objectID":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/:7:0","tags":["k8s","云计算"],"title":"K8s 编程 - 8 - Custom Scheduler","uri":"/posts/cloud_computing/k8s_programming/8-extend-scheduler/"},{"categories":["Kubernetes 编程"],"content":" Kubernetes 编程系列 主要记录一些开发 Controller 所相关的知识，大部分内容来自于《Programming Kubernetes》（推荐直接阅读）。 下面代码都来自于 agnhost/webhook，这是官方的 Admission Webhook 的示例 在 Custom APIServer/Admission 一节中看到，APISever 提供了 Admission Plugin 机制来进行 Mutating 与 Validating 的插件式扩展。不过 Admission Plugin 是要重新编译 APISever 来实现的，因此 APISever 提供了更加灵活的 Admission Webhook。 Admission Webhook 的调用时机在 Admission Plugin 的尾部，在 Quota Plugin 之前。 Admission Webhook 的使用包含： 部署 Webhook Server，用于 APISever 转发请求； 部署 ValidatingWebhookConfiguration/MutatingWebhookConfiguration 资源，向 APISever 注册 Webhook Server； RBAC（如果 Webhook Server 需要） ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:0:0","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"1 WebhookConfiguration 通过部署 ValidatingWebhookConfiguration/MutatingWebhookConfiguration 来向 APIServer 注册 Validating/Mutating Webhook Server。下面是一个 ValidatingWebhookConfiguration 示例，而 MutatingWebhookConfiguration 是类似的。 apiVersion:admissionregistration.k8s.io/v1kind:ValidatingWebhookConfigurationmetadata:name:\"pod-policy.example.com\"webhooks:- name:\"pod-policy.example.com\"# objectSelector:# matchLabels:# foo: bar# namespaceSelector:# matchExpressions:# - key: runlevel# operator: NotIn# values: [\"0\",\"1\"]matchPolicy:Equivalentrules:- apiGroups:[\"\"]apiVersions:[\"v1\"]operations:[\"CREATE\"]resources:[\"pods\"]scope:\"Namespaced\"clientConfig:# url: \" https://my-webhook.example.com:9443/my-webhook-path\"service:namespace:\"example-namespace\"name:\"example-service\"caBundle:\"Ci0tLS0tQk...\u003c`caBundle` is a PEM encoded CA bundle which will be used to validate the webhook's server certificate.\u003e...tLS0K\"admissionReviewVersions:[\"v1\",\"v1beta1\"]sideEffects:NonetimeoutSeconds:5reinvocationPolicy:IfNeededfailurePolicy:Fail webhooks: 定义一个或者多个 webhook servers name - 定义多个 webhook 情况下，需要定义一个唯一的 name clientConfig - 定义 APIServer 转发的目标，可以为 URL 或者 Service admissionReviewVersions - 表明 webhook server 支持的 AdmissionReview 的版本 sideEffects - 表明 Webhook 是否支持 Side effect timeoutSeconds（1-30) - 配置 APIServer 等待 webhook server 回复的超时时间，超时后根据 failure policy 处理 rules - 定义转发的匹配规则 objectSelector - 根据 object 的 label 来筛选 APIServer 需要转发的请求 namespaceSelector - 根据 namespace 来筛选 APIServer 需要转发的请求 matchPolicy - 定义 rule 如何匹配请求 reinvocationPolicy - 定义是否需要重跑 webhook，在 object 修改后 failurePolicy - 定义请求失败后的行为 ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:1:0","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"1.1 请求匹配规则 1.1.1 rules 每个 webhook 必须指定一组 rules，用以让 APIServer 决定是否转发请求给 webhook server。当一个请求能够匹配 operation、group、version、resource、scope，那么请求就会转发。 rules:- apiGroups:[\"\"]apiVersions:[\"v1\"]operations:[\"CREATE\"]resources:[\"pods\"]scope:\"Namespaced\" operations - 匹配请求的操作，支持 CREATE UPDATE DELETE CONNECT * apiGroups - 匹配请求的操作的 API Group，\"\" 表示 core API，\"*\" 匹配所有的 API Group apiVersions - 匹配 API Group 下的多个版本，\"*\" 匹配所有版本 resources - 匹配 API Group 下多个 resource “*” 表示匹配所有 resource，但是不包含 subresource “*/” 表示匹配所有 resource 与 subresource “pods/” 表示匹配 Pod 下的所有 subresource “*/status” 表示匹配所有 resource 下的 status subresource scope - 定义需要匹配 resource 与 subresource 的范围，支持 Cluster Namespaced * 1.1.2 objectSelector 通过 objectSelector 根据请求操作的 object 的 label 来筛选请求，成功匹配的请求才会被转发。 下面示例中，经过 rules 匹配后的请求，还会经过 objectSelector 筛选（需要包含 label foo:bar）。 apiVersion:admissionregistration.k8s.io/v1kind:MutatingWebhookConfiguration# ...webhooks:- name:my-webhook.example.comobjectSelector:matchLabels:foo:barrules:- operations:[\"CREATE\"]apiGroups:[\"*\"]apiVersions:[\"*\"]resources:[\"*\"]scope:\"*\"# ... 1.1.3 namespaceSelector 通过 namespaceSelector 根据请求操作的 object 的 namespace 来筛选请求。 apiVersion:admissionregistration.k8s.io/v1kind:MutatingWebhookConfiguration# ...webhooks:- name:my-webhook.example.comnamespaceSelector:matchExpressions:- key:runleveloperator:NotInvalues:[\"0\",\"1\"]rules:- operations:[\"CREATE\"]apiGroups:[\"*\"]apiVersions:[\"*\"]resources:[\"*\"]scope:\"Namespaced\"# ... 1.1.4 matchPolicy 一个资源可能属于多个 API Group，这在升级资源版本时很常见。例如，Deployment 支持 extensions/v1beta1，apps/v1beta1 等。 matchPolicy 用于定义 rules 如何匹配请求，其值可以为： Exact - 表示请求需要完全匹配 Equivalent（默认值） - 表示请求可以匹配不同 APIGroup 的相同资源 例如下面示例，通过 matchPolicy 也可以匹配 extensions/v1beta1 的 Deployment 了。 apiVersion:admissionregistration.k8s.io/v1kind:ValidatingWebhookConfiguration# ...webhooks:- name:my-webhook.example.commatchPolicy:Equivalentrules:- operations:[\"CREATE\",\"UPDATE\",\"DELETE\"]apiGroups:[\"apps\"]apiVersions:[\"v1\"]resources:[\"deployments\"]scope:\"Namespaced\"# ... 如何实现的？ 这是根据资源版本转换的实现，因为 extensions/v1beta1 Deployment 在 APIServer 中会被转化为 v1 请求，从而可以匹配到 Webhook 转发。 ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:1:1","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"1.2 与 Webhook 通信方式 1.2.1 URL URL 为一个 webhook server 地址，以标准的 URL 格式。但是，不允许使用用户或者基本身份认证（例如 URL 中的 user:password@），也不允许 URL 中使用 # 和 ? 传参。 apiVersion:admissionregistration.k8s.io/v1kind:MutatingWebhookConfiguration# ...webhooks:- name:my-webhook.example.comclientConfig:url:\" https://my-webhook.example.com:9443/my-webhook-path\"# ... 1.2.2 Service 如果 Webhook Server 运行在集群中，可以指定 Service 来转发请求。其 port 默认为 443，path 默认为 “/\"。 apiVersion:admissionregistration.k8s.io/v1kind:MutatingWebhookConfiguration# ...webhooks:- name:my-webhook.example.comclientConfig:caBundle:\"Ci0tLS0tQk...\u003cbase64-encoded PEM bundle containing the CA that signed the webhook's serving certificate\u003e...tLS0K\"service:namespace:my-service-namespacename:my-service-namepath:/my-pathport:1234# ... ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:1:2","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"1.3 Side effects 有些情况下，Webhook Server 不仅仅是处理 AdmissionReview 对象，而要进行一些 “带外” 操作（指操作实际的资源），这也被称为 Side effect。 sideEffects 字段用于指定 Webhook Server 能否处理 dryRun 的 请求（dryRun:true）。值可以为： Unknown - 未知，对于 dryRun 请求要转发给 Webhook 的，直接视为请求失败。 None - 表明 Webhook 没有 Side effect。 Some - 表明 Webhook 有一些 side effect，对于 dryRun 请求要转发给 Webhook 的，将直接视为请求失败。 NoneOnDryRun - 表明 Webhook 有一些 side effect，而 Webhook 能够处理 dryRun 请求。 apiVersion:admissionregistration.k8s.io/v1kind:ValidatingWebhookConfiguration# ...webhooks:- name:my-webhook.example.comsideEffects:NoneOnDryRun# ... ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:1:3","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"1.4 Reinvocation policy 对于 Mutating Admission Plugin，顺序调用不适用于所有的情况。可能 Webhook 修改了对象的结构，但是其他 Mutating Plugin 对于其新结构是拒绝的（但是其已经被调用过了，因此不再被调用）。 而 1.15 后，如果 Mutating Webhook 更改对象，内置的 Mutating Admission Plugin 会重跑。 reinvocationPolicy 字段可以控制 Webhook Server 在这种情况下，是否需要重跑。值可以为： Never - 一次中 Admission Control 不能多次调用 Webhook。 IfNeeded - Webhook 调用后的对象又被其他 Admission Plugin 修改了，那么 Webhook 会再次重跑。 apiVersion:admissionregistration.k8s.io/v1kind:MutatingWebhookConfiguration# ...webhooks:- name:my-webhook.example.comreinvocationPolicy:IfNeeded# ... 不过需要注意： 不保证额外调用次数是一次 如果额外调用，导致对象再一次被修改，那么不保证还会再次调用 Webhook 使用此选项的 Webhook 可能会被重新排序，以减少额外调用数。 要保证修改后验证对象，应该使用 Validate Admission Webhook 这也表明了，Mutating Webhook 必须是幂等的。 ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:1:4","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"1.5 Failure policy failurePolicy 字段定义了，当 APIServer 请求 Webhook 失败后，如何处理（包括超时错误）。 Ignore - Webhook 返回的错误会被忽略，API 请求继续 Fail（默认） - Webhook 返回错误，API 请求被拒绝 apiVersion:admissionregistration.k8s.io/v1kind:MutatingWebhookConfiguration# ...webhooks:- name:my-webhook.example.comfailurePolicy:Fail# ... ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:1:5","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"2 Request 与 Response ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:2:0","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"2.1 Request APIServer 发送 POST HTTP 请求，其设置了 HTTP Header 中 Content-Type: application/json，body 内容为 AdmissionReview 对象的 JSON 格式，设置了其中的 “request” 字段。 下面示例展示了对于 apps/v1 Deployment scale 子资源的调用请求，对应的 AdmissionReview 对象内容。 { \"apiVersion\": \"admission.k8s.io/v1\", \"kind\": \"AdmissionReview\", \"request\": { # 随机 uid，用于标识这次 admission 调用 \"uid\": \"705ab4f5-6393-11e8-b7cc-42010a800002\", # 请求中对象的 GVK \"kind\": {\"group\":\"autoscaling\",\"version\":\"v1\",\"kind\":\"Scale\"}, # 请求中对象的 GVR \"resource\": {\"group\":\"apps\",\"version\":\"v1\",\"resource\":\"deployments\"}, # 请求对象的 subresource \"subResource\": \"scale\", # 请求源对象的 GVK # 因为 `matchPolicy: Equivalent` 可能会让请求转变，而这里保存转变前的 GVK \"requestKind\": {\"group\":\"autoscaling\",\"version\":\"v1\",\"kind\":\"Scale\"}, # 请求源对象的 GVR # 因为 `matchPolicy: Equivalent` 可能会让请求转变，而这里保存转变前的 GVR # Fully-qualified group/version/kind of the resource being modified in the original request to the API server. # This only differs from `resource` if the webhook specified `matchPolicy: Equivalent` and the # original request to the API server was converted to a version the webhook registered for. \"requestResource\": {\"group\":\"apps\",\"version\":\"v1\",\"resource\":\"deployments\"}, # 请求源对象的 subresource # 因为 `matchPolicy: Equivalent` 可能会让请求转变，而这里保存转变前的 subresource \"requestSubResource\": \"scale\", # 请求对象的 name \"name\": \"my-deployment\", # 请求对象的 namespace \"namespace\": \"my-namespace\", # 请求的操作，CREATE UPDATE DELETE CONNECT \"operation\": \"UPDATE\", \"userInfo\": { # APIServer 身份认证的 username \"username\": \"admin\", # APIServer 身份认证的 uid \"uid\": \"014fbff9a07c\", # APIServer 身份认证的 group \"groups\": [\"system:authenticated\",\"my-admin-group\"], # Arbitrary extra info associated with the user making the request to the API server. # This is populated by the API server authentication layer and should be included # if any SubjectAccessReview checks are performed by the webhook. \"extra\": { \"some-key\":[\"some-value1\", \"some-value2\"] } }, # 请求操作的对象，可以通过 scheme 解析为具体的结构 \"object\": {\"apiVersion\":\"autoscaling/v1\",\"kind\":\"Scale\",...}, # 当前集群中的对象 # It is null for CREATE and CONNECT operations. \"oldObject\": {\"apiVersion\":\"autoscaling/v1\",\"kind\":\"Scale\",...}, # 操作的 Option，包含 CreateOptions、UpdateOptions、DeleteOptions \"options\": {\"apiVersion\":\"meta.k8s.io/v1\",\"kind\":\"UpdateOptions\",...}, # dryRun 表明请求是否是 dry run mode # See http://k8s.io/docs/reference/using-api/api-concepts/#make-a-dry-run-request for more details. \"dryRun\": false } } ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:2:1","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"2.2 Response Webhook server 应该返回 200 HTTP code，并设置了 HTTP Header 中 Content-Type: application/json，body 内容为 AdmissionReview 对象的 JSON 格式，设置了其中的 “response” 字段。 不同的操作可能有着不同的 response，不过至少其必须包含以下字段： { \"apiVersion\": \"admission.k8s.io/v1\", \"kind\": \"AdmissionReview\", \"response\": { \"uid\": \"\u003cvalue from request.uid\u003e\", \"allowed\": true } } uid - 复制 request.uid allowed - 设置为 true 或者 false，表明允许或者禁止 拒绝请求时，通过 status 字段可以提供需要 APIServer 返回给 client 的 HTTP code 与 message。 { \"apiVersion\": \"admission.k8s.io/v1\", \"kind\": \"AdmissionReview\", \"response\": { \"uid\": \"\u003cvalue from request.uid\u003e\", \"allowed\": false, \"status\": { \"code\": 403, \"message\": \"You cannot do this because it is Tuesday and your name starts with A\" } } } 对于 MutatingWebhook 可以对对象进行修改，这是通过 JSON Patch 实现的。 { \"apiVersion\": \"admission.k8s.io/v1\", \"kind\": \"AdmissionReview\", \"response\": { \"uid\": \"\u003cvalue from request.uid\u003e\", \"allowed\": true, \"patchType\": \"JSONPatch\", \"patch\": \"W3sib3AiOiAiYWRkIiwgInBhdGgiOiAiL3NwZWMvcmVwbGljYXMiLCAidmFsdWUiOiAzfV0=\" } } patchType - 指定 patch 类型，目前仅仅支持 JSONPatch patch - patch 操作的 base64 编码，示例中为 “[{“op”: “add”, “path”: “/spec/replicas”, “value”: 3}]” 1.19 后，也可以选择返回一个 WARN 信息，通过 warnings 字段。 { \"apiVersion\": \"admission.k8s.io/v1\", \"kind\": \"AdmissionReview\", \"response\": { \"uid\": \"\u003cvalue from request.uid\u003e\", \"allowed\": true, \"warnings\": [ \"duplicate envvar entries specified with name MY_ENV\", \"memory request less than 4MB specified for container mycontainer, which will not start successfully\" ] } } warn 信息不能超过 120 字节 ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:2:2","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"3 与 APIServer 的鉴权方式 Admission Webhook 与 APIServer 支持三种鉴权方式：basic auth、bearer token、cert。 需要三个阶段来进行配置： 启动 APIServer 时，通过 \"–admission-control-config-file” 参数指定 Admission Control 配置文件。 在 Admission Control 配置文件中，指定 MutatingAdmissionWebhook Controller 与 ValidatingAdmissionWebhook Controller 读取的证书。 在配置文件，通过 kubeConfigFile 字段指定对应的 kubeconfig 文件。 apiVersion:apiserver.config.k8s.io/v1kind:AdmissionConfigurationplugins:- name:ValidatingAdmissionWebhookconfiguration:apiVersion:apiserver.config.k8s.io/v1kind:WebhookAdmissionConfigurationkubeConfigFile:\"\u003cpath-to-kubeconfig-file\u003e\"- name:MutatingAdmissionWebhookconfiguration:apiVersion:apiserver.config.k8s.io/v1kind:WebhookAdmissionConfigurationkubeConfigFile:\"\u003cpath-to-kubeconfig-file\u003e\" 在指定的 kubeConfig 文件中，提供需要的证书 apiVersion:v1kind:Configusers:# name should be set to the DNS name of the service or the host (including port) of the URL the webhook is configured to speak to.# If a non-443 port is used for services, it must be included in the name when configuring 1.16+ API servers.## For a webhook configured to speak to a service on the default port (443), specify the DNS name of the service:# - name: webhook1.ns1.svc# user: ...## For a webhook configured to speak to a service on non-default port (e.g. 8443), specify the DNS name and port of the service in 1.16+:# - name: webhook1.ns1.svc:8443# user: ...# and optionally create a second stanza using only the DNS name of the service for compatibility with 1.15 API servers:# - name: webhook1.ns1.svc# user: ...## For webhooks configured to speak to a URL, match the host (and port) specified in the webhook's URL. Examples:# A webhook with `url: https://www.example.com`:# - name: www.example.com# user: ...## A webhook with `url: https://www.example.com:443`:# - name: www.example.com:443# user: ...## A webhook with `url: https://www.example.com:8443`:# - name: www.example.com:8443# user: ...#- name:'webhook1.ns1.svc'user:client-certificate-data:\"\u003cpem encoded certificate\u003e\"client-key-data:\"\u003cpem encoded key\u003e\"# The `name` supports using * to wildcard-match prefixing segments.- name:'*.webhook-company.org'user:password:\"\u003cpassword\u003e\"username:\"\u003cname\u003e\"# '*' is the default match.- name:'*'user:token:\"\u003ctoken\u003e\" Note 配置中仅仅配置了 client 证书与私钥，而在 ValidatingWebhookConfiguration/MutatingWebhookConfiguration 定义中指定了 CA Bundle，即 CA 证书。 ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:3:0","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"4 实现 接下来我们来看 Webhook Server 的实现，幸运的是，其比 Custom APIServer 要简单的多。 ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:4:0","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"4.1 HTTP Server Webhook Server 本质上就是一个 HTTP Server，因此其 main 函数中，主要就是启动了一个 HTTP Server。 func main(cmd *cobra.Command, args []string) { config := Config{ CertFile: certFile, KeyFile: keyFile, } // 定义 HTTP handle // 下面每个 Handle 都是一个 Admission Webhook // ** 只要一个 HTTP Endpoint 能够处理 AdmissionReview 对象，那么就可以作为一个 Admission Webhook ** http.HandleFunc(\"/always-allow-delay-5s\", serveAlwaysAllowDelayFiveSeconds) http.HandleFunc(\"/always-deny\", serveAlwaysDeny) http.HandleFunc(\"/add-label\", serveAddLabel) http.HandleFunc(\"/pods\", servePods) http.HandleFunc(\"/pods/attach\", serveAttachingPods) http.HandleFunc(\"/mutating-pods\", serveMutatePods) http.HandleFunc(\"/mutating-pods-sidecar\", serveMutatePodsSidecar) http.HandleFunc(\"/configmaps\", serveConfigmaps) http.HandleFunc(\"/mutating-configmaps\", serveMutateConfigmaps) http.HandleFunc(\"/custom-resource\", serveCustomResource) http.HandleFunc(\"/mutating-custom-resource\", serveMutateCustomResource) http.HandleFunc(\"/crd\", serveCRD) http.HandleFunc(\"/readyz\", func(w http.ResponseWriter, req *http.Request) { w.Write([]byte(\"ok\")) }) // 创建被启动 APIServer server := \u0026http.Server{ Addr: fmt.Sprintf(\":%d\", port), TLSConfig: configTLS(config), } err := server.ListenAndServeTLS(\"\", \"\") if err != nil { panic(err) } } 上面的 HTTP Server 注册了许多的 HTTP API。因为每个 API 都可以处理 AdmissionReview 请求，所以就可以作为一个 Admission Webhook。当对应的 Webhook Configuration 定义部署后，就可以使用。 ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:4:1","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"4.2 通用处理逻辑 serve 所有的 HTTP Endpoint HandleFunc 都是基于 serve 的调用，例如： func serveAlwaysAllowDelayFiveSeconds(w http.ResponseWriter, r *http.Request) { serve(w, r, newDelegateToV1AdmitHandler(alwaysAllowDelayFiveSeconds)) } 因为所有的 Webhook 有着最基本的编解码的逻辑，这一部分是相同的，而 server 就是处理这部分逻辑的。 // serve 处理 HTTP 请求，从 body 中解析得到 AdmissionReview，然后调用 admit 回调处理，最后将结果回复 func serve(w http.ResponseWriter, r *http.Request, admit admitHandler) { // read http body var body []byte if r.Body != nil { if data, err := ioutil.ReadAll(r.Body); err == nil { body = data } } // 检查 Header Content-Type，Webhook 只支持 \"application/json\" contentType := r.Header.Get(\"Content-Type\") if contentType != \"application/json\" { klog.Errorf(\"contentType=%s, expect application/json\", contentType) return } klog.V(2).Info(fmt.Sprintf(\"handling request: %s\", body)) // 解析 body -\u003e runtime.Object // 因为 AdmissionReview 也是一个 Kubernetes 资源，所以直接使用 Kubernetes 进行解析 deserializer := codecs.UniversalDeserializer() obj, gvk, err := deserializer.Decode(body, nil, nil) if err != nil { msg := fmt.Sprintf(\"Request could not be decoded: %v\", err) klog.Error(msg) http.Error(w, msg, http.StatusBadRequest) return } // 进一步得到 Admission Request var responseObj runtime.Object switch *gvk { case v1beta1.SchemeGroupVersion.WithKind(\"AdmissionReview\"): // v1beta1 版本 AdmissionReview 对象支持 requestedAdmissionReview, ok := obj.(*v1beta1.AdmissionReview) if !ok { klog.Errorf(\"Expected v1beta1.AdmissionReview but got: %T\", obj) return } // 得到真正的 v1beta1.AdmissionReview 对象 responseAdmissionReview := \u0026v1beta1.AdmissionReview{} responseAdmissionReview.SetGroupVersionKind(*gvk) // 调用回调处理，返回值为 AdmissionReview.Response responseAdmissionReview.Response = admit.v1beta1(*requestedAdmissionReview) // 设置好 Response responseAdmissionReview.Response.UID = requestedAdmissionReview.Request.UID responseObj = responseAdmissionReview case v1.SchemeGroupVersion.WithKind(\"AdmissionReview\"): // v1 版本 AdmissionReview 对象支持 requestedAdmissionReview, ok := obj.(*v1.AdmissionReview) if !ok { klog.Errorf(\"Expected v1.AdmissionReview but got: %T\", obj) return } // 得到真正的 v1.AdmissionReview 对象 responseAdmissionReview := \u0026v1.AdmissionReview{} responseAdmissionReview.SetGroupVersionKind(*gvk) // 调用回调处理，返回值为 AdmissionReview.Response responseAdmissionReview.Response = admit.v1(*requestedAdmissionReview) // 设置好 Response responseAdmissionReview.Response.UID = requestedAdmissionReview.Request.UID responseObj = responseAdmissionReview default: msg := fmt.Sprintf(\"Unsupported group version kind: %v\", gvk) klog.Error(msg) http.Error(w, msg, http.StatusBadRequest) return } // 编码 http response klog.V(2).Info(fmt.Sprintf(\"sending response: %v\", responseObj)) respBytes, err := json.Marshal(responseObj) if err != nil { klog.Error(err) http.Error(w, err.Error(), http.StatusInternalServerError) return } // 回复 w.Header().Set(\"Content-Type\", \"application/json\") if _, err := w.Write(respBytes); err != nil { klog.Error(err) } } 从中我们也可以总结出基本的 Webhook 处理逻辑： 读取 HTTP 请求的 body。 检查 HTTP 请求的 Content-Type 是否为 application/json。 将 body 解析为 AdmissionReview 对象。 业务处理，得到 AdmissionReview Response。 将回复的 AdmissionReview 对象 JSON 编码。 设置好 Content-Type:application/json 后回复。 ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:4:2","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"4.3 业务处理逻辑 admitHandler 在 serve 中看到，抽象了 admitHandler 来进行业务逻辑的处理。其包含处理 v1beta1 与 v1 版本的 AdmissionReview 对象的回调。 // admitv1beta1Func handles a v1beta1 admission type admitv1beta1Func func(v1beta1.AdmissionReview) *v1beta1.AdmissionResponse // admitv1beta1Func handles a v1 admission type admitv1Func func(v1.AdmissionReview) *v1.AdmissionResponse // admitHandler 是 Webhook handler 的抽象，需要能够处理 v1beta1 v1 版本 AdmissionReview 对象 type admitHandler struct { v1beta1 admitv1beta1Func v1 admitv1Func } 为了让大部分的处理函数仅仅只需要支持 v1 版本的 AdmissionReview 对象，内置了 newDelegateToV1AdmitHandler 函数： // newDelegateToV1AdmitHandler 返回能够自动支持 v1beta1 版本 AdmissionReview 的 Handler func newDelegateToV1AdmitHandler(f admitv1Func) admitHandler { return admitHandler{ v1beta1: delegateV1beta1AdmitToV1(f), v1: f, } } func delegateV1beta1AdmitToV1(f admitv1Func) admitv1beta1Func { return func(review v1beta1.AdmissionReview) *v1beta1.AdmissionResponse { in := v1.AdmissionReview{Request: convertAdmissionRequestToV1(review.Request)} // 将 v1beta1 AdmissionReview Request 转换为 v1 版本 out := f(in) return convertAdmissionResponseToV1beta1(out) // 将 v1 AdmissionReview Response 转换为 v1beta1 版本 } } func convertAdmissionRequestToV1(r *v1beta1.AdmissionRequest) *v1.AdmissionRequest { return \u0026v1.AdmissionRequest{ Kind: r.Kind, Namespace: r.Namespace, Name: r.Name, Object: r.Object, Resource: r.Resource, Operation: v1.Operation(r.Operation), UID: r.UID, DryRun: r.DryRun, OldObject: r.OldObject, Options: r.Options, RequestKind: r.RequestKind, RequestResource: r.RequestResource, RequestSubResource: r.RequestSubResource, SubResource: r.SubResource, UserInfo: r.UserInfo, } } func convertAdmissionResponseToV1beta1(r *v1.AdmissionResponse) *v1beta1.AdmissionResponse { var pt *v1beta1.PatchType if r.PatchType != nil { t := v1beta1.PatchType(*r.PatchType) pt = \u0026t } return \u0026v1beta1.AdmissionResponse{ UID: r.UID, Allowed: r.Allowed, AuditAnnotations: r.AuditAnnotations, Patch: r.Patch, PatchType: pt, Result: r.Result, Warnings: r.Warnings, } } 通过提供的 convert 相关函数，我们所有的 Webhook 业务逻辑仅仅只需要支持 v1 AdmissionReview 即可。 ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:4:3","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"4.4 业务逻辑 4.4.1 Validate 对请求进行 Validate，并允许请求。核心就是设置 Reponse.Allowed 字段： // alwaysAllowDelayFiveSeconds 是一个 v1 AdmissionReview Handler，sleep 5s 后返回 allowed response func alwaysAllowDelayFiveSeconds(ar v1.AdmissionReview) *v1.AdmissionResponse { klog.V(2).Info(\"always-allow-with-delay sleeping for 5 seconds\") time.Sleep(5 * time.Second) klog.V(2).Info(\"calling always-allow\") reviewResponse := v1.AdmissionResponse{} reviewResponse.Allowed = true // 设置 Response.Allowed 为 true，表明请求允许 reviewResponse.Result = \u0026metav1.Status{Message: \"this webhook allows all requests\"} return \u0026reviewResponse } 拒绝请求也是类似： // alwaysDeny 拒绝所有请求 func alwaysDeny(ar v1.AdmissionReview) *v1.AdmissionResponse { klog.V(2).Info(\"calling always-deny\") reviewResponse := v1.AdmissionResponse{} reviewResponse.Allowed = false // 返回 allowed 为 false 表明拒绝请求 reviewResponse.Result = \u0026metav1.Status{Message: \"this webhook denies all requests\"} return \u0026reviewResponse } 复杂一点，就需要通过编码器对 Request.Object.Raw 进行解析，得到一个具体的 Resource 对象。 // admitPods 检查 Pod 的 image func admitPods(ar v1.AdmissionReview) *v1.AdmissionResponse { // 判断资源是否是 Pod klog.V(2).Info(\"admitting pods\") podResource := metav1.GroupVersionResource{Group: \"\", Version: \"v1\", Resource: \"pods\"} if ar.Request.Resource != podResource { err := fmt.Errorf(\"expect resource to be %s\", podResource) klog.Error(err) return toV1AdmissionResponse(err) } // 解码，解析为 Pod raw := ar.Request.Object.Raw pod := corev1.Pod{} deserializer := codecs.UniversalDeserializer() if _, _, err := deserializer.Decode(raw, nil, \u0026pod); err != nil { klog.Error(err) return toV1AdmissionResponse(err) } reviewResponse := v1.AdmissionResponse{} reviewResponse.Allowed = true var msg string if v, ok := pod.Labels[\"webhook-e2e-test\"]; ok { if v == \"webhook-disallow\" { reviewResponse.Allowed = false msg = msg + \"the pod contains unwanted label; \" } if v == \"wait-forever\" { reviewResponse.Allowed = false msg = msg + \"the pod response should not be sent; \" \u003c-make(chan int) // Sleep forever - no one sends to this channel } } for _, container := range pod.Spec.Containers { if strings.Contains(container.Name, \"webhook-disallow\") { reviewResponse.Allowed = false msg = msg + \"the pod contains unwanted container name; \" } } if !reviewResponse.Allowed { reviewResponse.Result = \u0026metav1.Status{Message: strings.TrimSpace(msg)} } return \u0026reviewResponse } 对于 Subresource 的请求，就需要用到 Request.SubResource，以及明确 HTTP body 的内容： // denySpecificAttachment 检查 Pod attch 参数 func denySpecificAttachment(ar v1.AdmissionReview) *v1.AdmissionResponse { klog.V(2).Info(\"handling attaching pods\") if ar.Request.Name != \"to-be-attached-pod\" { return \u0026v1.AdmissionResponse{Allowed: true} } // 检查 Resource 为 Pod podResource := metav1.GroupVersionResource{Group: \"\", Version: \"v1\", Resource: \"pods\"} if e, a := podResource, ar.Request.Resource; e != a { err := fmt.Errorf(\"expect resource to be %s, got %s\", e, a) klog.Error(err) return toV1AdmissionResponse(err) } // 检查请求的 SubResource 为 attach if e, a := \"attach\", ar.Request.SubResource; e != a { err := fmt.Errorf(\"expect subresource to be %s, got %s\", e, a) klog.Error(err) return toV1AdmissionResponse(err) } // 解码，转变为 PodAttachOptions raw := ar.Request.Object.Raw podAttachOptions := corev1.PodAttachOptions{} deserializer := codecs.UniversalDeserializer() if _, _, err := deserializer.Decode(raw, nil, \u0026podAttachOptions); err != nil { klog.Error(err) return toV1AdmissionResponse(err) } // validate klog.V(2).Info(fmt.Sprintf(\"podAttachOptions=%#v\\n\", podAttachOptions)) if !podAttachOptions.Stdin || podAttachOptions.Container != \"container1\" { return \u0026v1.AdmissionResponse{Allowed: true} } return \u0026v1.AdmissionResponse{ Allowed: false, Result: \u0026metav1.Status{ Message: \"attaching to pod 'to-be-attached-pod' is not allowed\", }, } } 4.4.2 Mutate Mutate 可以在 Validate 基础上可以对对象进行修改，核心是设置好 Response.Patch 与 Response.PatchType 字段。 const ( podsInitContainerPatch string = `[ {\"op\":\"add\",\"path\":\"/spec/initContainers\",\"value\":[{\"image\":\"webhook-added-image\",\"name\":\"webhook-adde","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:4:4","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":"参考 《Programming Kubernetes》 Dynamic Admission Control ","date":"2021-09-21","objectID":"/posts/cloud_computing/k8s_programming/7-admission-webhook/:5:0","tags":["k8s","云计算"],"title":"K8s 编程 - 7 - Admission Webhook","uri":"/posts/cloud_computing/k8s_programming/7-admission-webhook/"},{"categories":["Kubernetes 编程"],"content":" Kubernetes 编程系列 主要记录一些开发 Controller 所相关的知识，大部分内容来自于《Programming Kubernetes》（推荐直接阅读）。 Custom APIServer 可以作为 CRD 的替代品，它可以像 Kubernetes 的原生的 APIServer 一样为 API Group 提供服务。 下面代码都来自于 kubernetes/sample-apiserver，这是官方的 Custom APIServer 的示例 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:0:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"1 Custom APISever 的适用场景 自定义 APIServer 以一定开发与使用复杂性的代价，而做到十分灵活的功能。我们通过对比 CRD 与 APIServer 来看一下其特有的好处： CRD 的缺陷： 只能使用 Kubernetes 使用的存储方式（默认 etcd） 不支持 protobuf，只支持 JSON 仅仅支持 /status 和 /scale 两种子资源 不支持平滑删除，你需要靠 Finalizer 来模拟这个行为 显著增加了 Kubernetes APIServer 的 CPU 负载，因为所有算法都用一个通用方式实现 对 API HTTP Endpoint 仅仅实现了标准的 CRUD 语义，不能扩展 不支持资源共栖（即不同 APIGroup 的资源或不同名字的资源在底层共栖存储） APIServer 可以实现： 可以使用任何存储作为后端存储 可以实现 Protobuf 支持 可以提供任意自定义的子资源，例如 /exec、/logs、/port-forward 等 可以实现平滑删除逻辑 可以用 Go 高效的实现所有操作，包括验证、准入和转换 可以自定义语义，即定义 CRUD 之外的 API 可以为相同存储机制的不同 APIGroup 或不同名称资源提供服务，例如 Deployment 最初存储在 extension/v1 组，后来挪到了 apps/v1 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:1:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"2 架构 自定义 APIServer 是为 APIGroup 提供服务的进程，通常会使用 k8s.io/apiserver 这个库来实现。 自定义 APIServer 可以在 Kubernetes 集群内运行，也可以在集群外运行。通常它们运行在 Pod 中，并通过 Service 提供服务。 Kubernetes 原生的 APIServer 称为 kube-apiserver。当 client 请求自定义 APIServer 时，请求首先会访问 kube-apiserver，然后由其转发给自定义 APIServer。这也代表了，kube-apiserver 知道所有的自定义 APIServer。 在 kube-apiserver 实现中，这个转发代理的组件称为 kube-aggregator，代理 API 请求的过程称为 API Aggregate。 向自定义 APIServer 发起请求的过程如下（下图中最左侧的流程）： kube-apiserver 收到请求。 请求经过处理链处理，包括身份认证、审计日志、切换用户、限流、授权等流程。 kube-apiserver 对相关 /apis/\u003caggregate-API-group-name\u003e HTTP Path 下的请求进行拦截。 将拦截下来的请求转发给自定义 APIServer。 总结一下，kube-aggregator 提供两种功能： Proxy - 可以为某个 HTTP Path 下的一个特定版本提供代理服务，例如 \"/apis/group-name/version\" Discovery - kube-aggregator 可以为所有聚合的 APIServer 提供 Discovery Endpoint，也是就说，你可以通过 \"/apis\" 和 \"/apis/group-name\" 找到所有的自定义 APIServer。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:2:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"2.1 APIService 如同 CRD 一样，为了让 Kubernetes APIServer 知道一个自定义 APIServer 的存在，必须创建一个 APIService 对象。 apiVersion:apiregistration.k8s.io/v1kind:APIService metadata:name:name spec:group:API-group-name version:API-group-version service:namespace:custom-API-server-service-namespace name:API-server-serviceport:1234caBundle:base64-caBundlegroupPriorityMinimum:2000versionPriority:20 spec.service - 指定自定义 APIServer 的 service，可以是集群内普通的 ClusterIP Service，也可以是外部的 ExternalName Service。 spec.caBundle - 用于设置访问自定义 APIServer 所需要的证书。 spec.groupPriorityMinimum - Group 的优先级，如果多个不同版本 APIService 设置了不同的值，值最大的会实际生效。 如果存在冲突的资源名称或者短名称，则拥有最大 groupPriorityMinimum 值的资源会生效。 spec.versionPriority - 给 Group 下各个版本排序，用于决定客户端优先使用哪个版本。 下面是 Kubernetes API 组的 groupPriorityMinimum 值： var apiVersionPriorities = map[schema.GroupVersion]priority{ {Group: \"\", Version: \"v1\"}: {group: 18000, version: 1}, // to my knowledge, nothing below here collides {Group: \"apps\", Version: \"v1\"}: {group: 17800, version: 15}, {Group: \"events.k8s.io\", Version: \"v1\"}: {group: 17750, version: 15}, {Group: \"events.k8s.io\", Version: \"v1beta1\"}: {group: 17750, version: 5}, {Group: \"authentication.k8s.io\", Version: \"v1\"}: {group: 17700, version: 15}, {Group: \"authorization.k8s.io\", Version: \"v1\"}: {group: 17600, version: 15}, {Group: \"autoscaling\", Version: \"v1\"}: {group: 17500, version: 15}, {Group: \"autoscaling\", Version: \"v2beta1\"}: {group: 17500, version: 9}, {Group: \"autoscaling\", Version: \"v2beta2\"}: {group: 17500, version: 1}, {Group: \"batch\", Version: \"v1\"}: {group: 17400, version: 15}, {Group: \"batch\", Version: \"v1beta1\"}: {group: 17400, version: 9}, {Group: \"batch\", Version: \"v2alpha1\"}: {group: 17400, version: 9}, {Group: \"certificates.k8s.io\", Version: \"v1\"}: {group: 17300, version: 15}, {Group: \"networking.k8s.io\", Version: \"v1\"}: {group: 17200, version: 15}, {Group: \"policy\", Version: \"v1\"}: {group: 17100, version: 15}, {Group: \"policy\", Version: \"v1beta1\"}: {group: 17100, version: 9}, {Group: \"rbac.authorization.k8s.io\", Version: \"v1\"}: {group: 17000, version: 15}, {Group: \"storage.k8s.io\", Version: \"v1\"}: {group: 16800, version: 15}, {Group: \"storage.k8s.io\", Version: \"v1beta1\"}: {group: 16800, version: 9}, {Group: \"storage.k8s.io\", Version: \"v1alpha1\"}: {group: 16800, version: 1}, {Group: \"apiextensions.k8s.io\", Version: \"v1\"}: {group: 16700, version: 15}, {Group: \"admissionregistration.k8s.io\", Version: \"v1\"}: {group: 16700, version: 15}, {Group: \"scheduling.k8s.io\", Version: \"v1\"}: {group: 16600, version: 15}, {Group: \"coordination.k8s.io\", Version: \"v1\"}: {group: 16500, version: 15}, {Group: \"node.k8s.io\", Version: \"v1\"}: {group: 16300, version: 15}, {Group: \"node.k8s.io\", Version: \"v1alpha1\"}: {group: 16300, version: 1}, {Group: \"node.k8s.io\", Version: \"v1beta1\"}: {group: 16300, version: 9}, {Group: \"discovery.k8s.io\", Version: \"v1\"}: {group: 16200, version: 15}, {Group: \"discovery.k8s.io\", Version: \"v1beta1\"}: {group: 16200, version: 12}, {Group: \"flowcontrol.apiserver.k8s.io\", Version: \"v1beta1\"}: {group: 16100, version: 12}, {Group: \"flowcontrol.apiserver.k8s.io\", Version: \"v1alpha1\"}: {group: 16100, version: 9}, {Group: \"internal.apiserver.k8s.io\", Version: \"v1alpha1\"}: {group: 16000, version: 9}, // Append a new group to the end of the list if unsure. // You can use min(existing group)-100 as the initial value for a group. // Version can be set to 9 (to have space around) for a new group. } 替换原生的 API 如果你替换原生的 API Group，就可以对自定义 APIService 指定为比上表中更低的优先级实现。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:2:1","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"2.2 Custom APIServer 的内部架构 Custom APIServer 大体上与 Kubernetes APIServer 相同，不过没有嵌入 kube-aggregator 和 apiextension-apiserver。 总结一下自定义 APIServer 的特点： 与 Kubernetes APIServer 内部结构一致。 拥有自己的处理链，包括身份认证、审计、切换用户等 拥有自己的资源处理流水线，包括解码、转换、准入、REST 映射和编码。 会调用 Webhook。 可以写 etcd，或者其他的后端存储。 拥有自己的 Scheme 并实现了自定义 API 组的 Registry。 再次进行身份认证。通常会发送一个 TokenAccessReview 请求对 Kubernetes APIServer 回调，实现基于 client 的证书认证和 token 认证。 自己进行审计。 使用 SubjectAccessReview 请求 Kubernetes APIServer 完成授权。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:2:2","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"2.3 身份认证机制 Custom APIServer 的处理在于 Kubernetes APIServer 之后，所以 Kubernetes APIServer 会先对请求进行认证，通过后再转发给自定义 APIServer。 Kubernetes APIServer 会将身份认证的结果保存在 HTTP 请求头里，通常是 X-Remote-User 和 X-Remote-Group Head。 Note 通过 APIServer 的启动命令参数 \"–requestheader-username-headers\" 和 \"–requestheader-group-headers\" 可以配置。 为了让 Custom APIServer 信任这两个 HTTP Head，Custom APIServer 会验证发送请求者的 CA。因此，需要通过一个 ConfigMap 配置 Kubernetes APIServer 的证书，证书要与 Custom APIServer 相同的根证书签名： apiVersion:v1 kind:ConfigMap metadata:name:extension-apiserver-authentication namespace:kube-system data:client-ca-file:| -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- requestheader-allowed-names:'[\"aggregator\"]'requestheader-client-ca-file:| -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- requestheader-extra-headers-prefix:'[\"X-Remote-Extra-\"]'requestheader-group-headers:'[\"X-Remote-Group\"]'requestheader-username-headers:'[\"X-Remote-User\"]' 这样认证的过程如下： Kubernetes APIServer 就会使用 data.client-ca-file 指定的客户端证书发起请求，进行 “预认证” 预认证后需要通过 data.requestheader-client-ca-file 证书发起转发请求，同时设置好相关的 Head。 最后，Custom APIServer 会通过 TokenAccessReview 的机制发送 Bearer Token（通过 HTTP Head：Authorization: bearer token）给 Kubernetes APIServer 来验证是否合法。 Note 上面这些过程主要是由 k8s.io/apiserver 库自动完成，我们只需要配置好证书。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:2:3","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"2.4 授权 完成身份认证后，每个请求都需要授权，Kubernetes 的授权机制是基于 RBAC 来完成的。 RBAC 将身份映射到角色，再将角色映射到授权规则，授权规则最终决定接受或者拒绝请求。我们需要理解，自定义 APIServer 是通过 SubjectAccessReview 代理授权来对请求授权的，它自身不会处理 RBAC 规则，而是委托给 Kubernetes APIServer 完成的。 为了什么需要 APIServer 授权 由于可能发送给 Custom APIServer 的请求是没有经过授权的，所以需要通过 Kubernetes APIServer 来进行授权。 自定义 APIServer 会发送一个 SubjectAccessReview 请求到 Kubernetes APIServer。 apiVersion:authorization.k8s.io/v1 kind:SubjectAccessReview spec:resourceAttributes:group:apps resource:deployments verb:create namespace:default version:v1 name:example user:michael groups:- system:authenticated - admins - authors Kubernetes APIServer 收到请求后，将基于 RBAC 规则进行判定，结果还是返回一个 SubjectAccessReview 对象。其中，会设置关键的 status 字段。 apiVersion:authorization.k8s.io/v1 kind:SubjectAccessReview status:allowed:truedenied:falsereason:\"rule foo allowed this request\" status.allowed - 表明是否允许 status.denied - 表明是否拒绝请求 Note allowed 与 denied 可能同时为 false，这表明 Kubernetes APIServer 无法对请求做出判断。这应该依靠自定义 APIServer 里的权限逻辑进行判断。 注意，处于性能方面考虑，委托授权机制在每个 Custom APIServer 中都维护了一个本地缓存： 默认缓存 1024 个授权条目。 所有通过的授权请求缓存过期时间为 5min。 所有拒绝的授权请求缓存过期时间为 30s。 Note 可以通过 \"–authorization-webhook-cache-authorized-ttl\" 和 \"–authorization-webhook-cache-unauthorized-ttl\" 来进行配置。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:2:4","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"3 多版本类型实现 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:3:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"3.1 处理流程 每个 APIServer 都可以提供多个资源和版本的接口。为了让一个资源的多版本共存称为可能，APIServer 需要将资源在多个版本之间进行转换。 APIServer 在真正实现 API 逻辑时，会使用一个 内部版本internal version，也称为 中枢版本hub version。它可以用作每个版本都可以与之转换的中间版本，所以内部 API 逻辑都是根据中枢版本实现的。 所以，APIServer 中存在三个版本的概念： External Version - 能够处理的版本，例如 v1beta1 v1 等； Internal Version - 用于 External Version 之间转换的中间版本； Storage Version - 保存到后端存储（etcd）的版本，属于 External Version 其中一个（通过 CRD 定义设置）； 下图展示了 APIServer 在一个 API 请求的生命周期中，三个版本之间的转换情况： 用户发送一个特定版本的请求（例如 v1，但是可以是任何支持的版本）。[External Version] APIServer 将请求解码，并转换为内部版本。[External Version -\u003e Internal Version] APIServer 对内部版本进行准入检测与验证。[Internal Version] 注册表中的 API 逻辑都是使用内部版本的。[Internal Version] etcd 读写或写入特定版本对象（例如使用 v2 作为存储版本），这个过程中，它需要与内部版本进行互转。[Internal Version -\u003e Storage Version] 结果转换为请求中的版本，例如 v1。[Internal Version -\u003e External Version] 从上图可以看到，一次写请求操作中，至少要做三次转换（步骤 2 5 6）。如果部署了 admission webhook，还会发生更多次的转换。 除了转换，下图中展示了默认值会何时处理，Default 填充未设置值字段的过程。默认值处理总是和转换一起出现，并且总是在用户请求、etcd 或者 admission webhook 时的 External Version 进行（也就是只在External Version 转换到 Internal Version 时进行），不会发生在中枢版本向外部版本转换过程中。 Note 可以看到，转换是至关重要的，所有的两个方向的转换都必须是正确的，即是 双程的roundtrippable。roundtrippable 意味着我们可以对任意的值在整个版本中来回转换，而不丢弃任何信息。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:3:1","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"3.2 Internal Version 类型实现 我们看下 Internal Version 实现，位于 “pkg/apis/\u003cgroup\u003e/types.go” 文件中： // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object type Flunder struct { metav1.TypeMeta metav1.ObjectMeta Spec FlunderSpec Status FlunderStatus } // +genclient // +genclient:nonNamespaced // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // Fischer is an example type with a list of disallowed Flunder.Names type Fischer struct { metav1.TypeMeta metav1.ObjectMeta // DisallowedFlunders holds a list of Flunder.Names that are disallowed. DisallowedFlunders []string } // ... 没有标签 注意，Internal Version 类型是没有 JSON 和 protobuf 标签的。因为 JSON 标签会被一些生成器用于探测一个 types.go 文件是对应内部版本还是外部版本，所以不能包含标签。 在将 Internal Version 类型注册到 Scheme 时，我们可以看到其注册的 GroupVersion 是一个特殊的 runtime.APIVersionInternal： var SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: runtime.APIVersionInternal} // Adds the list of known types to the given scheme. func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, \u0026Flunder{}, \u0026FlunderList{}, \u0026Fischer{}, \u0026FischerList{}, ) return nil } 除了基本的类型与对应的 DeepCopy 相关的实现，Internal Version 类型不需要更多的代码实现了。Convert 与 Default 相关都是针对特定的External Version 类型需要实现的。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:3:2","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"3.3 External Version 类型实现 在 sample-apiserver 中实现了 v1alpha1 v1beta1 两个 External Version。我们以 v1alpha1 版本主要说明。 一个 External Version 类型放置于 “pkg/apis/\u003cgroup\u003e/\u003cversion\u003e/types.go” 文件中。当然，其是必须包含 JSON 标签的。 // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object type Flunder struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` Spec FlunderSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` Status FlunderStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } // +genclient // +genclient:nonNamespaced // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object type Fischer struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` // DisallowedFlunders holds a list of Flunder.Names that are disallowed. DisallowedFlunders []string `json:\"disallowedFlunders,omitempty\" protobuf:\"bytes,2,rep,name=disallowedFlunders\"` } // ... 与 Internal Version 不同了，其注册到 Scheme 的 GroupVersion 就是对应版本的 string 了。 var SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: \"v1alpha1\"} func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, \u0026Flunder{}, \u0026FlunderList{}, \u0026Fischer{}, \u0026FischerList{}, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } 3.3.1 Convert 在 3.1 处理流程 中看到，APIServer 处理请求过程中有着多次 External Version 与 Internal Version 双向的转换，因此其需要注册对应的转换函数，其注册函数会自动生成位于 “pkg/apis/\u003cgroup\u003e/\u003cversion\u003e/zz_generated.conversion.go”。 可以看到，其类型转换函数通过 Scheme.AddGeneratedConversionFunc() 接口注册到 Scheme 中。而在 APIServer 抽象实现中，会按照 3.1 处理流程 中的请求处理流程调用 Scheme.Convert() 接口进行转换。 func init() { // 注册转换函数 localSchemeBuilder.Register(RegisterConversions) } // RegisterConversions adds conversion functions to the given scheme. // Public to allow building arbitrary schemes. func RegisterConversions(s *runtime.Scheme) error { // Scheme.AddGeneratedConversionFunc(a, b interface{}, fn conversion.ConversionFunc) 用于注册一个转换函数 // 表明 fn 会用于将 a -\u003e b // // fn 为 type ConversionFunc func(a, b interface{}, scope Scope) error // 实现实际的将 a -\u003e b，scope 表示范围（大部分情况不使用） if err := s.AddGeneratedConversionFunc((*Fischer)(nil), (*wardle.Fischer)(nil), func(a, b interface{}, scope conversion.Scope) error { return Convert_v1alpha1_Fischer_To_wardle_Fischer(a.(*Fischer), b.(*wardle.Fischer), scope) }); err != nil { return err } if err := s.AddGeneratedConversionFunc((*wardle.Fischer)(nil), (*Fischer)(nil), func(a, b interface{}, scope conversion.Scope) error { return Convert_wardle_Fischer_To_v1alpha1_Fischer(a.(*wardle.Fischer), b.(*Fischer), scope) }); err != nil { return err } if err := s.AddGeneratedConversionFunc((*FischerList)(nil), (*wardle.FischerList)(nil), func(a, b interface{}, scope conversion.Scope) error { return Convert_v1alpha1_FischerList_To_wardle_FischerList(a.(*FischerList), b.(*wardle.FischerList), scope) }); err != nil { return err } if err := s.AddGeneratedConversionFunc((*wardle.FischerList)(nil), (*FischerList)(nil), func(a, b interface{}, scope conversion.Scope) error { return Convert_wardle_FischerList_To_v1alpha1_FischerList(a.(*wardle.FischerList), b.(*FischerList), scope) }); err != nil { return err } if err := s.AddGeneratedConversionFunc((*Flunder)(nil), (*wardle.Flunder)(nil), func(a, b interface{}, scope conversion.Scope) error { return Convert_v1alpha1_Flunder_To_wardle_Flunder(a.(*Flunder), b.(*wardle.Flunder), scope) }); err != nil { return err } if err := s.AddGeneratedConversionFunc((*wardle.Flunder)(nil), (*Flunder)(nil), func(a, b interface{}, scope conversion.Scope) error { return Convert_wardle_Flunder_To_v1alpha1_Flunder(a.(*wardle.Flunder), b.(*Flunder), scope) }); err != nil { return err } if err := s.AddGeneratedConversionFunc((*FlunderList)(nil), (*wardle.FlunderList)(nil), func(a, b interface{}, scope conversion.Scope) error { return Convert_v1alpha1_FlunderList_To_wardle_FlunderList(a.(*FlunderList), b.(*wardle.Flund","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:3:3","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"3.4 向 Scheme 注册 最后在说明一下类型、Convert 函数、Default 函数是如何注册到 Scheme 中的。这三类都会注册到代码生成的一个 SchemeBuilder 对象，其作用就是用于调用注册的 funcs (scheme *Scheme) error 去设置 Scheme。 而前面看到的各个 Register 函数都是注册到 SchemeBuilder 对象中。 var ( SchemeBuilder runtime.SchemeBuilder localSchemeBuilder = \u0026SchemeBuilder // AddToScheme is a common registration function for mapping packaged scoped group \u0026 version keys to a scheme AddToScheme = localSchemeBuilder.AddToScheme ) func init() { // We only register manually written functions here. The registration of the // generated functions takes place in the generated files. The separation // makes the code compile even when the generated files are missing. localSchemeBuilder.Register(addKnownTypes, addDefaultingFuncs) } // addKnownTypes 注册类型 func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, \u0026Flunder{}, \u0026FlunderList{}, \u0026Fischer{}, \u0026FischerList{}, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } // addDefaultingFuncs 注册 Default 函数 func addDefaultingFuncs(scheme *runtime.Scheme) error { return RegisterDefaults(scheme) } func init() { localSchemeBuilder.Register(RegisterConversions) // 注册 Convert 函数 } 所有版本的 AddToScheme 都是在 “pkg/apis/\u003cgroup\u003e/install/install.go” 中一个辅助函数 Install 中注册： // Install registers the API group and adds types to a scheme func Install(scheme *runtime.Scheme) { // 注册 Internal Version utilruntime.Must(wardle.AddToScheme(scheme)) // 注册 v1beta1 utilruntime.Must(v1beta1.AddToScheme(scheme)) // 注册 v1alpha1 utilruntime.Must(v1alpha1.AddToScheme(scheme)) // 设置优先级（优先级用于客户端选择版本） utilruntime.Must(scheme.SetVersionPriority(v1beta1.SchemeGroupVersion, v1alpha1.SchemeGroupVersion)) } Install 函数在 APIServer 包中会自动调用，也就是说，程序运行一开始就将所有 Scheme 相关的注册处理好了。 var ( // Scheme defines methods for serializing and deserializing API objects. Scheme = runtime.NewScheme() ) func init() { install.Install(Scheme) // ... } ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:3:4","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"4 Registry 与 Strategy APIServer 本质上就是一个 HTTP Server，client 通过访问以 Resource 为单位的 RESTful 接口来进行交互。在 Custom APIServer 中，为了让 Custom APIServer 能够处理 CR 相关的接口，我们需要注册相关接口的 HTTP Handle。 k8s.io/apiserver 库提供了该功能的高度封装，其核心就是 Registry。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:4:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"4.1 APIServer 对象 在 Custom APIServer 实现中，不需要自己定义 HTTP Server，可以直接内嵌提供的 GenericAPIServer 类。 import genericapiserver \"k8s.io/apiserver/pkg/server\" // WardleServer contains state for a Kubernetes cluster master/api server. type WardleServer struct { GenericAPIServer *genericapiserver.GenericAPIServer } 在整个 Custom APIServer 程序启动流程中，会调用 GenericAPIServer.PrepareRun().Run() 运行一个 HTTP Server。（整个启动流程很长，后面单独说明） 在创建 WardleServer 过程中，我们会创建 APIGroupInfo 对象，而将其注册到 GenericAPIServer 对象中，可以理解为完成了某个 Group 下所有 Resource 对应的 HTTP Handle 注册。 func (c completedConfig) New() (*WardleServer, error) { // CompletedConfig 得到一个 GenericAPIServer genericServer, err := c.GenericConfig.New(\"sample-apiserver\", genericapiserver.NewEmptyDelegate()) if err != nil { return nil, err } // Custom APIServer 对象 s := \u0026WardleServer{ GenericAPIServer: genericServer, } // 构建 APIGroupInfo apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(wardle.GroupName, Scheme, metav1.ParameterCodec, Codecs) // 构建 APIGroup v1alpha1storage := map[string]rest.Storage{} v1alpha1storage[\"flunders\"] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) v1alpha1storage[\"fischers\"] = wardleregistry.RESTInPeace(fischerstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\"v1alpha1\"] = v1alpha1storage v1beta1storage := map[string]rest.Storage{} v1beta1storage[\"flunders\"] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\"v1beta1\"] = v1beta1storage // 注册 APIGroup 到 Kubernetes APIServer if err := s.GenericAPIServer.InstallAPIGroup(\u0026apiGroupInfo); err != nil { return nil, err } return s, nil } Note 这里暂时不去深入了解 APIGroupInfo 是如何映射到 HTTP Handle 的，因为这是 k8s.io/apiserver 库提供的封装。（其实我也不懂） ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:4:1","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"4.2 APIGroupInfo APIGroupInfo 能够变为一个 Group 下所有 Version 所有 Resource 的的 HTTP Handle。因此，我们可以想到，我们提供给 APIGroupInfo 什么信息： 该 Group 支持的所有 Version，以及每个 Version 支持的所有 Resource； 由调用 genericapiserver.NewDefaultAPIGroupInfo() 传入的 Group + Scheme + Codecs 提供。这样，处理 HTTP 请求时能够将数据序列化为特定的对象。 GroupVersionResource 维度的 RESTful HTTP Handle； 通过将 rest.Storage 注册到 APIGroupInfo.VersionedResourcesStorageMap 中。rest.Storage 为一个 RESTful HTTP Handle 的实现。 // 构建 APIGroupInfo apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(wardle.GroupName, Scheme, metav1.ParameterCodec, Codecs) // 构建 APIGroup // 一个 rest.Storage 对应了一个 HTTP API Endpoint // 即 /apis/\u003cgroup\u003e/\u003cversion\u003e/\u003cresource\u003e // 下面注册了两个 Endpoint // + /apis/wardle.example.com/v1alpha1/flunders // + /apis/wardle.example.com/v1alpha1/fischers v1alpha1storage := map[string]rest.Storage{} v1alpha1storage[\"flunders\"] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) v1alpha1storage[\"fischers\"] = wardleregistry.RESTInPeace(fischerstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\"v1alpha1\"] = v1alpha1storage // 注册到 APIGroupInfo v1alpha1 版本 // 下面注册了一个 Endpoint // + /apis/wardle.example.com/v1beta1/flunders v1beta1storage := map[string]rest.Storage{} v1beta1storage[\"flunders\"] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\"v1beta1\"] = v1beta1storage // 注册到 APIGroupInfo v1beta1 版本 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:4:2","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"4.3 rest.Storage 终于，我们看似来到了这个封装的最底层 rest.Storage，提供 REST 风格各个 Handle 的类。rest.Storage 本身的 interface 实现很简单，仅仅只需要提供 New() 方法就好了，也就是说，一个 Resource HTTP Endpoint 至少要提供一个 Create/Update 接口。 // Storage is a generic interface for RESTful storage services. // Resources which are exported to the RESTful API of apiserver need to implement this interface. It is expected // that objects may implement any of the below interfaces. type Storage interface { // New returns an empty object that can be used with Create and Update after request data has been put into it. // This object must be a pointer type for use with Codec.DecodeInto([]byte, runtime.Object) New() runtime.Object } 实现 Custom APIServer 时，为了能够支持多个 HTTP Endpoint，使用的是 rest.StandardStorage interface。 // StandardStorage is an interface covering the common verbs. Provided for testing whether a // resource satisfies the normal storage methods. Use Storage when passing opaque storage objects. type StandardStorage interface { Getter Lister CreaterUpdater GracefulDeleter CollectionDeleter Watcher } // Getter is an object that can retrieve a named RESTful resource. type Getter interface { // Get finds a resource in the storage by name and returns it. // Although it can return an arbitrary error value, IsNotFound(err) is true for the // returned error value err when the specified resource is not found. Get(ctx context.Context, name string, options *metav1.GetOptions) (runtime.Object, error) } // Lister is an object that can retrieve resources that match the provided field and label criteria. type Lister interface { // NewList returns an empty object that can be used with the List call. // This object must be a pointer type for use with Codec.DecodeInto([]byte, runtime.Object) NewList() runtime.Object // List selects resources in the storage which match to the selector. 'options' can be nil. List(ctx context.Context, options *metainternalversion.ListOptions) (runtime.Object, error) // TableConvertor ensures all list implementers also implement table conversion TableConvertor } // CreaterUpdater is a storage object that must support both create and update. // Go prevents embedded interfaces that implement the same method. type CreaterUpdater interface { Creater Update(ctx context.Context, name string, objInfo UpdatedObjectInfo, createValidation ValidateObjectFunc, updateValidation ValidateObjectUpdateFunc, forceAllowCreate bool, options *metav1.UpdateOptions) (runtime.Object, bool, error) } // Creater is an object that can create an instance of a RESTful object. type Creater interface { // New returns an empty object that can be used with Create after request data has been put into it. // This object must be a pointer type for use with Codec.DecodeInto([]byte, runtime.Object) New() runtime.Object // Create creates a new version of a resource. Create(ctx context.Context, obj runtime.Object, createValidation ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) } // GracefulDeleter knows how to pass deletion options to allow delayed deletion of a // RESTful object. type GracefulDeleter interface { // Delete finds a resource in the storage and deletes it. // The delete attempt is validated by the deleteValidation first. // If options are provided, the resource will attempt to honor them or return an invalid // request error. // Although it can return an arbitrary error value, IsNotFound(err) is true for the // returned error value err when the specified resource is not found. // Delete *may* return the object that was deleted, or a status object indicating additional // information about deletion. // It also returns a boolean which is set to true if the resource was instantly // deleted or false if it will be deleted asynchronously. Delete(ctx context.Context, name string, deleteValidation ValidateObjectFunc, options *metav1.DeleteOptions) (runtime.Object, bool, error) } // CollectionDeleter is an object that can delete a collection // of RESTful resources. type CollectionDelet","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:4:3","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"4.4 registry.Store 与 Strategy 要实现 rest.Storage interface？Kubernetes 依旧提供了实现了 rest.Storage 的类 registry.Store。我们真正要做的就是将一个回调函数设置到 Store 对象中。 // Store implements k8s.io/apiserver/pkg/registry/rest.StandardStorage. It's // intended to be embeddable and allows the consumer to implement any // non-generic functions that are required. This object is intended to be // copyable so that it can be used in different ways but share the same // underlying behavior. // // All fields are required unless specified. // // The intended use of this type is embedding within a Kind specific // RESTStorage implementation. This type provides CRUD semantics on a Kubelike // resource, handling details like conflict detection with ResourceVersion and // semantics. The RESTCreateStrategy, RESTUpdateStrategy, and // RESTDeleteStrategy are generic across all backends, and encapsulate logic // specific to the API. // // TODO: make the default exposed methods exactly match a generic RESTStorage type Store struct { // ... } 我们仅仅关注在如何使用 rest.Store，以 Flunder 的 REST 实现为例: // REST implements a RESTStorage for API services against etcd type REST struct { *genericregistry.Store } func NewREST(scheme *runtime.Scheme, optsGetter generic.RESTOptionsGetter) (*registry.REST, error) { strategy := NewStrategy(scheme) store := \u0026genericregistry.Store{ NewFunc: func() runtime.Object { return \u0026wardle.Flunder{} }, NewListFunc: func() runtime.Object { return \u0026wardle.FlunderList{} }, PredicateFunc: MatchFlunder, DefaultQualifiedResource: wardle.Resource(\"flunders\"), CreateStrategy: strategy, UpdateStrategy: strategy, DeleteStrategy: strategy, // TODO: define table converter that exposes more than name/creation timestamp TableConvertor: rest.NewDefaultTableConvertor(wardle.Resource(\"flunders\")), } options := \u0026generic.StoreOptions{RESTOptions: optsGetter, AttrFunc: GetAttrs} if err := store.CompleteWithOptions(options); err != nil { return nil, err } return \u0026registry.REST{store}, nil } Store 已经实现了 Creater、Updater 等 interface，确切的说是 rest.StandardStorage。也就是说，Store 将最基本的 CRUD 逻辑实现了，我们需要实现的就是扩展点的函数。 每个操作的扩展点就称为 Strategy（总算看到了标题）。看下我们自己实现的 Strategy: type flunderStrategy struct { runtime.ObjectTyper names.NameGenerator } func (flunderStrategy) NamespaceScoped() bool { return true } func (flunderStrategy) PrepareForCreate(ctx context.Context, obj runtime.Object) { } func (flunderStrategy) PrepareForUpdate(ctx context.Context, obj, old runtime.Object) { } // Validate 用于对对象进行验证 func (flunderStrategy) Validate(ctx context.Context, obj runtime.Object) field.ErrorList { flunder := obj.(*wardle.Flunder) return validation.ValidateFlunder(flunder) } // WarningsOnCreate returns warnings for the creation of the given object. func (flunderStrategy) WarningsOnCreate(ctx context.Context, obj runtime.Object) []string { return nil } func (flunderStrategy) AllowCreateOnUpdate() bool { return false } func (flunderStrategy) AllowUnconditionalUpdate() bool { return false } func (flunderStrategy) Canonicalize(obj runtime.Object) { } func (flunderStrategy) ValidateUpdate(ctx context.Context, obj, old runtime.Object) field.ErrorList { return field.ErrorList{} } // WarningsOnUpdate returns warnings for the given update. func (flunderStrategy) WarningsOnUpdate(ctx context.Context, obj, old runtime.Object) []string { return nil } ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:4:4","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"4.5 总结 兜兜绕绕了一圈，我们总结一下这个复杂的结构： 整个 APIServer 就是一个 HTTP Server； HTTPServer 由多个 APIGroupInfo，每个 APIGroupInfo 代表了一组 HTTP 接口，即 \"/apis/\u003cgroup\u003e\"； 每个 APIGroupInfo 包含多个版本，即 \"/apis/\u003cgroup\u003e/\u003cversion\u003e\"； 每个版本可以包含多个 Resource 对应的 rest.Storage 实现，一个 rest.Storage 就是该 Resource 的 REST HTTP Handle，即 \"/apis/\u003cgroup\u003e/\u003cversion\u003e/\u003cresource\u003e\"。 registry.Store 是 Kubernetes 库提供的实现了 rest.Storage 的类，其提供多个扩展点，扩展点就称为 Strategy。 我们再回头看，其实这样复杂的结构实现就是为能够实现了 /apis/\u003cgroup\u003e/\u003cversion\u003e/\u003cresource\u003e 以及处理函数的抽象。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:4:5","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"5 Validate 准入过程中的 Validate 能够实现针对字段动态验证，比 CRD 定义中的静态验证更加灵活。由下图可见，Validate 过程是在 Mutate Plugin 与 Validate Plugin 之间完成的。 因此 Validate 只需要为 Internal Version 实现一次，不许为各个 External Version 分别实现。Registry 与 Strategy 为我们封装了上面的真个 Resource Handler，因此 Validate 的入口其实就是 Strategy 中对于 Create/Update 的检查。 type flunderStrategy struct { runtime.ObjectTyper names.NameGenerator } // ... // Validate 用于对 Create Object 时进行验证 func (flunderStrategy) Validate(ctx context.Context, obj runtime.Object) field.ErrorList { flunder := obj.(*wardle.Flunder) return validation.ValidateFlunder(flunder) } 后续就是我们自己实现的 Validate 逻辑，这里使用了 field.ErrorList 这样的方式来方便的展示出错误的字段。 // ValidateFlunder validates a Flunder. func ValidateFlunder(f *wardle.Flunder) field.ErrorList { allErrs := field.ErrorList{} // 嵌套的属性认证 allErrs = append(allErrs, ValidateFlunderSpec(\u0026f.Spec, field.NewPath(\"spec\"))...) // field.NewPath(记录属性路径) return allErrs } 由于 Validate 逻辑都是在代码中编写的，所以你可以对比多个字段来进行 Validate，灵活性页更高。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:5:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"6 Admission 还是下图，每个请求在经过反序列化、默认值处理、转换为 Internal Version 后，都会经过 Admission Plugin Chain 处理（图中的 Admission 一块）。 Admission Plugin 因为两个阶段，也就是可以分为两类插件： Mutate Plugin Validate Plugin 一个 Admission Plugin 可以同时属于这两类，那么在 Admission Controll 过程中，该插件会被调用两次（要么同时启动、要么同时禁用）。 Mutate 阶段，所有 Mutate Plugin 会依次调用； Validate 阶段，所有 Validate Plugin 会被调用（可能并发地调用）； Webhook Admission 这里所说的都是继承到 APIServer 代码内部的 Admission Plugin，Kubernetes 也提供了外部基于 Webhook 的 Admission Control，也就是图中调用外部服务的地方。 下面从入口开始看 Admission Plugin 如何实现的。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:6:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"6.1 Admission Plugin 选项 在 APIServer 中，所有的 Admission Plugin 以及相关选项都记录在 APIServer Option RecommendedOptions.Admission 字段中，其是 AdmissionOptions 类型。 // AdmissionOptions holds the admission options type AdmissionOptions struct { // RecommendedPluginOrder holds an ordered list of plugin names we recommend to use by default RecommendedPluginOrder []string // DefaultOffPlugins is a set of plugin names that is disabled by default DefaultOffPlugins sets.String // EnablePlugins indicates plugins to be enabled passed through `--enable-admission-plugins`. EnablePlugins []string // DisablePlugins indicates plugins to be disabled passed through `--disable-admission-plugins`. DisablePlugins []string // ConfigFile is the file path with admission control configuration. ConfigFile string // Plugins contains all registered plugins. Plugins *admission.Plugins // Decorators is a list of admission decorator to wrap around the admission plugins Decorators admission.Decorators } RecommendedPluginOrder - Plugin 执行顺序 EnablePlugins - 通过命令行参数 --enable-admission-plugins 打开的 Plugin DisablePlugins - 通过命令行参数 --disable-admission-plugins 关闭的 Plugin Plugins - 所有注册了的 Plugin 与 Kubernetes APIServer 包含的 Plugin 不同，NewAdmissionOptions() 仅仅包含三个默认的 Plugin： NamespaceLifecycle - 根据 namespace 阶段来执行一些约束，包括：禁止在删除中的 namespace 中创建新对象，对不存在的 namespace 请求拒绝，禁止删除 default、kube-system、kube-public 三个 namespace。 ValidatingAdmissionWebhook - 允许注册与使用 Validating Admission Webhook。 MutatingAdmissionWebhook - 允许注册与使用 Mutating Admission Webhook。 Note Kubernetes APIServer 提供了许多的 Admission Plugin，具体见 使用准入控制器 代码中，在 APIServer 启动过程中的 Complete() 时会进行自定义 Plugin 的注册，调用 *admission.Plugins.Register() 即可 // Complete fills in fields required to have valid data func (o *WardleServerOptions) Complete() error { // 注册 BanFlunder 的 Admission Plugin // register admission plugins banflunder.Register(o.RecommendedOptions.Admission.Plugins) // 记录到 Plugins 中 // add admission plugins to the RecommendedPluginOrder o.RecommendedOptions.Admission.RecommendedPluginOrder = append(o.RecommendedOptions.Admission.RecommendedPluginOrder, \"BanFlunder\") return nil } // banflunder.Register // Register registers a plugin func Register(plugins *admission.Plugins) { plugins.Register(\"BanFlunder\", func(config io.Reader) (admission.Interface, error) { return New() }) } ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:6:1","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"6.2 Admission Plugin Interface 要实现一个 Admission Plugin 需要实现以下接口： addmission.Interface - 用于注册到 Plugin Chain 中 addmission.MutatingInterface（可选）- 能够在 Mutate 阶段被调用 addmission.ValidationInterface（可选）- 能够在 Validate 阶段被调用 这三个接口都在 “k8s.io/apiserver/pkg/admission” 包中： // Interface is an abstract, pluggable interface for Admission Control decisions. type Interface interface { // Handles returns true if this admission controller can handle the given operation // where operation can be one of CREATE, UPDATE, DELETE, or CONNECT Handles(operation Operation) bool } type MutationInterface interface { Interface // Admit makes an admission decision based on the request attributes. // Context is used only for timeout/deadline/cancellation and tracing information. Admit(ctx context.Context, a Attributes, o ObjectInterfaces) (err error) } // ValidationInterface is an abstract, pluggable interface for Admission Control decisions. type ValidationInterface interface { Interface // Validate makes an admission decision based on the request attributes. It is NOT allowed to mutate // Context is used only for timeout/deadline/cancellation and tracing information. Validate(ctx context.Context, a Attributes, o ObjectInterfaces) (err error) } Interface.Handles() 根据操作对请求进行过滤，因此可以针对特定的请求才进行 Mutate 与 Validate 阶段； MutationInterface.Admit() 验证请求，并允许变更对象； ValidationInterface.Validate() 仅仅验证请求； ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:6:2","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"6.3 实现 Admission Plugin 好了，我们已经知道在哪里注册 Plugin，以及 Plugin 应该实现哪些接口了，我们开始看一个 Plugin 的实现： // New creates a new ban flunder admission plugin func New() (*DisallowFlunder, error) { return \u0026DisallowFlunder{ Handler: admission.NewHandler(admission.Create), }, nil } // DisallowFlunder is a ban flunder admission plugin type DisallowFlunder struct { *admission.Handler lister listers.FischerLister } // SetInternalWardleInformerFactory gets Lister from SharedInformerFactory. // The lister knows how to lists Fischers. func (d *DisallowFlunder) SetInternalWardleInformerFactory(f informers.SharedInformerFactory) { d.lister = f.Wardle().V1alpha1().Fischers().Lister() d.SetReadyFunc(f.Wardle().V1alpha1().Fischers().Informer().HasSynced) } // ValidateInitialization checks whether the plugin was correctly initialized. func (d *DisallowFlunder) ValidateInitialization() error { if d.lister == nil { return fmt.Errorf(\"missing fischer lister\") } return nil } admission.Handler - 内嵌 admission.Handler 的对象实现了 Interface.Handles() 方法，我们只需要注册我们感兴趣的 Operation（Create）； SetInternalWardleInformerFactory - 传入需要的 Lister Admit() 实现就很简单了，进行一些检查： func (d *DisallowFlunder) Admit(ctx context.Context, a admission.Attributes, o admission.ObjectInterfaces) error { // 检查 GVK // we are only interested in flunders if a.GetKind().GroupKind() != wardle.Kind(\"Flunder\") { return nil } // 等待 Lister 准备好 if !d.WaitForReady() { return admission.NewForbidden(a, fmt.Errorf(\"not yet ready to handle request\")) } // 进行业务上的 Validate metaAccessor, err := meta.Accessor(a.GetObject()) if err != nil { return err } flunderName := metaAccessor.GetName() fischers, err := d.lister.List(labels.Everything()) if err != nil { return err } // 黑名单检查 for _, fischer := range fischers { for _, disallowedFlunder := range fischer.DisallowedFlunders { if flunderName == disallowedFlunder { return errors.NewForbidden( a.GetResource().GroupResource(), a.GetName(), fmt.Errorf(\"this name may not be used, please change the resource name\"), ) } } } return nil } ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:6:3","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"6.4 基础组件 Admission Plugin 通常需要 client 和 Informer 去访问其他资源，可以在插件的初始化逻辑中准备好这些基础组件。 库中提供了 admission.PluginInitializer interface 抽象来进行 Plugin 的初始化操作，初始化过程中会对所有的 Plugin 调用每个 PluginInitializer.Initialize()。 // PluginInitializer is used for initialization of shareable resources between admission plugins. // After initialization the resources have to be set separately type PluginInitializer interface { Initialize(plugin Interface) } 根据前面的 Plugin 实现中的 SetInternalWardleInformerFactory() 接口，我们只需要实现了自己的 Initializer 来传递 Informer 即可： type pluginInitializer struct { informers informers.SharedInformerFactory } // New creates an instance of wardle admission plugins initializer. func New(informers informers.SharedInformerFactory) pluginInitializer { return pluginInitializer{ informers: informers, } } // Initialize checks the initialization interfaces implemented by a plugin // and provide the appropriate initialization data func (i pluginInitializer) Initialize(plugin admission.Interface) { if wants, ok := plugin.(WantsInternalWardleInformerFactory); ok { wants.SetInternalWardleInformerFactory(i.informers) // 将 Informer 传递给 Plugin } } // WantsInternalWardleInformerFactory defines a function which sets InformerFactory for admission plugins that need it type WantsInternalWardleInformerFactory interface { SetInternalWardleInformerFactory(informers.SharedInformerFactory) admission.InitializationValidator } WantsInternalWardleInformerFactory - 仅仅是特针对于 Informer 的 interface 实现，也是为了单向的抽象，这样如果多个 Plugin 依赖于同一个 Informer，那么至需要创建一个 Initializer 最后，所有 admission.PluginInitializer 记录到 RecommendedOptions.ExtraAdmissionInitializers 字段，这样就会自动调用进行初始化。 o.RecommendedOptions.ExtraAdmissionInitializers = func(c *genericapiserver.RecommendedConfig) ([]admission.PluginInitializer, error) { // 创建 ClientSet client, err := clientset.NewForConfig(c.LoopbackClientConfig) if err != nil { return nil, err } // 创建 CustomResource Informer，并记录到 Option 中 informerFactory := informers.NewSharedInformerFactory(client, c.LoopbackClientConfig.Timeout) o.SharedInformerFactory = informerFactory // 构建 Admission Plugin Initializer，传递了 Informer // Initializer.Initialize 将 Informer 传递给了 Admission Plugin return []admission.PluginInitializer{wardleinitializer.New(informerFactory)}, nil } ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:6:4","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"6.5 总结 又是一个复杂的框架搬的实现，看下重要的几个点： RecommendedOptions.Admission 记录着所有需要执行的 Admission Plugin，自己编写的也是注册到其中。 RecommendedOptions.ExtraAdmissionInitializers 记录着初始化 Plugin 的实现，用于在代码中向各个 Plugin 传递 Informer 等基础组件。 我们需要编写类实现 admission.Interface + admission.MutationInterface + admission.ValidationInterface。在 Admit() 函数中进行 Mutate，在 Validate() 函数中进行 Validate。 如果我们的 Plugin 需要使用 Informer 这样的组件，还需要编写一个类实现 admission.PluginInitializer，用于传递 Informer。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:6:5","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"7 初始化与启动 看 main 函数，一切的入口是一个 Option: func main() { // 初始化日志 logs.InitLogs() defer logs.FlushLogs() // 注册 SIGTERM 与 SIGINT 信号 stopCh := genericapiserver.SetupSignalHandler() // 构建 Option options := server.NewWardleServerOptions(os.Stdout, os.Stderr) // 得到 cmd 命令行 cmd := server.NewCommandStartWardleServer(options, stopCh) cmd.Flags().AddGoFlagSet(flag.CommandLine) if err := cmd.Execute(); err != nil { klog.Fatal(err) } } NewCommandStartWardleServer 得到一个 cobra.Command，所以其真正的运行入口在这里面： // NewCommandStartWardleServer provides a CLI handler for 'start master' command // with a default WardleServerOptions. func NewCommandStartWardleServer(defaults *WardleServerOptions, stopCh \u003c-chan struct{}) *cobra.Command { o := *defaults cmd := \u0026cobra.Command{ Short: \"Launch a wardle API server\", Long: \"Launch a wardle API server\", RunE: func(c *cobra.Command, args []string) error { if err := o.Complete(); err != nil { return err } if err := o.Validate(args); err != nil { return err } if err := o.RunWardleServer(stopCh); err != nil { return err } return nil }, } // 注册命令行参数，Option 中所有字段都可以使用命令行参数配置 flags := cmd.Flags() o.RecommendedOptions.AddFlags(flags) utilfeature.DefaultMutableFeatureGate.AddFlag(flags) return cmd } ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:7:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"7.1 Options 启动 Server 的第一个阶段是得到 Option，代码中创建的是 WardleServerOptions，其最重要的就是包含了 RecommendedOptions，这是 k8s.io/apiserver 库提供的包含所有最基本的 APIServer 配置。 // WardleServerOptions contains state for master/api server type WardleServerOptions struct { // RecommendedOptions 记录官方的配置，包含大多数 APIServer 指定的配置 RecommendedOptions *genericoptions.RecommendedOptions SharedInformerFactory informers.SharedInformerFactory StdOut io.Writer StdErr io.Writer } // RecommendedOptions contains the recommended options for running an API server. // If you add something to this list, it should be in a logical grouping. // Each of them can be nil to leave the feature unconfigured on ApplyTo. type RecommendedOptions struct { Etcd *EtcdOptions // 后端存储相关 SecureServing *SecureServingOptionsWithLoopback // HTTPS 相关配置 Authentication *DelegatingAuthenticationOptions Authorization *DelegatingAuthorizationOptions Audit *AuditOptions // 审计相关，默认关闭，开启后可以输出审计日志或者发送审计事件到外部后端系统 Features *FeatureOptions // 开启或禁用某些 Alpha 或 Beta 功能 CoreAPI *CoreAPIOptions // 访问 Kubernetes APIServer 的 kubeconfig 文件路径 // FeatureGate is a way to plumb feature gate through if you have them. FeatureGate featuregate.FeatureGate // ExtraAdmissionInitializers is called once after all ApplyTo from the options above, to pass the returned // admission plugin initializers to Admission.ApplyTo. ExtraAdmissionInitializers func(c *server.RecommendedConfig) ([]admission.PluginInitializer, error) Admission *AdmissionOptions // API Server Egress Selector is used to control outbound traffic from the API Server EgressSelector *EgressSelectorOptions // Traces contains options to control distributed request tracing. Traces *TracingOptions } 我们重点关注使用 RecommendedOptions，使用提供的 genericoptions.NewRecommendedOptions() 来创建： const defaultEtcdPathPrefix = \"/registry/wardle.example.com\" // NewWardleServerOptions returns a new WardleServerOptions func NewWardleServerOptions(out, errOut io.Writer) *WardleServerOptions { o := \u0026WardleServerOptions{ // NewRecommendedOptions 创建 APIServer 推荐的配置，大多数都包含了默认的配置 RecommendedOptions: genericoptions.NewRecommendedOptions( defaultEtcdPathPrefix, // 存在 Etcd 中的 path 前缀 apiserver.Codecs.LegacyCodec(v1alpha1.SchemeGroupVersion), // 注册解码器 ), StdOut: out, StdErr: errOut, } o.RecommendedOptions.Etcd.StorageConfig.EncodeVersioner = runtime.NewMultiGroupVersioner(v1alpha1.SchemeGroupVersion, schema.GroupKind{Group: v1alpha1.GroupName}) return o } L13 - 定义 ETCD 存储的版本 7.1.1 Complete Option 启动 Server 的第一步，就是调用 WardleServerOptions.Complete()，其中主要作用就是注册自定义的 Admission Plugin： // Complete fills in fields required to have valid data func (o *WardleServerOptions) Complete() error { // 注册 BanFlunder 的 Admission Plugin // register admission plugins banflunder.Register(o.RecommendedOptions.Admission.Plugins) // 记录到 Plugins 中 // add admission plugins to the RecommendedPluginOrder o.RecommendedOptions.Admission.RecommendedPluginOrder = append(o.RecommendedOptions.Admission.RecommendedPluginOrder, \"BanFlunder\") return nil } 7.1.2 Validate Option 启动 Server 的第二步，调用 WardleServerOptions.Validate() 进行参数验证。因为我们没有使用额外的参数，所以直接调用 RecommendedOptions.Validate(): // Validate validates WardleServerOptions func (o WardleServerOptions) Validate(args []string) error { errors := []error{} // 验证 Option 的合法性，执行各个 XXXOptions.Validate() errors = append(errors, o.RecommendedOptions.Validate()...) return utilerrors.NewAggregate(errors) } 7.1.3 Run Server 启动 Server 的第三步，调用 WardleServerOptions.RunWardleServer 运行一个 Server。其流程为：Options -\u003e Config -\u003e APIServer，最后真正运行： // RunWardleServer starts a new WardleServer given WardleServerOptions func (o WardleServerOptions) RunWardleServer(stopCh \u003c-chan struct{}) error { // Option 转化为 APIServer Config config, err := o.Config() if err != nil { return err } // config.Complete() 填充一些默认的参数 // New() 得到一个 Server 对象，并注册 APIGroup 到 Kubernetes APIServer // NOTE: Custom APIServer 的关键就在这里，注册了一个 APIGroup，Kubernetes APIServer // 会根据注册的 APIGroup 来转发请求 server, err := config.Complete().New() if err != nil { return err } ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:7:1","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"7.2 Config 启动 Server 的第二阶段是 Config 类，与 Option 类似，我们会包含 k8s.io/apiserver 库提供的 RecommendedConfig: type Config struct { GenericConfig *genericapiserver.RecommendedConfig ExtraConfig ExtraConfig } RecommendedConfig 有着许多的配置项，所以我们不深入其实现，主要观察如何使用它。也就是如何从 Config，得到 APIServer 对象。 正如之前看到的，由 Option 转化为 Config 对象： // 新建 RecommendedConfig serverConfig := genericapiserver.NewRecommendedConfig(apiserver.Codecs) // 配置生成 OpenAPI 相关配置 serverConfig.OpenAPIConfig = genericapiserver.DefaultOpenAPIConfig(sampleopenapi.GetOpenAPIDefinitions, openapi.NewDefinitionNamer(apiserver.Scheme)) serverConfig.OpenAPIConfig.Info.Title = \"Wardle\" serverConfig.OpenAPIConfig.Info.Version = \"0.1\" // 将 Option 转换为 Config if err := o.RecommendedOptions.ApplyTo(serverConfig); err != nil { return nil, err } // 得到 APIServer Config // 包含: // 1. RecommendedConfig - 预定的通用 Config // 2. ExtraConfig - 自身可传递的一些 Config config := \u0026apiserver.Config{ GenericConfig: serverConfig, ExtraConfig: apiserver.ExtraConfig{}, } 从 Config 得到 Server 对象就很简单了： server, err := config.Complete().New() if err != nil { return err } 7.2.1 Complete Config Complete 过程依旧简单，执行 GenericConfig.Complete() 以及设置到版本号即可，返回一个 CompletedConfig 对象： // CompletedConfig embeds a private pointer that cannot be instantiated outside of this package. type CompletedConfig struct { *completedConfig } func (cfg *Config) Complete() CompletedConfig { c := completedConfig{ cfg.GenericConfig.Complete(), \u0026cfg.ExtraConfig, } c.GenericConfig.Version = \u0026version.Info{ Major: \"1\", Minor: \"0\", } return CompletedConfig{\u0026c} } 为何使用新的对象 CompletedConfig? CompletedConfig 才包含 New() 方法，这样能保证 New APIServer 时一定是经过 Complete Config 的。 7.2.2 New APIServer New APIServer 过程在创建一个 APIServer 对象后，就完成了 HTTP API Handle 的注册了，而这就是最关键的地方： // New returns a new instance of WardleServer from the given config. func (c completedConfig) New() (*WardleServer, error) { // CompletedConfig 得到一个 GenericAPIServer genericServer, err := c.GenericConfig.New(\"sample-apiserver\", genericapiserver.NewEmptyDelegate()) if err != nil { return nil, err } // Custom APIServer 对象 s := \u0026WardleServer{ GenericAPIServer: genericServer, } // 构建 APIGroupInfo apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(wardle.GroupName, Scheme, metav1.ParameterCodec, Codecs) // 构建 APIGroup // 一个 rest.Storage 对应了一个 HTTP API Endpoint // 即 /apis/\u003cgroup\u003e/\u003cversion\u003e/\u003cresource\u003e // 下面注册了两个 Endpoint // + /apis/wardle.example.com/v1alpha1/flunders // + /apis/wardle.example.com/v1alpha1/fischers v1alpha1storage := map[string]rest.Storage{} v1alpha1storage[\"flunders\"] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) v1alpha1storage[\"fischers\"] = wardleregistry.RESTInPeace(fischerstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\"v1alpha1\"] = v1alpha1storage // 下面注册了一个 Endpoint // + /apis/wardle.example.com/v1beta1/flunders v1beta1storage := map[string]rest.Storage{} v1beta1storage[\"flunders\"] = wardleregistry.RESTInPeace(flunderstorage.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)) apiGroupInfo.VersionedResourcesStorageMap[\"v1beta1\"] = v1beta1storage // 注册 APIGroup 到 Kubernetes APIServer if err := s.GenericAPIServer.InstallAPIGroup(\u0026apiGroupInfo); err != nil { return nil, err } return s, nil } ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:7:2","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"7.3 Server 总算来到最后的阶段，Server 对象就是一个 APIServer 的实现了，我们只需要调用库提供的 GenericAPIServer 实现的 PrepareRun() 与 Run() 接口实现即可运行。 server, err := config.Complete().New() if err != nil { return err } // 注册一个 PostStart Hook // Hook 会在 HTTPs Server 启动并监听后被调用 server.GenericAPIServer.AddPostStartHookOrDie(\"start-sample-server-informers\", func(context genericapiserver.PostStartHookContext) error { // 启动原生对象的 SharedInformer config.GenericConfig.SharedInformerFactory.Start(context.StopCh) // 启动 Custom Resource Informer o.SharedInformerFactory.Start(context.StopCh) return nil }) // PrepareRun 执行启动前的准备工作 // Run 运行 HTTPs Server return server.GenericAPIServer.PrepareRun().Run(stopCh) ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:7:3","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"8 部署 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:8:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"8.1 部署清单 清单： APIService Service Deployment ServiceAccount + ClusterRole + ClusterRoleBinding 之前提到，APIService 对象向原生 Kubernetes APIServer 注册一个 Custom APISever。因此这是必须要部署的。 apiVersion:apiregistration.k8s.io/v1kind:APIServicemetadata:name:v1alpha1.wardle.example.comspec:insecureSkipTLSVerify:truegroup:wardle.example.comgroupPriorityMinimum:1000versionPriority:15service:name:apinamespace:wardleversion:v1alpha1 注意，测试环境我们将 spec.insecureSkipTLSVerify 设为 true，而生产环境不能这么做。 APIService 仅仅是让 Kubernetes APIServer 知晓 Custom APIServer 存在，为了能够转发请求，我们还需要部署 Custom APIServer 使用的 Service 对象。 apiVersion:v1kind:Servicemetadata:name:apinamespace:wardlespec:ports:- port:443protocol:TCPtargetPort:443selector:apiserver:\"true\" 运行我们 Custom APIServer 程序的 Deployment。 apiVersion:apps/v1kind:Deploymentmetadata:name:wardle-servernamespace:wardlelabels:apiserver:\"true\"spec:replicas:1selector:matchLabels:apiserver:\"true\"template:metadata:labels:apiserver:\"true\"spec:serviceAccountName:apiservercontainers:- name:wardle-server# build from staging/src/k8s.io/sample-apiserver/artifacts/simple-image/Dockerfile# or# docker pull k8s.gcr.io/e2e-test-images/sample-apiserver:1.17.4# docker tag k8s.gcr.io/e2e-test-images/sample-apiserver:1.17.4 kube-sample-apiserver:latestimage:kube-sample-apiserver:latestimagePullPolicy:Neverargs:[\"--etcd-servers=http://localhost:2379\"]- name:etcdimage:quay.io/coreos/etcd:v3.5.0 Note APIServer 是无状态的，所以如果你只部署一个 etcd 作为后端存储情况下，可以运行多个 Custom APIServer，并且不需要进行选举。 访问 Kubernetes APIServer 使用的 ServiceAccount： kind:ServiceAccountapiVersion:v1metadata:name:apiservernamespace:wardle ClusterRole 提供相关的访问权限，主要包括： namespace - 为了实现 Namespace 删除时，其下相关对象也被删除，需要 namespace 相关权限 admission webhook - 为了能够支持 admission webhook kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:aggregated-apiserver-clusterrolerules:- apiGroups:[\"\"]resources:[\"namespaces\"]verbs:[\"get\",\"watch\",\"list\"]- apiGroups:[\"admissionregistration.k8s.io\"]resources:[\"mutatingwebhookconfigurations\",\"validatingwebhookconfigurations\"]verbs:[\"get\",\"watch\",\"list\"] ClusterRoleBinding 连接 ClusterRole 与 ServiceAccount，并且还需要绑定一些预创建的 ClusterRole。 apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:sample-apiserver-clusterrolebindingroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:aggregated-apiserver-clusterrolesubjects:- kind:ServiceAccountname:apiservernamespace:wardleapiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:wardle:system:auth-delegatorroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:system:auth-delegatorsubjects:- kind:ServiceAccountname:apiservernamespace:wardle# 为了代理认证和授权apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:name:wardle-auth-readernamespace:kube-systemroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:extension-apiserver-authentication-readersubjects:- kind:ServiceAccountname:apiservernamespace:wardle ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:8:1","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"8.2 配置证书 前面我们使用 APIService 中的 spec.insecureSkipTLSVerify 为 false 来让 Custom APIServer 与 Kubernetes APIServer 之前通信跳过 TLS 鉴权。 如果需要配置 TLS，可以在 APIService 中的 spec.caBundle 字段配置 Custom APISever 的根证书，这样 Kubernetes APIServer 就会使用该证书来对 Custom APISever 进行鉴权。 对于 Custom APIServer，我们创建好 Server 的证书与私钥后，创建一个 Secret 对象，然后将其挂载到 Pod 的 /var/run/apiserver/serving-cert/tls.{crt,key} 文件中。 Note /var/run/apiserver/serving-cert/tls.{crt,key} 是默认的放置 APIServer 证书与私钥的目录。 你可以通过命令行参数 –cert-dir “dir” 指定证书与私钥的目录。 或者，通过 –tls-cert-file “file” 与 –tls-private-key-file “file” 指定证书文件与私钥文件。 ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:8:2","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":"参考 k8s.io/apiserver 《Programming Kubernetes》 Configure the Aggregation Layer ","date":"2021-08-22","objectID":"/posts/cloud_computing/k8s_programming/6-custom-api-server/:9:0","tags":["k8s","云计算"],"title":"K8s 编程 - 6 - Custom APIServer","uri":"/posts/cloud_computing/k8s_programming/6-custom-api-server/"},{"categories":["Kubernetes 编程"],"content":" Kubernetes 编程系列 主要记录一些开发 Controller 所相关的知识，大部分内容来自于《Programming Kubernetes》（推荐直接阅读）。 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:0:0","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"1 打包 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:1:0","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"1.1 Helm Helm 已经成为 Kubernetes 包管理器的事实标准。通过引入一个 Chart 的概念，帮助安装与升级 Kubernetes 应用。 Chart 实质上是一个参数化的 YAML 文件，下面是 Chart 一个 template 文件的例子： apiVersion:apps/v1 kind:Deployment metadata:name:{{include \"flagger.fullname\" . }} # ... spec:replicas:1strategy:type:Recreate selector:matchLabels:app.kubernetes.io/name:{{template \"flagger.name\" . }} app.kubernetes.io/instance:{{.Release.Name }} template:metadata:labels:app.kubernetes.io/name:{{template \"flagger.name\" . }} app.kubernetes.io/instance: {{ .Release.Name }} spec:serviceAccountName:{{template \"flagger.serviceAccountName\" . }} containers:- name:flagger securityContext:readOnlyRootFilesystem:truerunAsUser:10001image:\"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" 可以看到，所有的变量都以 {{ .Some.value.here }} 的格式表示。 通过 helm install 命令，就可以安装一个 Chart。最简单的方式是安装官方发布的一些 Chart： # get the latest list of charts: $ helm repo update # install MySQL: $ helm install stable/mysql Released smiling-penguin # list running apps: $ helm ls NAME VERSION UPDATED STATUS CHART smiling-penguin 1 Wed Sep 28 12:59:46 2016 DEPLOYED mysql-0.1.0 # remove it: $ helm delete smiling-penguin Removed smiling-penguin 想要发布的你的 Controller，需要编写一个 Helm Chart 并上传到公共仓库。默认情况，你可以把它发布到 Artifact Hub 提供的公共仓库。 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:1:1","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"1.2 Kustomize Kustomize 提供了对资源文件的一种声明式的配置方法。 Kustomize 可以帮助你自定义 YAML 文件，而不用修改原始的文件。 例如，你先定义一个 kustomization.yaml 文件： imageTags:- name:quay.io/programming-kubernetes/cnat-operator newTag:0.1.0resources:- cnat-controller.yaml 然后创建原始的文件 cnat-controller.yaml 上： apiVersion:apps/v1beta1 kind:Deployment metadata:name:cnat-controller spec:replicas:1template:metadata:labels:app:cnat spec:containers:- name:custom-controller image:quay.io/programming-kubernetes/cnat-operator 使用 kustomize build 命令，cat-controller.yaml 文件不变，但是会输出如下配置： apiVersion:apps/v1beta1 kind:Deployment metadata:name:cnat-controller spec: replicas:1template:metadata:labels:app:cnat spec:containers:- name:custom-controller image:quay.io/programming-kubernetes/cnat-operator:0.1.0l ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:1:2","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"1.3 最佳打包实践 无论你用什么机制，下面这些经验都很适用： 提供一个合适的权限控制设置：一个专门的 ServiceAccount，ServiceAccount 绑定最小权限的 RBAC 许可。 确认你的 Controller 的控制范围，一个命名空间还是多个命名空间。参考 Twitter 中的讨论。 测试和分析你的 Controller 使得你对它运行所需要的资源有所概念。例如，Red Hat 在 OperatorHub 有着详细的规定与说明。 确保 CRD 和 Controller 有着齐全的文档。 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:1:3","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"2 生命周期管理 生命周期管理的基本思想：考虑从开发到发布到升级的整个链路，把这个过程的自动化做到极致。为了安装和升级 Operator，你需要一个专门的 Operator 来处理其他的 Operator。而这就是所谓的 Operator Lifecycle Manager，简称 OLM。 简而言之，OLM 提供了一种声明式的方式来安装和升级 Operator 以及其依赖。 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:2:0","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"3 部署 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:3:0","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"3.1 将权限设置正确 CRD Controller 需要一系列正确的权限，通过 RBAC 相关的设置来实现。 其重点是：你需要创建一个专门的 ServiceAccount 来运行 Controller，而不要使用 default ServiceAccount。 最佳实践是：定义一个拥有必要的 RBAC 规则的 ClusterRole 和一个 RoleBinding，并将其绑定到指定的命名空间。并且只赋予 Controller 工作的最小权限。 一些实践的经验是，Controller 不应该具有以下功能： 对代码里通常是只读资源的写权限。例如，如果你只需要 watch Service 和 Deployment，那么就不需要提供 create、update 等权限。 访问所有的 Secret，限制 Role 只能访问必要的 Secret。 写入 MutatingWebhookConfiguration 或 ValidatingWebhook 的权限。有了这两个原型都等于拥有了对集群里所有资源的访问权限一样。 写入 CustomResourceDefinition 的权限。CRD 的创建应该是由另外的进程创建，而不是控制器自身。 写入自己不负责管理的外部资源的 /status 子资源。 audit2rbac audit2rbac 工具通过分析审计日志来自动生成合适的权限，这可以帮助你进行权限的控制。 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:3:1","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"3.2 自动化构建与测试 当你的 Controller 开发处于稳定时，就可以且必须开始各种测试： Kubernetes 自身使用性能相关测试，以及 kboom 工具，都可以用于性能系相关测试。 浸泡测试，用于发现长期使用场景下是否出现资源泄漏问题。 作为最佳实践，这些测试应该是你的 CI 流程的一部分。建议看看这篇文章 Spawning Kubernetes Clusters in CI for Integration and E2E tests。 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:3:2","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"3.3 可观测性 3.3.1 日志 基于容器的部署方案中，日志一般都打印在 stdout 上，通过 kubectl logs 命令查看。 在 Kubernetes 代码库中，有两个广泛使用的方法： logger interface，httplog.go 里提供了该 interface，以及一个具体的实现（respLogger），它可以捕捉状态或错误等信息。 klog，Google glog 的一个分支，是 Kubernetes 项目中使用的结构化日志。 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:3:3","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":"参考 《Programming Kubernetes》 ","date":"2021-08-20","objectID":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/:4:0","tags":["k8s","云计算"],"title":"K8s 编程 - 5 - 发布 Operator","uri":"/posts/cloud_computing/k8s_programming/5-shipping-controllers/"},{"categories":["Kubernetes 编程"],"content":" Kubernetes 编程系列 主要记录一些开发 Controller 所相关的知识，大部分内容来自于《Programming Kubernetes》（推荐直接阅读）。 最简单的方式是调用 generate-groups.sh 或者 hack/update-codegen.sh 脚本来生成。 $ vendor/k8s.io/code-generator/generate-groups.sh all \\ github.com/programming-kubernetes/cnat/cnat-client-go/pkg/generated \\ github.com/programming-kubernetes/cnat/cnat-client-go/pkg/apis \\ cnat:v1alpha1 \\ --output-base \"${GOPATH}/src\" \\ --go-header-file \"hack/boilerplate.go.txt\" arg2 - 指定要生成的 client、list、informer 的包名； arg3 - APIGroup 所在的包； arg4 - Group:Version； –output-base 作为参数传递给所有 generator，作为查找包的基础路径； –go-header 提供生成代码后加上的版权头信息； all 参数意味着会调用四种标准的 code generator： deepcopy-gen - 生成 func(t *T) DeepCopy() 和 func(t *T) DeepCopyInto(*T) 方法，用于对象深拷贝。 client-gen - 生成强类型的 clientset。 informer-gen - 生成自定义资源的 Informer。 lister-gen - 生成自定义资源的 Lister 对象，为 Get 和 List 请求提供只读缓存层。 另外，code-generator 还有一些额外的 generator，当你需要开发自己的 APIServer 时可能会用到： conversion-go - 创建用于转换内部类型与外部类型的函数。 defaulter-gen - 生成处理默认值字段的代码。 verify-codegen.sh 代码仓库中还会包含 hack/verify-codegen.sh，用于检查生产的代码是否是最新的，常常用于 CI 中。 除了通过命令行参数，可以通过在源码中使用特定的标签来控制代码生成。标签分为两种： doc.go 文件中 package 行前面的全局标签 类声明前的局部标签。 ","date":"2021-08-19","objectID":"/posts/cloud_computing/k8s_programming/4-generate-code/:0:0","tags":["k8s","云计算"],"title":"K8s 编程 - 4 - 代码生成","uri":"/posts/cloud_computing/k8s_programming/4-generate-code/"},{"categories":["Kubernetes 编程"],"content":"1 全局标签 全局标签 会写在 doc.go 文件中。 // +k8s:deepcopy-gen=package // Package v1 is the v1alpha1 version of the API. // +groupName=cnat.programming-kubernetes.info package v1alpha1 L1 - 表明要为包中的所有类型生成 DeepCopy() 方法。 如果后续某些类型不需要生成 DeepCopy，可以通过 // +k8s:deepcopy-gen=false 表明。 L4 - 定义了 APIGroup 的全名，如果 Go 的包名不遵循 Group 的命名，那么可以用这个标签指定 Group。 通过 // +groupName 标签，代码生成器才能使用正确的 HTTP API Path。 +groupGoName 除了 +groupName，还有 // +groupGoName 可以自定义 CR 类型的 Kind。默认下，变量与类型的名字与 Kind 相同，都是以首字母大写作为表示。 例如 Cnat，更好的命名应该是 CNAt，通过 // +groupGoName=CNAt，client-gen 生成的代码就是下面样子： type Interface interface { Discovery() discovery.DiscoveryInterface CNatV1() atv1alpha1.CNatV1alpha1Interface } ","date":"2021-08-19","objectID":"/posts/cloud_computing/k8s_programming/4-generate-code/:1:0","tags":["k8s","云计算"],"title":"K8s 编程 - 4 - 代码生成","uri":"/posts/cloud_computing/k8s_programming/4-generate-code/"},{"categories":["Kubernetes 编程"],"content":"2 局部标签 局部标签 写在 API 类型的前面，或者它前面第二个注释块中： // AtSpec defines the desired state of At type AtSpec struct { // Schedule is the desired time the command is supposed to be executed. // Note: the format used here is UTC time https://www.utctime.net Schedule string `json:\"schedule,omitempty\"` // Command is the desired command (executed in a Bash shell) to be executed. Command string `json:\"command,omitempty\"` // Important: Run \"make\" to regenerate code after modifying this file } // AtStatus defines the observed state of At type AtStatus struct { // Phase represents the state of the schedule: until the command is executed // it is PENDING, afterwards it is DONE. Phase string `json:\"phase,omitempty\"` // Important: Run \"make\" to regenerate code after modifying this file } // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // At runs a command at a given schedule. type At struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec AtSpec `json:\"spec,omitempty\"` Status AtStatus `json:\"status,omitempty\"` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // AtList contains a list of At type AtList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Items []At `json:\"items\"` } ","date":"2021-08-19","objectID":"/posts/cloud_computing/k8s_programming/4-generate-code/:2:0","tags":["k8s","云计算"],"title":"K8s 编程 - 4 - 代码生成","uri":"/posts/cloud_computing/k8s_programming/4-generate-code/"},{"categories":["Kubernetes 编程"],"content":"3 deepcopy-gen 标签 通常，我们会使用 // +k8s:deepcopy-gen=package 默认为所有类型都生成 DeepCopy() 方法。 当然，我们可以为一个特定的类型禁止生成 DeepCopy() 方法。 // +k8s:deepcopy-gen=false // // Helper is a helper struct, not an API type. type Helper struct { ... } ","date":"2021-08-19","objectID":"/posts/cloud_computing/k8s_programming/4-generate-code/:3:0","tags":["k8s","云计算"],"title":"K8s 编程 - 4 - 代码生成","uri":"/posts/cloud_computing/k8s_programming/4-generate-code/"},{"categories":["Kubernetes 编程"],"content":"4 runtime.Object 与 DeepCopyObject // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object 前面提到，runtime.Object 必须实现 DeepCopyObject() 方法，而该标签为类型生成对应的 DeepCopyObject 方法。 我们应该为所有的 ”顶级对象“（内嵌了 metav1.TypeMeta）使用该标签。 ","date":"2021-08-19","objectID":"/posts/cloud_computing/k8s_programming/4-generate-code/:4:0","tags":["k8s","云计算"],"title":"K8s 编程 - 4 - 代码生成","uri":"/posts/cloud_computing/k8s_programming/4-generate-code/"},{"categories":["Kubernetes 编程"],"content":"5 client-gen 标签 // +genclient 标签用于告诉 client-gen 需要为这个类型创建一个 client。但是，你不需要也不能把这个标签用到 List 类型上。 // +genclient 如果有些类型没有 status 字段，或者没有进行 spec-status 分离。这些情况下，可以使用 // +genclient:noStatus 标签来避免生成 UpdateStatus() 方法。 // +genclient:noStatus Note 如果没有这个标签，client-gen 会直接生成 UpdateStatus() 方法。但是，只有使用了 /status 子资源，才会分离出 /status。 如果没有开启 /status 子资源，调用该接口只会直接返回错误。 对于集群范围的资源（不属于 namespace），可以使用下面标签： // +genclient:noNamespaced 如果你想要控制 client 能够访问的 HTTP 方法，可以通过一系列标签指定： // +genclient:noVerbs // +genclient:onlyVerbs=create,delete // +genclient:skipVerbs=get,list,create,update,patch,delete,watch // +genclient:method=Create,verb=create, // result=k8s.io/apimachinery/pkg/apis/meta/v1.Status 大多数看名字就能理解，最后需要一些解释：指定了 L5 标签后，生成的 Create 方法中只会执行创建动作，并返回一个 metav1.Status 对象，而不是它自身的 API 类型对象。对于自定义 APIServer 来说，可能会用到这样的资源。 使用 // +genclient:method 的常见场景是为资源增加 scale 相关方法。在启动 /scale 子资源后，下面标签就可以生成对应的 client 方法。 // +genclient:method=GetScale,verb=get,subresource=scale,\\ // result=k8s.io/api/autoscaling/v1.Scale // +genclient:method=UpdateScale,verb=update,subresource=scale,\\ // input=k8s.io/api/autoscaling/v1.Scale,result=k8s.io/api/autoscaling/v1.Scale ","date":"2021-08-19","objectID":"/posts/cloud_computing/k8s_programming/4-generate-code/:5:0","tags":["k8s","云计算"],"title":"K8s 编程 - 4 - 代码生成","uri":"/posts/cloud_computing/k8s_programming/4-generate-code/"},{"categories":["Kubernetes 编程"],"content":"6 informer-gen 和 lister-gen informer-gen 和 lister-gen 都会处理 client-gen 的 // +genclient 标签，所有指定了生成客户端的类型都会自顶生成相应的 Informer 和 Lister。 ","date":"2021-08-19","objectID":"/posts/cloud_computing/k8s_programming/4-generate-code/:6:0","tags":["k8s","云计算"],"title":"K8s 编程 - 4 - 代码生成","uri":"/posts/cloud_computing/k8s_programming/4-generate-code/"},{"categories":["Kubernetes 编程"],"content":"参考 《Programming Kubernetes》 ","date":"2021-08-19","objectID":"/posts/cloud_computing/k8s_programming/4-generate-code/:7:0","tags":["k8s","云计算"],"title":"K8s 编程 - 4 - 代码生成","uri":"/posts/cloud_computing/k8s_programming/4-generate-code/"},{"categories":["Kubernetes 编程"],"content":" Kubernetes 编程系列 主要记录一些开发 Controller 所相关的知识，大部分内容来自于《Programming Kubernetes》（推荐直接阅读）。 ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:0:0","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"1 自定义资源定义 从 1.7 版本考试，Kubernetes 提供了 CR 的功能，CR 会与其他元素的 Kubernetes 资源存放在同一个 etcd 中，并由 APISever 为其提供 HTTP API 服务。 具体实现上，APIServer 中的 apiextensions-apiserver 会对 CR 相关的 HTTP 请求进行处理： CR 由 CustomResourceDefinition（简称 CRD）来定义，其本身就是一种 Kubernetes 资源，用于描述可以在当前集群使用的 CR。 apiVersion:apiextensions.k8s.io/v1beta1 kind:CustomResourceDefinition metadata:name:ats.cnat.programming-kubernetes.info spec:group:cnat.programming-kubernetes.info names:kind:At listKind:AtList plural:ats singular:at scope:Namespaced subresources:status:{}version:v1alpha1 versions:- name:v1alpha1 served:truestorage:true 关于 CR 与 CRD 更多的信息见 K8s 学习 - CRD。 ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:1:0","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"2 服务发现信息 当部署 CRD 后，kubectl 就可以发现对应的 CR 了。一个关键的问题是：kubectl 如何发现一个新的 CR 的？ 通过如下命令卡其 kubectl 执行日志，我们可以了解具体的细节： $ kubectl get tidbcluster -v=7 kubectl 通过请求 APIServer 的 /apis 查询所有的 API Group。 $ curl -H \"Authorization: Bearer $TOKEN\" --insecure $APISERVER/apis { \"kind\": \"APIGroupList\", \"apiVersion\": \"v1\", \"groups\": [ { \"name\": \"apiregistration.k8s.io\", \"versions\": [ { \"groupVersion\": \"apiregistration.k8s.io/v1\", \"version\": \"v1\" }, { \"groupVersion\": \"apiregistration.k8s.io/v1beta1\", \"version\": \"v1beta1\" } ], \"preferredVersion\": { \"groupVersion\": \"apiregistration.k8s.io/v1\", \"version\": \"v1\" } }, // ... 对于所有的 API Group，请求一次 /apis/group/version 查询该 GroupVersion 下支持的所有 Resource。找到符合命名对应的 Resource。 $ curl --insecure -H \"Authorization: Bearer $TOKEN\" $APISERVER/apis/pingcap.com/v1alpha1 { \"kind\": \"APIResourceList\", \"apiVersion\": \"v1\", \"groupVersion\": \"pingcap.com/v1alpha1\", \"resources\": [ { \"name\": \"tidbclusters\", \"singularName\": \"tidbcluster\", \"namespaced\": true, \"kind\": \"TidbCluster\", \"verbs\": [ \"delete\", \"deletecollection\", \"get\", \"list\", \"patch\", \"create\", \"update\", \"watch\" ], \"shortNames\": [ \"tc\" ], \"storageVersionHash\": \"2dlERqlmc8s=\" }, // ... kubectl 将获取到的信息转换为三元组： Group（如 pingcap.com） Version（如 v1alpha1） Resource（如 tidbclusters） 可以想到，这些映射关系在代码中都是靠着 RESTMapper 实现的。 kubectl 的缓存 kubectl 还会在 \"~/.kube/cache\" 目录中缓存一份 Resource 的列表，有效期为 10min。 所以如何 CRD 发生变化，最多需要 10min 才会在 CLI 中体现出来。 ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:2:0","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"3 CustomResourceDefinition ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:3:0","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"3.1 基本定义 ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:3:1","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"3.2 高级功能 3.2.1 CRD 合法性验证 创建和更新 CR 时，会由 APIServer 进行合法性验证。该验证基于 CRD 定义中的 validation 字段所指定的 OpenAPI v3 Schema 进行。 更复杂的验证 CRD 中的验证只能做到类型与值的合法性验证，如果需要更复杂的验证，可以通过 Admission Webhook 做到。Webhook 会在基于 OpenAPI 的验证逻辑完成后立即被调用。 在 1.14 之前，OpenAPI v3 Schema 是可选的，也就是可以不用在 Schema 中指定所有的字段。 从 1.15 开始，CRD Schema 会作为 Kubernetes APIServer 的 OpenAPI 一部分发布，当使用 kubectl 时，它将用于客户端验证，可以及时发现未定义的字段。例如，用于在对象中使用了 foo:bar 类型，但是 OpenAPI Schema 中却没有定义 foo 类型，那么 kubectl 就会拒绝该对象。 未来，未在 Schema 中指定过的字段也将不会被持久化，因此必须为 CRD 指定 OpenAPI Schema 来保证其合法性。 3.2.2 ShortName 与 Category CRD 也可以使用 ShortName，也称为别名。一种资源可以由多个别名。 kubectl api-resource 命令可以列出所有短名字： $ kubectl api-resource NAME SHORTNAMES APIVERSION NAMESPACED KIND bindings v1 true Binding componentstatuses cs v1 false ComponentStatus configmaps cm v1 true ConfigMap endpoints ep v1 true Endpoints events ev v1 true Event limitranges limits v1 true LimitRange namespaces ns v1 false Namespace CRD 中也可以指定 categories 字段，加入一个类别。这样 kubectl get \u003ccategory\u003e 就可以列出一个类别下的所有资源了。 apiVersion:apiextensions.k8s.io/v1beta1 kind:CustomResourceDefinition metadata:name:ats.cnat.programming-kubernetes.info spec:# ... categories:- all 3.2.3 打印列 3.2.4 SubResource SubResource 是一个特殊的 HTTP Endpoint，在普通资源的 HTTP API Path 后加上一个后缀得到的，例如 /logs、/portforward、/exex 等。 目前 CRD 支持两种 SubResource：/scale 与 /status。 (1) Status /status 用于用户将 CR 实例的 spec 与 status 字段权限隔离。因为: 用户一般不会更新 status 字段。 Controller 不应该更新 spec 字段。 RBAC 无法做到控制这两个字段的权限，而引入 /status SubResource 用于解决这个问题。启用 status SubResource 后，status 的更新与读取就会基于 /status API，而 RBAC 可以做到控制 HTTP endpoint 的权限。 也就是说，RBAC + status SubResource 实现了 spec 与 status 字段的权限隔离。 apiVersion:rbac.authorization.k8s.io/v1 kind:Role metadata:#... rules:- apiGroups:[\"\"]resources:[\"ats/status\"]verbs:[\"update\",\"patch\"] 当你开启 status Resource 时，你需要注意的一些变化： 主 HTTP Endpoint 上创建或更新资源时，会自动忽略 status 字段的值。 对应的， /status HTTP Endpoint 的任何操作都会忽略 status 字段以外的值。 当 metadata 和 status 以外的字段值发生变化时，才会递增 metadata.generation 字段的值（这表明了 spec 发生变化）。 你可以通过如下方式为 CRD 开启 status SubResource： apiVersion:apiextensions.k8s.io/v1beta1 kind:CustomResourceDefinition spec:subresources:status:{} WARN 可以看到，启动 status 子资源会导致更新 status 字段的 HTTP API 发生变化。这就表明了，你不能开启 /status SubResource 后热更新 CRD，因为正在运行中的 Controller 还是会请求主 HTTP Endpoint 来更新 status 字段，而更新后这将失败。 对此，你可能需要升级 CRD 的版本。例如： apiVersion:apiextensions.k8s.io/v1beta1 kind:CustomResourceDefinition spec:# ... versions:- name:v1alpha1 served:truestorage:true- name:v1beta1 served:truesubresources:status:{} (2) Scale scale 子资源用于查看或修改资源中定义的副本数量。这个子资源主要是用于类似 Deployment 和 ReplicaSet 这样有副本数的资源，通过它可以对资源进行扩容和缩容。 kubectl scale 就是通过 /scale 子资源来实现的。 $ kubectl scale --replicas=3 your-custom-resource -v=7 I0429 21:17:53.138353 66743 round_trippers.go:383] PUT https://host/apis/group/v1/your-custom-resource/scale apiVersion:apiextensions.k8s.io/v1beta1 kind:CustomResourceDefinition spec:subresources:scale:specReplicasPath:.spec.replicas statusReplicasPath:.status.replicas labelSelectorPath:.status.labelSelector ... 当然，/scale 只能修改 replica 的值，而其具体的操作还是需要自定义 Controller 实现的。 /scale HTTP Endpoint 读取或写入的对象 Kind 为 Scale，属于 autoscaling/v1 APIGroup。 // Scale represents a scaling request for a resource. type Scale struct { metav1.TypeMeta `json:\",inline\"` // Standard object metadata; More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata. // +optional metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` // defines the behavior of the scale. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status. // +optional Spec ScaleSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // current status of the scale. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status. Read-only. // +optional Status ScaleStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } // ScaleSpec describes the attributes of a scale subresource type ScaleSpec struct { // desired num","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:3:2","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"4 代码中使用 CR 在 Golang 中有着以下方式可以访问 CR： 使用 client-go 的 dynamic client（无强类型）。 kubernetes/controller-runtime 提供的 client，在 Operator SDK 与 Kubebuilder 中使用。 client-gen 生成的 client，与 client-go 包中使用的一样。 ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:4:0","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"4.1 dynamic client “k8s.io/client-go/dynamic” 中提供了 client 可以对 GVK 完全无感知。它只会使用 unstructured.Unstructured 结构。 dynamic client 不使用 Scheme 与 RESTMapper，需要手动进行 GVR 的注册。 gvr := schema.GroupVersionResource{ Group: \"apps\", Version: \"v1\", Resource: \"deployments\", } client, err := NewForConfig(cfg) client.Resource(gvr). Namespace(namespace).Get(\"foo\", metav1.GetOptions{}) 其输入与输出都是 *unstructured.Unstructured 对象，数据结构与 json.Unmarshal 反序列化后输出一样： 对象通过 map[string]interface{} 表示。 数组通过 []interface{} 表示。 基础数据类型为：string、bool、float64、int64。 UnstructuredContent() 提供了访问 unstructed 对象内部数据的功能： name, found, err := unstructured.NestedString(u.Object, \"metadata\", \"name\") 可以看到，dynamic client 提供了一种抽象访问资源的方法，因此主要在通用类型的控制器中使用。例如垃圾回收控制器，可以删除任何资源进行操作，所以需要 dynamic client。 ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:4:1","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"4.2 强类型 client 强类型 client 为每种 GVK 都采用了具体的 Golang 类。它们使用起来更方便，也能更加安全。不过因为其类都需要在编译期都确定，所以需要通过工具去生成对应的类。 4.2.1 类型风格 对应资源的结构体通常与 Kind 相同名字命名，并且放在所属 GVK 的 Group + Version 对应包中。例如，group/verion.Kind 会放在包 “pkg/apis/group/version” 。 通常，对应的结构体会放在 “types.go” 文件。如之前提到的，CR 的定义也要包含 TypeMeta 与 ObjectMeta 结构体，通常也会包含 Spec 与 Status 字段。 例如 Deployment 对象放在 “k8s.io/kubernetes/apps/v1/types.go” 文件里： type Deployment struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec DeploymentSpec `json:\"spec,omitempty\"` Status DeploymentStatus `json:\"status,omitempty\"` } 4.2.2 包结构 除了 types.go 文件，还需要了解一些其他文件。 “doc.go” 文件描述 API 的功能，并包含了一系列全局的代码生成标签： // Package v1alpha1 contains the cnat v1alpha1 API group // // +k8s:deepcopy-gen=package // +groupName=cnat.programming-kubernetes.info package v1alpha1 “register.go” 文件包含一些用于把 CRD 注册到 Scheme 中的辅助函数： package version import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/apimachinery/pkg/runtime/schema\" group \"repo/pkg/apis/group\" ) // SchemeGroupVersion is group version used to register these objects var SchemeGroupVersion = schema.GroupVersion{ Group: group.GroupName, Version: \"version\", } // Kind takes an unqualified kind and returns back a Group qualified GroupKind func Kind(kind string) schema.GroupKind { return SchemeGroupVersion.WithKind(kind).GroupKind() } // Resource takes an unqualified resource and returns a Group // qualified GroupResource func Resource(resource string) schema.GroupResource { return SchemeGroupVersion.WithResource(resource).GroupResource() } var ( SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) AddToScheme = SchemeBuilder.AddToScheme ) // Adds the list of known types to Scheme. func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, \u0026SomeKind{}, \u0026SomeKindList{}, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } “zz_generated.deepcopy.go” 为自定义资源对应的类定义了 DeepCopy() 方法，包括其所有的子结构体（例如 spec 与 status）。 ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:4:2","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"4.3 controller-runtime client controller-runtime 提供的 client 用于处理任何在 Scheme 中注册的 Kind，也就是说，它也是动态的。 它使用 APIServer 提供的发现信息，将不同的 Kind 映射到不同的 HTTP API Path 上。 import ( \"flag\" corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes/scheme\" \"k8s.io/client-go/tools/clientcmd\" runtimeclient \"sigs.k8s.io/controller-runtime/pkg/client\" ) // 读取 config kubeconfig = flag.String(\"kubeconfig\", \"~/.kube/config\", \"kubeconfig file path\") flag.Parse() config, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) // 创建 client cl, _ := runtimeclient.New(config, client.Options{ Scheme: scheme.Scheme, }) // 查找 Pods podList := \u0026corev1.PodList{} err := cl.List(context.TODO(), client.InNamespace(\"default\"), podList) 可以看到，List() 方法可以作用于任意指定 Scheme 中注册过的，将结果解析到传递的类型中。 对于 CR，通过自定义的 Scheme 创建 client 即可： import ( \"flag\" corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes/scheme\" \"k8s.io/client-go/tools/clientcmd\" runtimeclient \"sigs.k8s.io/controller-runtime/pkg/client\" cnatv1alpha1 \"github.com/.../cnat/cnat-kubebuilder/pkg/apis/cnat/v1alpha1\" ) kubeconfig = flag.String(\"kubeconfig\", \"~/.kube/config\", \"kubeconfig file\") flag.Parse() config, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) // 注册 CR Scheme crScheme := runtime.NewScheme() cnatv1alpha1.AddToScheme(crScheme) // 创建 client cl, _ := runtimeclient.New(config, client.Options{ Scheme: crScheme, }) // 操作 CR list := \u0026cnatv1alpha1.AtList{} err := cl.List(context.TODO(), client.InNamespace(\"default\"), list) ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:4:3","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":"参考 《Programming Kubernetes》 ","date":"2021-08-17","objectID":"/posts/cloud_computing/k8s_programming/3-custom-resource/:5:0","tags":["k8s","云计算"],"title":"K8s 编程 - 3 - Custom Resource","uri":"/posts/cloud_computing/k8s_programming/3-custom-resource/"},{"categories":["Kubernetes 编程"],"content":" Kubernetes 编程系列 主要记录一些开发 Controller 所相关的知识，大部分内容来自于《Programming Kubernetes》（推荐直接阅读）。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:0:0","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"1 核心代码库 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:1:0","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"1.1 k8s.io/client-go 编写 Operator 代码时，最核心的库就是 k8s.io/client-go，其提供了访问 Kubernetes 最基本的 client 实现。 client-go 与 Kubernetes 同时发布，对应 Kubernetes 1.x.y 版本就会有着对应的 client-go 版本。不过 client-go 还会发布独立的版本号，例如 client-go 9.0.0。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:1:1","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"1.2 k8s.io/api k8s.io/api 库包含了所有 Kubernetes 基本对象的结构体，其按照 GroupVersion 进行组织。 例如 Pod 一类的最基本的对象，包含在 core Group 中，因此放在 k8s.io/api/core/v1 包中的 types.go 文件中。其他对象也是类似。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:1:2","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"1.3 k8s.io/apimachinery k8s.io/apimachinery 提供了 Kubernetes API 所需要的一些通用代码。 apimachinery 也包含了很多通用的 API 类型，例如 ObjectMeta、TypeMeta、GetOptions。 Tip 因为 API Machinery 的设计优秀，你可以在任何 API 相关的业务代码上尝试使用该库。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:1:3","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"2 使用 client-go ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:2:0","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"2.1 创建并使用 下面是最基本在一个 Go 项目中使用 client-go 的方式（省略了异常处理）： import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/tools/clientcmd\" \"k8s.io/client-go/kubernetes\" ) // 读取 config kubeconfig = flag.String(\"kubeconfig\", \"~/.kube/config\", \"kubeconfig file\") flag.Parse() config, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) // 创建 client set clientset, err := kubernetes.NewForConfig(config) // 通过 client set 访问资源 pod, err := clientset.CoreV1().Pods(\"book\").Get(\"example\", metav1.GetOptions{}) 在项目中，所有的资源（包括 CR）都是通过 ClientSet 类的对象来访问。ClientSet 包含了多个 Group 多个 Version 的 Client。 在集群内部 Pod 中，每个容器会挂载一个 ServiceAccount，路径为目录 /var/run/secrets/kubernetes.io/serviceaccount。通过该目录下提供的证书与 Token，可以替代 kubeconfig 文件来创建 ClientSet，用于访问 Kubernetes。 // 读取 ServiceAccount 的目录创建 config config, err := rest.InClusterConfig() if err != nil { // 回滚使用 kubeconfig 文件创建 config kubeconfig := filepath.Join(\"~\", \".kube\", \"config\") if envvar := os.Getenv(\"KUBECONFIG\"); len(envvar) \u003e0 { kubeconfig = envvar } config, err = clientcmd.BuildConfigFromFlags(\"\", kubeconfig) if err != nil { fmt.Printf(\"The kubeconfig cannot be loaded: %v\\n\", err os.Exit(1) } } // 创建 client set clientset, err := kubernetes.NewForConfig(config) 默认下所有 Client 以 JSON 格式与 APIServer 进行通信，你可以通过以下方式使用 Protobuf 格式： cfg, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig) cfg.AcceptContentTypes = \"application/vnd.kubernetes.protobuf,application/json\" // 允许回复的格式 cfg.ContentType = \"application/vnd.kubernetes.protobuf\" // 发送请求的格式 clientset, err := kubernetes.NewForConfig(cfg) ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:2:1","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"2.2 版本与兼容性 可以看到，ClientSet 使用的资源类型都是静态通过 api 库导入的，与 APIServer 使用的资源类型没有直接关系。也就是说，如果 client-go 库与 Kubernetes APIServer 对应的版本不同，那么就可能双方使用的资源定义不同。 为此，client-go 仓库的 README 中，会发布一个兼容性矩阵。 √ 表示 client-go 与 Kubernetes 集群有着完全相同的 API 组版本。 + 表示 client-go 包含一些功能或者 API 对象，Kubernetes 集群还不支持。不过公共的部分还是能够正常工作。 - 表示 Kubernetes 的一些特性 client-go 不能使用。不过大部分公共的部分能够正常工作。 因此，当你使用的 client-go 库与 Kubernetes 版本不一致时，需要清楚某些特性是否能够兼容。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:2:2","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"2.3 API 兼容性保证 在 Kubernetes 风格中，API 可能包含以下版本： Alpha - v1alpha1、v1aplpha2 等称为 Alpha 版本，表示不稳定的版本。 这些版本可能随时会消失或者进行不兼容的改变。默认是被禁用的，需要管理员手动启用它们。 Beta - v1beta1、v1beta2、v2beta1 等称为 Beta 版本，表示相对稳定的版本。 通常不会发生不兼容的改动，但是不是严格保证。Beta 版本功能默认会被启用。 GA - v1、v2 等不带后缀的都是稳定版本。 会一直存在，并且一直保持兼容性。 在进行项目开发时，要记住两点： 代码中一个资源定义的某些字段也可能准许上述版本定义（注释中说明）。 访问 APIServer 时，APIServer 会在相同的资源不同版本间自动进行转换。 例如，某个资源是由 v1beta1 版本的 API 创建的，但是你可以通过 v1 版本的 API 来访问该资源。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:2:3","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"3 Kubernetes 对象 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:3:0","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"3.1 Object Go 语言中的 Kubernetes 对象都实现了 runtime.Object interface，该接口位于 k8s.io/apimachinery/pkg/runtime 包： // Object interface must be supported by all API types registered with Scheme. Since objects in a scheme are // expected to be serialized to the wire, the interface an Object must provide to the Scheme allows // serializers to set the kind, version, and group the object is represented as. An Object may choose // to return a no-op ObjectKindAccessor in cases where it is not expected to be serialized. type Object interface { GetObjectKind() schema.ObjectKind DeepCopyObject() Object } GetObjectKind() - 得到 ObjectKind，用以返回或设置 GVK DeepCopyObject() - 深拷贝得到一个 Object schema.ObjectKind 就是代表一个具有 Kind 的 Object： // All objects that are serialized from a Scheme encode their type information. This interface is used // by serialization to set type information from the Scheme onto the serialized version of an object. // For objects that cannot be serialized or have unique requirements, this interface may be a no-op. type ObjectKind interface { // SetGroupVersionKind sets or clears the intended serialized kind of an object. Passing kind nil // should clear the current setting. SetGroupVersionKind(kind GroupVersionKind) // GroupVersionKind returns the stored group, version, and kind of an object, or an empty struct // if the object does not expose or provide these fields. GroupVersionKind() GroupVersionKind } SetGroupVersionKind() - 设置 Object 的 GVK GroupVersionKind() - 得到 Object 的 GVK 总结一下，Object 对象可以得到对应的 Kind，以及允许进行深拷贝。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:3:1","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"3.2 TypeMeta Object 仅仅是一个 interface，所有实际的 Kubernetes 对象都是通过内嵌 metav1.TypeMeta 结构来包含 GroupVersionKind 方法的。 // TypeMeta describes an individual object in an API response or request // with strings representing the type of the object and its API schema version. // Structures that are versioned or persisted should inline TypeMeta. // // +k8s:deepcopy-gen=false type TypeMeta struct { // Kind is a string value representing the REST resource this object represents. // Servers may infer this from the endpoint the client submits requests to. // Cannot be updated. // In CamelCase. // More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds // +optional Kind string `json:\"kind,omitempty\" protobuf:\"bytes,1,opt,name=kind\"` // APIVersion defines the versioned schema of this representation of an object. // Servers should convert recognized schemas to the latest internal value, and // may reject unrecognized values. // More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources // +optional APIVersion string `json:\"apiVersion,omitempty\" protobuf:\"bytes,2,opt,name=apiVersion\"` } func (obj *TypeMeta) GetObjectKind() schema.ObjectKind { return obj } Kind - 对象的类型； APIVersion - 对象的 API 版本； 这也就是你在资源定义 YAML 时看到的 apiVersion 与 kind 字段了： apiVersion:v1kind:Pod# ... Core Group Version Pod 以及一些其他类型是 Kubernetes 很早就有的 Core Group，它们的 Group 为空，因此 apiVersion 就是 v1。 后来，Kubernetes 引入了 API Group 的概念，apiVersion 通过 \u003cGroup\u003e/\u003cversion\u003e 表示，例如 apps/v1。 而 Core Group 因为历史原因其 Group 为空，所以 apiVersion 的值看上去可能不太一样。 不要直接使用 Kind 与 Version 字段 在前面使用示例获取 Pod 后，你会发现返回的 Pod 对象并没有设置 Kind 与 Version 字段。这是 Kubernetes 在解码时故意抹去的。 // Decode does not do conversion. It removes the gvk during deserialization. func (d WithoutVersionDecoder) Decode(data []byte, defaults *schema.GroupVersionKind, into Object) (Object, *schema.GroupVersionKind, error) { obj, gvk, err := d.Decoder.Decode(data, defaults, into) if obj != nil { kind := obj.GetObjectKind() // clearing the gvk is just a convention of a codec kind.SetGroupVersionKind(schema.GroupVersionKind{}) } return obj, gvk, err } 不清楚为啥，目前可以通过这个 issue 跟进这个问题。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:3:2","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"3.3 ObjectMeta 除了 TypeMeta，所有的对象都有一个 metav1.ObjectMeta 类型的字段： // ObjectMeta is metadata that all persisted resources must have, which includes all objects // users must create. type ObjectMeta struct {Clayton Coleman, 5 years ago: • iQEcBAABCAAGBQJYfoneAAoJED0WkGtPHFyzBGEH/ROj… Name string `json:\"name,omitempty\" protobuf:\"bytes,1,opt,name=name\"` GenerateName string `json:\"generateName,omitempty\" protobuf:\"bytes,2,opt,name=generateName\"` Namespace string `json:\"namespace,omitempty\" protobuf:\"bytes,3,opt,name=namespace\"` // DEPRECATED SelfLink string `json:\"selfLink,omitempty\" protobuf:\"bytes,4,opt,name=selfLink\"` UID types.UID `json:\"uid,omitempty\" protobuf:\"bytes,5,opt,name=uid,casttype=k8s.io/kubernetes/pkg/types.UID\"` ResourceVersion string `json:\"resourceVersion,omitempty\" protobuf:\"bytes,6,opt,name=resourceVersion\"` Generation int64 `json:\"generation,omitempty\" protobuf:\"varint,7,opt,name=generation\"` CreationTimestamp Time `json:\"creationTimestamp,omitempty\" protobuf:\"bytes,8,opt,name=creationTimestamp\"` DeletionTimestamp *Time `json:\"deletionTimestamp,omitempty\" protobuf:\"bytes,9,opt,name=deletionTimestamp\"` DeletionGracePeriodSeconds *int64 `json:\"deletionGracePeriodSeconds,omitempty\" protobuf:\"varint,10,opt,name=deletionGracePeriodSeconds\"` Labels map[string]string `json:\"labels,omitempty\" protobuf:\"bytes,11,rep,name=labels\"` Annotations map[string]string `json:\"annotations,omitempty\" protobuf:\"bytes,12,rep,name=annotations\"` OwnerReferences []OwnerReference `json:\"ownerReferences,omitempty\" patchStrategy:\"merge\" patchMergeKey:\"uid\" protobuf:\"bytes,13,rep,name=ownerReferences\"` Finalizers []string `json:\"finalizers,omitempty\" patchStrategy:\"merge\" protobuf:\"bytes,14,rep,name=finalizers\"` ClusterName string `json:\"clusterName,omitempty\" protobuf:\"bytes,15,opt,name=clusterName\"` ManagedFields []ManagedFieldsEntry `json:\"managedFields,omitempty\" protobuf:\"bytes,17,rep,name=managedFields\"` } Name - Object 的命名。 GenerateName - 生成基于 GenerateName 的随机名字，填充到 Name。 Namespace - 对象所属的 namespace，对于 namespace 下的对象，空代表着 “default”。 UID - 对象的的唯一 ID。 系统填充，只读不允许更改。 ResourceVersion - 对象的版本号，可用于 client 判断资源是否发生变更。 可以用于乐观并发，变更检测，watch 操作时使用，client 必须将其原封不动传递给 APIServer。 系统填充，只读不允许更改。 Generation - 资源期望状态的序列号。 系统填充，只读不允许更改。 CreationTimestamp - Server 创建对象的时间。 系统填充，只读不允许更改。 DeletionTimestamp - 对象将被删除的时间。 当用户优雅删除一个资源时，由系统设置。资源将预计在该时间后被删除。 如果存在着 finalizer，删除会阻塞等待 finalizer 去除。 一旦调用删除之后，无法取消删除。 DeletionGracePeriodSeconds - 优雅停止时间。当 DeletionTimestamp 设置时才会设置。 Labels - 对象的 label。 Annotations - 对象的 annotation。 OwnerReferences - 此对象依赖的对象的列表。 如果列表中所有的对象都被删除，该对象也会被垃圾回收。 如果该对象要由 Controller 来管理，将列表中其中一个对象指向 Controller，并设置 controler 字段。 Finalizers - 在对象被集群删除前，Finalizers 必须为空。 一旦 DeletionTimestamp 被设置后，表明组件可以开始进行清理操作，清理完成后删除对应的 Finalizer。 ClusterName - 对象所属的集群名字，用于区分不同集群内同名同 namespace 的资源。 目前该资源没有被 APIServer 使用到。 ManagedFields - ?? ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:3:3","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"3.4 Spec 与 Status 几乎所有顶级对象都包含 spec 与 status 字段，这体现了 Kubernetes 的声明式 API 设计。 spec 是用户期望的对象的状态，status 是对象的当前状态。 spec 通常由用户部署时配置，status 主要由系统中的 Controller 进行填充。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:3:4","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"4 ClientSet 在前面的例子，通过 kubernetes.NewForConfig(config) 会返回一个 ClientSet。ClientSet 包含可以访问多个 APIGroup 的资源。 对于 client-go 返回的 ClientSet，几乎可以访问 Kubernetes APIServer 的所有资源（除了 APIService 与 CRD）。其主要的接口如下： type Interface interface { Discovery() discovery.DiscoveryInterface AdmissionregistrationV1() admissionregistrationv1.AdmissionregistrationV1Interface AdmissionregistrationV1beta1() admissionregistrationv1beta1.AdmissionregistrationV1beta1Interface InternalV1alpha1() internalv1alpha1.InternalV1alpha1Interface AppsV1() appsv1.AppsV1Interface AppsV1beta1() appsv1beta1.AppsV1beta1Interface AppsV1beta2() appsv1beta2.AppsV1beta2Interface AuthenticationV1() authenticationv1.AuthenticationV1Interface AuthenticationV1beta1() authenticationv1beta1.AuthenticationV1beta1Interface AuthorizationV1() authorizationv1.AuthorizationV1Interface AuthorizationV1beta1() authorizationv1beta1.AuthorizationV1beta1Interface AutoscalingV1() autoscalingv1.AutoscalingV1Interface AutoscalingV2beta1() autoscalingv2beta1.AutoscalingV2beta1Interface AutoscalingV2beta2() autoscalingv2beta2.AutoscalingV2beta2Interface BatchV1() batchv1.BatchV1Interface BatchV1beta1() batchv1beta1.BatchV1beta1Interface CertificatesV1() certificatesv1.CertificatesV1Interface CertificatesV1beta1() certificatesv1beta1.CertificatesV1beta1Interface CoordinationV1beta1() coordinationv1beta1.CoordinationV1beta1Interface CoordinationV1() coordinationv1.CoordinationV1Interface CoreV1() corev1.CoreV1Interface DiscoveryV1() discoveryv1.DiscoveryV1Interface DiscoveryV1beta1() discoveryv1beta1.DiscoveryV1beta1Interface EventsV1() eventsv1.EventsV1Interface EventsV1beta1() eventsv1beta1.EventsV1beta1Interface ExtensionsV1beta1() extensionsv1beta1.ExtensionsV1beta1Interface FlowcontrolV1alpha1() flowcontrolv1alpha1.FlowcontrolV1alpha1Interface FlowcontrolV1beta1() flowcontrolv1beta1.FlowcontrolV1beta1Interface NetworkingV1() networkingv1.NetworkingV1Interface NetworkingV1beta1() networkingv1beta1.NetworkingV1beta1Interface NodeV1() nodev1.NodeV1Interface NodeV1alpha1() nodev1alpha1.NodeV1alpha1Interface NodeV1beta1() nodev1beta1.NodeV1beta1Interface PolicyV1() policyv1.PolicyV1Interface PolicyV1beta1() policyv1beta1.PolicyV1beta1Interface RbacV1() rbacv1.RbacV1Interface RbacV1beta1() rbacv1beta1.RbacV1beta1Interface RbacV1alpha1() rbacv1alpha1.RbacV1alpha1Interface SchedulingV1alpha1() schedulingv1alpha1.SchedulingV1alpha1Interface SchedulingV1beta1() schedulingv1beta1.SchedulingV1beta1Interface SchedulingV1() schedulingv1.SchedulingV1Interface StorageV1beta1() storagev1beta1.StorageV1beta1Interface StorageV1() storagev1.StorageV1Interface StorageV1alpha1() storagev1alpha1.StorageV1alpha1Interface } 可以看到，包含了所有 Kubernetes 原生的 GroupVersion 接口，用于操作所有 GroupVersion 下的资源。 在每个 GroupVersion 方法内部（例如 AppsV1beta1），就是该 APIGroup 下的所有资源了： type AppsV1beta1Interface interface { RESTClient() rest.Interface ControllerRevisionsGetter DeploymentsGetter StatefulSetsGetter } RESTClient 是一个通用的 REST Client： // Interface captures the set of operations for generically interacting with Kubernetes REST apis. type Interface interface { GetRateLimiter() flowcontrol.RateLimiter Verb(verb string) *Request Post() *Request Put() *Request Patch(pt types.PatchType) *Request Get() *Request Delete() *Request APIVersion() schema.GroupVersion } 而其他资源接口就是可以操作对应的资源了： // DeploymentsGetter has a method to return a DeploymentInterface. // A group's client should implement this interface. type DeploymentsGetter interface { Deployments(namespace string) DeploymentInterface } // DeploymentInterface has methods to work with Deployment resources. type DeploymentInterface interface { Create(ctx context.Context, deployment *v1beta1.Deployment, opts v1.CreateOptions) (*v1beta1.Deployment, error) Update(ctx context.Context, deployment *v1beta1.Deployment, opts v1.UpdateOptions) (*v1beta1.Deployment, error) UpdateStatus(ctx context.Context, deployment *v1beta1.Deployment, opts v1.UpdateOptions) (*v1beta1.Deployment, error) Delete(ctx context.Context, name","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:4:0","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"5 Informer Controller 如果每次需要查询就去访问 APIServer，会造成比较大的压力。Informer 提供了对象的 内存缓存，而通过 Watch 机制来监听对象的即时变化。 Informer 内部实现了完善的错误处理能力：当与 APIServer Watch 的长连接发生断开时，它会通过一个新的 Watch 请求尝试恢复，从事件中找到正确的事件继续处理。也就是不会丢失事件。如果连接断开时间很长，Informer 会获取完整的对象列表重新构建缓存。 Informer 还提供了 resync 的机制：没经过一定的时间，它就会调用注册过的事件处理函数（Update 回调），提供完成的对象列表。 Note resync 也是基于 Informer 内存缓存提供的对象列表，不会向 APIServer 发起请求。 为了减少程序中使用多个 Informer 带来的开销，可以通过 SharedInformerFactory 来方便地对 Informer 进行复用。SharedInformerFactory 允许在一个应用程序中复用 Informer，这样底层只有一个与 APIServer 构建的 Watch 连接。 通过 ClientSet 可以很方便的创建一个 SharedInformerFactory： import ( // ... \"k8s.io/client-go/informers\" ) clientset, err := kubernetes.NewForConfig(config) // 创建 SharedInformerFactory： informerFactory := informers.NewSharedInformerFactory(clientset, time.Second*30) // 注册事件回调 podInformer := informerFactory.Core().V1().Pods() podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(new interface{}) {...}, UpdateFunc: func(old, new interface{}) {...}, DeleteFunc: func(obj interface{}) {...}, }) // 启动 informerFactory.Start(wait.NeverStop) // Cache 同步 informerFactory.WaitForCacheSync(wait.NeverStop) // 使用（纯内存的查询） pod, err := podInformer.Lister().Pods(\"programming-kubernetes\").Get(\"client-go\") AddEventHandler - 添加事件回调，这也是 Controller 的基础。 Start - 启动 InformerFactory，内部会启动一些 groutine 去访问 APIServer。 WaitForCacheSync - 让代码停下来，等待 InformerFactory 第一次同步所有的缓存。 例子中设置了 30s 的 resync 周期，也就是每隔 30s Informer 针对所有的对象会，触发一次 UpdateFunc 函数回调。因此，UpdateFunc 可能需要通过 ObjectMeta.resourceVersion 字段来判断是否是真正的更新事件。 ReadOnly 程序只能从 Informer 中读取、查询对象，不能通过 Informer 修改对象（需通过 ClientSet），而且也不能修改 Informer 返回的对象，因为这些对象是由 Informer 管理的。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:5:0","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"6 WorkQueue client-go 库中在 “k8s.io/client-go/util/workqueue” 中提供了一种强大的优先队列，可以实现让控制变得更加方便。 所有的 WorkQueue 都实现了下面的 interface： type Interface interface { Add(item interface{}) Len() int Get() (item interface{}, shutdown bool) Done(item interface{}) ShutDown() ShuttingDown() bool } Add(item) - 用于添加一个元素，反复添加同一个未出队的 item 只会对该元素打上标记。 Len() - 返回队列的当前长度。 Get() - 返回一个最高优先级的元素，Get() 并不代表元素从队列中取出。 Done(item) - 表明元素处理完成，从队列中移出。 DelayingInterface 是基于基础 interface 的扩展，用于延迟添加元素。这适用于很方便地把处理失败的元素延时重新加入队列。 // DelayingInterface is an Interface that can Add an item at a later time. This makes it easier to // requeue items after failures without ending up in a hot-loop. type DelayingInterface interface { Interface // AddAfter adds an item to the workqueue after the indicated duration has passed AddAfter(item interface{}, duration time.Duration) } AddAfter() - 在指定时间后将元素重新加入队列。 RateLimitingInterface 是更加常用的 interface 的扩展，对元素加入队列进行频率限流： // RateLimitingInterface is an interface that rate limits items being added to the queue. type RateLimitingInterface interface { DelayingInterface // AddRateLimited adds an item to the workqueue after the rate limiter says it's ok AddRateLimited(item interface{}) // Forget indicates that an item is finished being retried. Doesn't matter whether it's for perm failing // or for success, we'll stop the rate limiter from tracking it. This only clears the `rateLimiter`, you // still have to call `Done` on the queue. Forget(item interface{}) // NumRequeues returns back how many times the item was requeued NumRequeues(item interface{}) int } AddRateLimited() - 添加元素到队列，收到频率限制。 Forget() - 表明一个元素处理结束，重置该元素的限频值（仅仅针对频率限制，还是依旧要调用 Done()）。 NumRequeues() - 返回一个元素入队的次数。 client-go 提供了多种方式的限频算法，包括： BucketRateLimiter ItemExponentialFailureRateLimiter ItemFastSlowRateLimiter MaxOfRateLimiter 不过大部分情况下，我们都只需要使用 func DefaultControllerRateLimiter() *RateLimiter 指数级的退避策略，从 5ms 考试，最大 1000s，每次退避时间翻倍。 允许每秒向队列中添加 10 个元素，允许突发添加 100 个元素。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:6:0","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"7 基本类型系统 首先，我们看着下图，有着在对代码中类型系统的转化路径的整体概念： 代码中，我们得到一个对象，可以通过 Scheme 得到对应的 GroupVersionKind 从 GroupVersionKind，通过 RESTMapper 可以映射到对应的 GroupVersionResource client 基于 GroupVersionResource 就可以请求对应的 HTTP API Path ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:7:0","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"7.1 GroupVersionKind 在 API 基本概念 - Kind 中说过，Kind 表示一个对象的类型。因为对象都是基于 GroupVersion 来说的，所以在代码中，Kind 的代表就是核心概念：GroupVersionKind，简写为 GVK。 看下具体的结构： // GroupVersionKind unambiguously identifies a kind. It doesn't anonymously include GroupVersion // to avoid automatic coercion. It doesn't use a GroupVersion to avoid custom marshalling type GroupVersionKind struct { Group string Version string Kind string } ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:7:1","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"7.2 GroupVersionResource 在 API 基本概念 - Resource 中说明了 Resource 的概念，Resource 也是基于 GroupVersion 来说的，所以代码中也有着对应的 GroupVersionResource 的概念，简写为 GVR。 // GroupVersionResource unambiguously identifies a resource. It doesn't anonymously include GroupVersion // to avoid automatic coercion. It doesn't use a GroupVersion to avoid custom marshalling type GroupVersionResource struct { Group string Version string Resource string } // GroupKind specifies a Group and a Kind, but does not force a version. This is useful for identifying // concepts during lookup stages without having partially valid types type GroupKind struct { Group string Kind string } 每个 GVR 都对应了一个 HTTP API Path。例如 apps/v1.deployments 这个 GVR 对应于 HTTP APi /apis/apps/v1/$namespace/deployments。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:7:2","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"7.3 REST Map 大多数 Kind 与 Resource 存在着一一对应的关系，用于记录 GVK 与 GVR 之间的映射关系被称为 REST Map。 代码中 RestMapper 为记录 RESTMap 的抽象 interface，其提供多个用于 GVK 与 GVR 之间转换的方法： // RESTMapper allows clients to map resources to kind, and map kind and version // to interfaces for manipulating those objects. It is primarily intended for // consumers of Kubernetes compatible REST APIs as defined in docs/devel/api-conventions.md. // // The Kubernetes API provides versioned resources and object kinds which are scoped // to API groups. In other words, kinds and resources should not be assumed to be // unique across groups. // // TODO: split into sub-interfaces type RESTMapper interface { // KindFor takes a partial resource and returns the single match. Returns an error if there are multiple matches KindFor(resource schema.GroupVersionResource) (schema.GroupVersionKind, error) // KindsFor takes a partial resource and returns the list of potential kinds in priority order KindsFor(resource schema.GroupVersionResource) ([]schema.GroupVersionKind, error) // ResourceFor takes a partial resource and returns the single match. Returns an error if there are multiple matches ResourceFor(input schema.GroupVersionResource) (schema.GroupVersionResource, error) // ResourcesFor takes a partial resource and returns the list of potential resource in priority order ResourcesFor(input schema.GroupVersionResource) ([]schema.GroupVersionResource, error) // RESTMapping identifies a preferred resource mapping for the provided group kind. RESTMapping(gk schema.GroupKind, versions ...string) (*RESTMapping, error) // RESTMappings returns all resource mappings for the provided group kind if no // version search is provided. Otherwise identifies a preferred resource mapping for // the provided version(s). RESTMappings(gk schema.GroupKind, versions ...string) ([]*RESTMapping, error) ResourceSingularizer(resource string) (singular string, err error) } KindFor() - 部分 GVR 转换为单个 GVK KindsFor() - 部分 GVR 转换为所有 GVK ResourceFor() - 部分 GVR 转换为单个 GVR ResourcesFor() - 部分 GVR 转换为所有 GVR RESTMapping() - 从一个 Kind 得到对应最优先的 RestMapping RESTMappings() - 从一个 Kind 得到所有对应的 RestMapping 部分 GVR 部分 GVR 指的是仅仅设置了部分字段的 GVR。例如 kubectl get pods 时，没有指定 Group 与 Version，但是能够映射到 v1 Pod 这样的 Kind。 RESTMapping 就代表了一个 GVK 与 GVR 的一对一映射： // RESTMapping contains the information needed to deal with objects of a specific // resource and kind in a RESTful manner. type RESTMapping struct { // Resource is the GroupVersionResource (location) for this endpoint Resource schema.GroupVersionResource // GroupVersionKind is the GroupVersionKind (data format) to submit to this endpoint GroupVersionKind schema.GroupVersionKind // Scope contains the information needed to deal with REST Resources that are in a resource hierarchy Scope RESTScope } RESTMapper interface 有着许多的 implement，不过在编写 Operator 时，通常我们使用基于发现机制的 DeferredDiscoveryRESTMapper 实现。它通过调用 APIServer 的接口动态解析 REST Mapping。 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:7:3","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"7.4 Scheme Scheme 应该是编写代码时最重要的结构了。其包含了多个 Golang 类型与 GVK 的映射，并且特定的 Kind，可以注册不同版本的转换函数，以及设置默认值的函数。 总结一下其主要作用： GVK 与 Golang 类型的转换； 注册 Convert 函数，用于 Kind 不同版本对象之间的转换； 注册 Default 函数，用于设置对象的默认值； Note 当我们需要实现 Custom APIServer 时，就会要了解到 Convert 与 Default。 7.4.1 类型转换 Scheme 提供 GVK 与特定类型转换功能。 首先，我们需要通过 Scheme.AddKnownTypes 接口来注册 Golang 类型与 GroupVersion。通常，代码生成器会在 register.go 文件中生成该代码。 // SchemeGroupVersion is group version used to register these objects var SchemeGroupVersion = schema.GroupVersion{Group: groupName, Version: \"v1alpha1\"} func init() { // We only register manually written functions here. The registration of the // generated functions takes place in the generated files. The separation // makes the code compile even when the generated files are missing. localSchemeBuilder.Register(addKnownTypes) } func addKnownTypes(scheme *runtime.Scheme) error { Scheme = scheme // Scheme 是全局的 Scheme 对象 scheme.AddKnownTypes(SchemeGroupVersion, \u0026TidbCluster{}, // 自定义的类型 ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } 通常，我们会有一个全局的 Scheme 对象，然后通过向其中注册 Golang 类型。这样，我们就可以通过 Scheme.New 方法从一个 GVK 得到一个 Golang 对象，然后通过类型断言转换为特定的类型。 robj, err := t.scheme.New(gvk) // 得到 GVK 对应的空的对象 cobj, ok := robj.(*TidbCluster) // 类型断言得到具体的对象 总结一下，Scheme 提供的相关接口： // AddKnownTypeWithName 注册 GVK 与类型（obj 的类型） func (s *Scheme) AddKnownTypeWithName(gvk schema.GroupVersionKind, obj Object) // AddKnownTypes 注册 GV 与其下的多个类型，Kind 就是类型的名字 func (s *Scheme) AddKnownTypes(gv schema.GroupVersion, types ...Object) // KnownTypes 从 GV 中取出包含的类型 func (s *Scheme) KnownTypes(gv schema.GroupVersion) map[string]reflect.Type // New 根据 GVK 创建一个对应类型的对象 func (s *Scheme) New(kind schema.GroupVersionKind) (Object, error) // ObjectKinds 返回对象对应的可能的 GVK，如果是 unversioned object 返回 true，如果类型未注册返回错误 func (s *Scheme) ObjectKinds(obj Object) ([]schema.GroupVersionKind, bool, error) // PreferredVersionAllGroups 返回每个 Group 的优先级最高 Version func (s *Scheme) PreferredVersionAllGroups() []schema.GroupVersion // PrioritizedVersionsForGroup 按照优先级顺序返回所有的 GroupVersion func (s *Scheme) PrioritizedVersionsForGroup(group string) []schema.GroupVersion // PrioritizedVersionsForGroup 按照优先级返回一个 group 所有的 version func (s *Scheme) PrioritizedVersionsForGroup(group string) []schema.GroupVersion // Recognizes GVK 是否是注册过的 func (s *Scheme) Recognizes(gvk schema.GroupVersionKind) bool // SetVersionPriority 设置一个 Group 下的 Version 的优先级 // NOTE: versions 必须是同一个 Group 下的不同 Version func (s *Scheme) SetVersionPriority(versions ...schema.GroupVersion) error 可以看到，这样就实现了 Kind 与对象类型的互转。这也就是 Client 与 APIServer 通信时，进行数据序列化与反序列化的核心。 7.4.2 不同版本类型间转换 前面可以看到，Scheme 存储着许多 Group，其 Group 下可以有着不同 Version 的类型。在 APIServer 中，为了能够处理多个 Version 共存的情况，又有着一个 Internal Version。作为不同 Version 间转换中转。 而 Scheme 也是保存着各个版本之间转换的地方（因为其知道所有版本的类型），通过以下函数提供功能： // AddConversionFunc 注册一个转换路径，对应 a 类型到 b 类型的转换，使用 fn 进行转换 func (s *Scheme) AddConversionFunc(a, b interface{}, fn conversion.ConversionFunc) error func (s *Scheme) AddFieldLabelConversionFunc(gvk schema.GroupVersionKind, conversionFunc FieldLabelConversionFunc) error func (s *Scheme) AddGeneratedConversionFunc(a, b interface{}, fn conversion.ConversionFunc) error func (s *Scheme) AddIgnoredConversionType(from, to interface{}) error // Convert 将 in 类型对象转换为 out 类型对象 func (s *Scheme) Convert(in, out interface{}, context interface{}) error func (s *Scheme) ConvertFieldLabel(gvk schema.GroupVersionKind, label, value string) (string, string, error) // ConvertToVersion 将 in 类型对象转化为 taget 指定的版本对象 func (s *Scheme) ConvertToVersion(in Object, target GroupVersioner) (Object, error) 我们将在后面的 Custom APIServer 编写中看到何时使用这些函数。 7.4.3 设置默认值 Scheme 在处理数据类型序列化，和处理数据类型转换时，会通过默认值相关函数设置对象的默认值。通过以下函数： // AddTypeDefaultingFunc 注册一个对象的默认值处理函数 func (s *Scheme) AddTypeDefaultingFunc(srcType Object, fn func(interface{})) // Default 设置一个对象的默认值，就是使用注册的默认值处理函数 func (s *Scheme) Default(src Object) Note 注意，默认值设置在 APIServer 转换时是自动进行的，而 Scheme 中仅仅是用于 Default() 接口，不会自动进行。 因此，在编写 Custom APIServer 时要正确设置好设置默认值的函数。 我们将在后面的 Custom APIServe","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:7:4","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"参考 《Programming Kubernetes》 ","date":"2021-08-10","objectID":"/posts/cloud_computing/k8s_programming/2-client-go/:8:0","tags":["k8s","云计算"],"title":"K8s 编程 - 2 - 编程基础","uri":"/posts/cloud_computing/k8s_programming/2-client-go/"},{"categories":["Kubernetes 编程"],"content":"K8s 编程需要知晓的一些基本概念","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":" Kubernetes 编程系列 主要记录一些开发 Controller 所相关的知识，大部分内容来自于《Programming Kubernetes》（推荐直接阅读）。 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:0:0","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":"1 Controller 模式 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:1:0","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":"1.1 Control Loop 所有 Controller 的基本逻辑都是执行 控制循环Control Loop，其基本逻辑为： 读取资源的状态。 采用 Watch 方式监听资源，资源变更时会触发。 改变集群中的对象的状态，或者集群外部对象的状态。 例如，启动一个 Pod、创建云上资源等。 通过 APIServer 更新资源的状态。 循环执行这些逻辑。 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:1:1","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":"1.2 事件驱动模式 大多数分布式系统都是 RPC 来触发一个动作，而 Kubernetes 中都会通过 APIServer 的 Watch 功能来监听对象的变化。 以通过一个 Deployment 来启动 Pod 为例： Deployment Controller Watch 了 Deployment 的事件，因此触发后发现用户创建了一个 Deployment，执行相关的业务逻辑：创建一个 ReplicaSet 对象。 ReplicaSet Controller Watch 了 ReplicaSet 的事件，因此触发后发现新创建了一个 ReplicaSet，执行相关的业务逻辑：创建一个 Pod 对象。 Scheduler 本身也是 Controller，通过 Watch 机制发现有一个新创建的 Pod，并且 Pod 的 spec.nodeName 是空值，那么就会将其放入调度队列。 Scheduler 将 Pod 调度到一个合适的 Node，将 Node 更新到 Pod 的 spec.nodeName 字段，然后把它写入 APIServer。 kubectl Watch 了 Pod 事件，因此触发后发现 Pod 的 spec.nodeName 与自己所在 Node 相同，那么就会构建环境并启动 Pod。 可以看到，所有的 Controller 都是运行着独立的 Control Loop，对象状态的变化通过会以事件的方式通知到 Controller。 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:1:2","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":"1.3 乐观并发 在系统中，Controller 可能存在着多个副本，因此并发的更新资源操作可能会产生写入冲突。 对此，Kubernetes APIServer 使用 乐观并发Optimistic Concurrency 方式来解决冲突。 简单地说，如果 APIServer 发现了两个并发的写请求，它就会拒绝后面的请求，返回一个错误，而发送请求的 Client 要处理这种失败。 var err error for retries := 0; retries \u003c 10; retries++ { foo, err = client.Get(\"foo\", metav1.GetOptions{}) if err != nil { break } // 更新操作 ... _, err = client.Update(foo) if err != nil \u0026\u0026 errors.IsConflict(err) { // 处理冲突 ... continue } else if err != nil { break } } 问题是如何判断两个写操作是冲突的？答案是每个 Kubernetes 资源对象都会包含的 metadata.resourceVersion 字段。 kind:Podmetadata:name:fooresourceVersion:57spec:#...status:#... 例如，你想要更新该 Pod 对象，其 metadata.resourceVersion: 57，APIServer 按此尝试将数据写入 etcd，而 etcd 中存储的该 Pod 为 metadata.resourceVersion: 58。 etcd 发现该资源版本与自己保存的不一致，那么就会拒绝写入。 写入冲突完全是很正常的事情，因此你编写的 Controller 代码中必须正确处理冲突的场景。 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:1:3","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":"1.4 Operator Operator 本质上就是一个或多个 Controller，只是比一般的 Controller 复杂，要负责一些产品的运维操作。 一般 Operator 包含： 一组 CRD，用于定义特定领域的 Schema。 一个或多个 Controller，用于完成特定的运维操作。 可以在 OperatorHub 来查找特定领域的发布的 Operator。 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:1:4","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":"2 API 基础 Kubernetes 的核心是 APIServer，其主要提供两个功能： 为所有组件提供 API。 提供 Proxy 功能。 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:2:0","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":"2.1 HTTP API 对于客户端来说，APIServer 提供了一个 RESTful HTTP API，可以返回 JSON/Protobuf 格式的内容。 编码方式 由于性能考虑，集群内部组件通信主要使用 Protobuf。而客户端访问为了通用性，默认使用 JSON 格式。 基于 REST，每个 HTTP 请求的通过不同的 HTTP Verb（也称 Method）来表示不同类型的操作： HTTP GET - 查询一个或多个特定的资源； HTTP POST - 创建资源； HTTP PUT - 修改已有的资源，提供资源完整的定义； HTTP PATCH - 部分更新已有的资源，只提供资源部分定义即可； HTTP DELETE - 删除一个资源； 你可以通过 API 参考文档 看到所有 HTTP API，以及对应的示例（注意版本）。 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:2:1","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":"2.2 API 基本概念 2.2.1 Kind Kind 表示一个对象的类型，每个对象都会包含 Kind 字段（JSON 中为 kind，Golang 中为 Kind）。 kind:Pod 目前存在着三种那类型的 kind： 特定的对象，例如 Pod； 对象的集合，例如 PodLists、NodeLists； 特定行为或者非持久化的实体，例如 /scale、Status； 2.2.2 API Group Group 为一组 Kind 的集合。例如，批量任务相关（Job 或者 ScheduledJob）都包含在 “batch” 这个 API Group。 2.2.3 Version 每个 API Group 可以有着多个 Version。 特定版本的（比如 v1beta1）创建的对象，可以在其他支持的版本中获取并使用，APIServer 支持将对象在多个版本间进行无损转换。 Storage Version 后续可以看到，APIServer 使用 “Storage Version” 作为多个版本之间的内部版本。 2.2.4 Resource 在 HTTP API Endpoint 中，会使用小写复数的单词，例如 …/pods。在 HTTP 中该 Endpoint 就被称为 Resource。 有些 Resource 有着更多的 Endpoint 来表示一些操作，例如 ../pod/nginx/port-forward、../pod/nginx/logs 等。这些 Endpoint 就被称为 SubResource。 CRD 中的 SubResource 现在，可以理解为啥 CRD 中有个 subresouces 字段了： # ...spec:group:stable.example.comversions:- name:v1subresources:status:{}scale:specReplicasPath:.spec.replicas Kind 与 Resource 是不一样的概念，区别主要在于： Resource 会对应一个 HTTP API Path； Kind 是用于保存在对应定义中的，也会保存在 etcd 中。 大多数情况下，Kind 与 Resource 是一对一的映射关系。 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:2:2","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Kubernetes 编程"],"content":"参考 OperatorHub API 参考文档 《Programming Kubernetes》 ","date":"2021-08-06","objectID":"/posts/cloud_computing/k8s_programming/1-basic/:3:0","tags":["k8s","云计算"],"title":"K8s 编程 - 1 - 基本概念","uri":"/posts/cloud_computing/k8s_programming/1-basic/"},{"categories":["Prometheus 学习"],"content":"Grafana 基础","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"1 基本概念 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:1:0","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"1.1 数据源 Data Source Grafana 支持多种不同类型的时序数据库，称为数据源Data Source。目前官方支持：Graphite、InfluxDB、OpenTSDB、Prometheus、Elasticsearch、CloudWatch。 可以将多个数据源的数据合并到一个单独的仪表盘（Dashboard）上，但是每个面板（Panel）都绑定到属于特定组织的特定数据源。 添加一个数据源时，分为两种访问模式： Server 模式（默认）：所有请求都从浏览器发送到 Grafana 后端服务器，后端服务器将请求转发到数据源。 Browser 模式：浏览器访问模式，所有请求直接从浏览器发送到数据源。 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:1:1","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"1.2 组织 Organization Grafana 可以支持创建多个组织Organization，每个组织可以有一个或多个数据源。所有的仪表盘都归特定组织拥有。 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:1:2","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"1.3 用户 User 用户User是 Grafana 的账户，一个用户可以隶属于一个或多个组织，通过角色（Role）为其分配不同级别的权限。 Grafana 也支持各种用户认证方式。 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:1:3","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"1.4 面板 Panel 面板Panel是最基本的可视化模块。每个面板根据查询编辑器（例如 Prometheus 就是 PromQL）查询数据，并提供展示图表。 目前，Grafana 支持的面板许多面板类型，最常用的就是 Time series，展示基于时间的样本数据。 可以通过向 Grafana 用户分享链接，或者使用快照功能将正在查看的数据编码为静态和交互式 JSON 文件，来向其他人分享面板以及数据。 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:1:4","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"1.5 行 Row 行Row用于在仪表盘界面组织多个面板，一般为 12 单位的宽度。 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:1:5","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"1.6 查询编辑器 Query Editor 每个面板都有着一个或多个 Query Editor，通过编写语句来读取数据源的样本，将其展示到面板上。 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:1:6","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"1.7 仪表盘 Dashboard 多个面板或者多个行构成一个仪表盘Dashboard，用于分类不同的指标项。 通过分享链接或者通过快照功能，可以向他人分析仪表盘以及数据。 你也可以仅仅通过 Export 功能，将仪表盘结构导出为 JSON 文件。他人可以通过 Import 该 JSON 文件直接构建对应的仪表盘。 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:1:7","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"2 定制图表 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:2:0","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"2.1 定制仪表盘 创建新的仪表盘 点击 “Create” 菜单栏创建新的 Dashboard。点击 “Setting” 按钮进行配置。 配置 General 项 General 项包含仪表盘名称和一些常规配置。 Name - 仪表盘名称 Description - 仪表盘描述 Tages - 仪表盘 label Editable - Editable 可以编辑仪表盘，Read-only 不允许编辑仪表盘 Timezone - 配置时区 Auto-refresh - 定制相对时间显示和自动刷新选项，默认即可 配置 Variables 选项 可以为仪表盘配置多个模板变量，在面板中可以使用模板变量，变量会在仪表盘顶部展示为下拉选择框。 Variables 的类型包括： Interval - 表示时间跨度，来动态改变表达式中的时间段 Query - 允许用户编写数据源查询语句，根据查询结果变为变量的可选值 Datasource - 允许用户通过变量更改整个仪表盘的数据源 Custom - 用户自定义特定的可选变量值 Constant - 定义隐藏常量 Ad hoc filter - 仅用于某些特定数据源 Text box - 具有可选默认值的文本输入字段 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:2:1","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"2.2 定制面板 2.2.1 Graph 面板 配置 Query 项 Data source - 使用的数据源 Query options - 设置查询数据相关的参数，例如最小时间间隔等 Query - 设置查询语句，展示的数据就来自于通过查询语句查询数据源 Legend format - 可视化图折线的名称 配置 Panel options Title - 面板名称 Description - 面板描述 配置 Axes Axes 用于配置坐标抽的显示。可以使用坐标轴左右两侧 Y 轴的单位。 配置 Legend 配置图例的显示方式，可以展示一些特殊的值。 配置 Display Display 选项用于战术显示的样式。 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:2:2","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"参考 《Prometheus 监控技术与实践》 ","date":"2021-07-17","objectID":"/posts/cloud_computing/prometheus-learning/grafana-basic/:3:0","tags":["prometheus","monitor"],"title":"Prom 学习 - Grafana 基础","uri":"/posts/cloud_computing/prometheus-learning/grafana-basic/"},{"categories":["Prometheus 学习"],"content":"PromQL 概念以及语法","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"1 数据类型 首先，还是先理解 时间序列 这个概念： ^ - │ . . . . . . . . . . . . . . . |.| . . . │ . . . . . . . . . . . . . . |.| . . . . │ . . . . . . . . . . . . . |.| . . . . │ . . . . . . . . . . . . . . |.| . . . v - \u003c------------------ 时间 ----------------\u003e 上图中，横轴是时间，纵轴是时间线，每一个点就是一个 样本。而 Prometheus 每次接受数据时，收到的就是图中纵向的一条线。而这一组同时间记录的数据，就是 时间序列。 在实际中，时间序列就是 Metric 项的不同 label 的组合。 PromQL 将数据归为以下四种类型： 即时向量instant vector - 同一时刻的一组时间序列。可以理解为就是瞬时的样本的集合。 注意是样本的集合，而不是一个样本，可以理解为时间序列。 区间向量range vector - 一定时间范围内的一组时间序列。可以理解为时间范围的样本的集合。 注意是时间范围内的样本集合，可以理解为连续的多组时间序列。 标量scalar - 纯量数据，仅仅是一个数字，没有时间戳。 字符串string - 未被使用的纯字符串值。 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:1:0","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"2 Selector ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:2:0","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"2.1 Instant Vector Selector Instant Vector Selector 查询某一时间（默认最新时间）的时间序列，并通过 label 对其进行样本筛选，得到一个即时向量。 通过 {} 中添加 label 匹配运算，来进行样本的筛选。多个匹配之间通过 , 号分隔。 运算符 匹配运算符 示例 = 相等匹配 job=“mysqld_node” != 不相等匹配 job!=“redis_expoter” =~ 正则匹配 job=~“node_.*” !~ 正则不匹配 job!~“nginx-.*” 例如，我们要查找 /mnt 目录下除了 /mnt/local_pv 下的所有挂载点的大小： node_filesystem_size_bytes{mountpoint=~\"/mnt/.*\",mountpoint!~\"/mnt/local_pv/.*\"} __name__ Metric Name 在 Prometheus 底层会使用 label “name” 记录，因此通过 {name=“xxx”} 可以筛选 Metric 项。 不过该操作会遍历所有的 Metric 来过滤，所以要注意性能。 默认会查询最新时间的时间序列，可以通过 offset 来进行基准时间偏移。如下示例查询当前时间前一天的样本： node_filesystem_size_bytes{mountpoint=~\"/mnt/.*\"}offset1d ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:2:1","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"2.2 Range Vector Selectors Range Vector Selectors 查询一段时间范围内的时间序列，并通过 label 匹配进行样本过滤，最后得到一个区间向量。 在末尾加上 [\u003cduration\u003e] 的方式来指定获取过去 duration 时间的样本。时间单位支持：s(秒) m(分) h(小时) d(天) w(周) y(年)。 例如，下面在前面基础上查询过去 1m 的样本： node_filesystem_size_bytes{mountpoint=~\"/mnt/.*\",mountpoint!~\"/mnt/local_pv/.*\"}[1m] Note 这里可以看到，即时向量是每个样本只有一个值，区间向量是每个样本包含一组的值。 同样，可以用 “offset” 来指定基准时间： node_filesystem_size_bytes{mountpoint=~\"/mnt/.*\",mountpoint!~\"/mnt/local_pv/.*\"}[1m]offset1d ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:2:2","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"3 聚合操作 Prometheus 提供了许多内置的聚合操作，聚合操作仅仅适用于一个即时向量。通过聚合操作，会得到一个新的即时向量。 Note 可以理解为对即使向量进行一些操作，得到一组新的即使向量。 聚合操作的语法如下： \u003caggr-op\u003e([param,] \u003cvector\u003e) [without/by \u003clabel list\u003e] aggr-op - 聚合操作名称 param - 传递给操作的参数 vector - 即时向量 without/by - 操作即时向量前，通过 label 匹配对样本进一步进行过滤/筛选 without 按匹配到的之外的 label 分组，输出保留 label by 按匹配到的 label 分组，输出保留 label Note 如果不使用 without 或者 by，默认为 by()，也就是去除所有的 label。 例如，下面使用 without 得到节点所有 CPU 样本的平均值： avg(node_cpu_seconds_total)without(cpu){instance=\"localhost:9101\",job=\"prometheus\",mode=\"idle\"}2701333.81{instance=\"localhost:9101\",job=\"prometheus\",mode=\"iowait\"}1696.59625{instance=\"localhost:9101\",job=\"prometheus\",mode=\"irq\"}0{instance=\"localhost:9101\",job=\"prometheus\",mode=\"nice\"}1214.26125{instance=\"localhost:9101\",job=\"prometheus\",mode=\"softirq\"}4898.343750000001{instance=\"localhost:9101\",job=\"prometheus\",mode=\"steal\"}3861.9112499999997{instance=\"localhost:9101\",job=\"prometheus\",mode=\"system\"}49185.7{instance=\"localhost:9101\",job=\"prometheus\",mode=\"user\"}101896.9975 without 去除了即时向量中所有样本的 “cpu” label，而按照剩下的 label 进行分组，相同的 label 样本进行聚合求平均值。 同样，使用 by 将所有节点所有 CPU 的按照 mode 分组，求平均： avg(node_cpu_seconds_total)by(mode){mode=\"user\"}101910.36750000001{mode=\"idle\"}2701600.49625{mode=\"iowait\"}1696.73375{mode=\"irq\"}0{mode=\"nice\"}1214.3175{mode=\"softirq\"}4899.33375{mode=\"steal\"}3862.2212499999996{mode=\"system\"}49191.16 by 将即时向量所有样本的其他 label 去除，仅仅保留 label “mode”，然后按照 “mode” 分组聚合样本，并求平均值。 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:3:0","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"3.1 聚合操作符 下面列出了 Prometheus 提供的聚合操作符： 名称 描述 sum(\u003cvector\u003e) 将即时向量所有样本值相加 max(\u003cvector\u003e) 得到即时向量所有样本的最大值 min(\u003cvector\u003e) 得到即时向量所有样本的最小值 avg(\u003cvector\u003e) 得到即时向量所有样本的平均值 stddev(\u003cvector\u003e) 标准差，对一组数字分布情况的统计度量，用于检查异常值 stdvar(\u003cvector\u003e) 标准方差，标准差的平方 count(\u003cvector\u003e) 计数，计算样本的数量 count_values(“label”, \u003cvector\u003e) 对相同的 value 进行计数，统计每一个样本值出现的次数。 count_values 为根据 value 添加由参数指定的 label bottomk(N, \u003cvector\u003e) 对样本值进行小到大排序，返回前 N 个样本（其实不是聚合操作，因此 without 与 by 仅仅用于排序分组） topk(N, \u003cvector\u003e) 对样本值进行大到小排序，返回前 N 个样本（其实不是聚合操作，因此 without 与 by 仅仅用于排序分组） quantile(N, \u003cvector\u003e) 在 (0, N * 100%) 中样本的分布次数，类似于 Histogram 的概念 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:3:1","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"3.2 聚合操作理解 这里总结一下聚合操作大概的含义： 通过 without 与 by 对即时向量中所有样本过滤/保留 label； 按照剩余的 label 进行分组，label 值相同的为同一组； 按照组的维度，对每组样本进行聚合操作； 最后按照组得到了新的即时向量； ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:3:2","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"4 PromQL 运算符 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:4:0","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"4.1 算术运算符 算术运算符对值进行算术运算，目前包含 6 种算术运算符： 运算符 描述 + 相加 - 相减 * 相乘 / 相除 % 求余 ^ 幂等运算 标量 与 标量 -\u003e 标量 (5+1)*3scalar18 即时向量 与 标量 -\u003e 即时向量 对即时向量中每个样本会与标量进行算术运算，因此得到一组新的值的即时向量。 node_disk_read_bytes_total/1024/1024{device=\"dm-0\",instance=\"localhost:9101\",job=\"prometheus\"}26745.40966796875{device=\"dm-1\",instance=\"localhost:9101\",job=\"prometheus\"}3.28515625{device=\"dm-2\",instance=\"localhost:9101\",job=\"prometheus\"}2623.3681640625{device=\"sda\",instance=\"localhost:9101\",job=\"prometheus\"}29390.4736328125 即时向量 与 即时向量 -\u003e 即时向量 两个即时向量进行算术运算时，会遍历左边即时向量中样本，按照 label 完全匹配右边即时向量样本。如果找到匹配的样本，那么进行值的运算，否则直接丢弃样本。 node_disk_read_bytes_total+node_disk_written_bytes_total{device=\"dm-0\",instance=\"localhost:9101\",job=\"prometheus\"}700855357952{device=\"dm-1\",instance=\"localhost:9101\",job=\"prometheus\"}3444736{device=\"dm-2\",instance=\"localhost:9101\",job=\"prometheus\"}240470945280{device=\"sda\",instance=\"localhost:9101\",job=\"prometheus\"}941364913152 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:4:1","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"4.2 关系运算符 关系运算符用于值之间的大小比较，目前包含 6 种关系运算符： 运算符 描述 == 相等 != 不等 \u003e 大于 \u003c 小于 \u003e= 大于等于 \u003c= 小于等于 标量 与 标量 -\u003e 标量 0(false)/1(true) 标量之间使用关系运算符必须加上 bool 修饰符。 99\u003e=bool80scalar1 即时向量 与 标量 -\u003e 即时向量 对即使向量使用关系运算是用于筛选样本的。对即使向量的每个样本，会去比较标量。如果比较结果为 true 那么就保留样本，否则就会丢弃样本。 node_disk_write_time_seconds_total\u003e=100node_disk_write_time_seconds_total{device=\"dm-0\",instance=\"localhost:9101\",job=\"prometheus\"}118100.283node_disk_write_time_seconds_total{device=\"dm-2\",instance=\"localhost:9101\",job=\"prometheus\"}10610.281node_disk_write_time_seconds_total{device=\"sda\",instance=\"localhost:9101\",job=\"prometheus\"}88011.634 即时向量 与 即时向量 -\u003e 即时向量 两个即时向量之前的关系运算，会遍历左边即时向量中样本，按照 label 完全匹配右边即时向量样本。如果找到匹配的样本，那么进行值的比较，否则直接丢弃样本。 label 完全匹配的样本比较如果结果为 true，那么保留样本，否则删除样本。 node_disk_written_bytes_total\u003enode_disk_read_bytes_totalnode_disk_written_bytes_total{device=\"dm-0\",instance=\"localhost:9101\",job=\"prometheus\"}673182207488node_disk_written_bytes_total{device=\"dm-2\",instance=\"localhost:9101\",job=\"prometheus\"}237730660864node_disk_written_bytes_total{device=\"sda\",instance=\"localhost:9101\",job=\"prometheus\"}910928728576 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:4:2","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"4.3 向量匹配 之前看到，两个即时向量之间做运算时，都是遵循 label 完全匹配的样本进行计算。这种即时向量的匹配规则称为 向量匹配。 Prometheus 提供了两种向量匹配模式： one-to-one 两个即时向量运算时，除了 name label，其他 label 完全匹配的样本进行计算。 可以使用关键字 on 指定用于匹配的 label，或者使用关键字 ignoring 忽略指定的 label. many-to-one 和 one-to-many 多对一和一对多匹配模式，可以理解为一个样本匹配到多个样本的 label。通过 group_left 或 group_right 修饰符明确指定哪个向量具有更高的基数。 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:4:3","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"4.4 逻辑运算符 逻辑运算符用于向量与向量之间，产生一个新的向量。Prometheus 提供了三种逻辑运算符：and、or、unless。 Note 逻辑运算符是以 label 来保留样本的，不会对值进行运算。 and and 逻辑运算符对两个即时向量进行交集运算。新的即时向量中包含 vector1 完全匹配 vector2 的样本。 or or 逻辑运算符对两个即时向量进行并集运算。新的即使向量中包含 vector1 的所有原始样本，以及 vector2 中不与 vector1 完全匹配的样本。 unless unless 逻辑运算符对两个即时向量进行差集运算。新的即使向量中，只包含 vector1 独有的样本。 Note 可以看到，逻辑运算就是数学集合的运算，相同元素的判断条件就是 label 完全匹配。 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:4:4","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"5 PromQL 函数 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:5:0","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"5.1 数学函数 abs(v vector) 输入即时向量，返回其每个值的绝对值。 sqrt(v vector) 对即时向量的每个样本值进行平方根。 round(v vector) 即时向量每个样本值四舍五入到最近的整数。 clamp_max(v vector, max scalar) 和 clamp_minx(min scalar) 对即时向量每个样本的值设置上限和下限。 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:5:1","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"5.2 时间函数 time() 查询的计算时间以秒为单位返回。 时钟和日历类函数 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:5:2","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"5.3 标签操作函数 label_replace(v vector, dst_label string, replacement string, src_label strig, regex string) 对即时向量 v，将正则表达式 regex 与 src_label 标签的值进行匹配。如果匹配，则将 dst_label 标签的值（不存在会添加）替换为 replacement 指定的。 label_join(v vector, dst_label string, separator string, src_label string, src_label_2 string …) label_join 使用分隔符 separator 将所有 src_label 的值连接在一起，变为 dst_label 的值。 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:5:3","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"5.4 Counter 指标增长率 increase(v range-vector) increase 函数只能对 counter 类型的区间向量。其获取区间向量中第一个和最后一个样本，并返回其增长量。 rate(v range-vector) rate 函数只能对 counter 类型的区间向量调用。用于计算区间向量中时间序列每秒的平均增长率。 irate(v range-vector) irate 函数计算区间向量中时间序列的每秒增长率。 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:5:4","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"5.5 Gauge 指标趋势变化预测 predict_linear(v range-vector, t scalar) predict_linear 基于区间向量，使用简单的线性回归预测时间序列 t 秒的值，从而对时间序列的变化趋势做出预测。 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:5:5","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"参考 《Prometheus 监控技术与实践》 ","date":"2021-07-08","objectID":"/posts/cloud_computing/prometheus-learning/promql/:6:0","tags":["prometheus","monitor"],"title":"Prom 学习 - PromQL","uri":"/posts/cloud_computing/prometheus-learning/promql/"},{"categories":["Prometheus 学习"],"content":"Prometheus 的基本概念","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"1 数据模型 Prometheus 保存的所有数据都是 时间序列time series 数据，也就是每个数据会带有一个时间戳。 通过图表可以理解其模型： 图表中每个点都是一个时间序列数据，也称为 样本Sample。 不同的 label 与 Metric name 组合标识了一个数据流，也就是上面图表的一条线。 所有的数据都表示 prometheus_http_request_total 项统计。 所以，其数据模型为： Metric - 统计项 Metric Name 与 Labels - 统计项下的一个数据流 Sample - 数据流中的某个数据 ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:1:0","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"1.1 Metric Metric 定义了某个监控指标，都由如下格式表示： \u003cmetric name\u003e{\u003clabel name\u003e=\u003clabel value\u003e, ...} 该格式标识也被称为 Notation。 Metric name 表示监控项的含义，Labels 反映了样本的特征维度： node_cpu_seconds_total{cpu=\"0\",mode=\"idle\"} 2.01377426e+06 node_cpu_seconds_total{cpu=\"0\",mode=\"iowait\"} 1639.03 node_cpu_seconds_total{cpu=\"1\",mode=\"idle\"} 1.9973224e+06 node_cpu_seconds_total{cpu=\"1\",mode=\"iowait\"} 1535.02 可以看到，Labels 是不同值的组合，都是一个 Metric 实例化。因此 Metric name 与 Labels 的组合才代表一个唯一的数据。 其中，__ 作为前缀的 label 是系统保留的，只能在系统内部使用。例如 Prometheus 底层实现中指标名称实际上是以 __name__=\u003cmetric name\u003e 形式保存的： {__name__=\"node_cpu_seconds_total\", cpu=\"0\",mode=\"idle\"} ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:1:1","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"1.3 Sample Sample 为样本，可以理解为某个采集项某次采集的数据。按照时间排序的 Sample 形成了实际的数据序列数据列表。 每个 Sample 包含三个部分： metric - metric name 与当前样本的 labelsets value - 采集的数据，float64 timestamp - 采集数据的时间，ms Prometheus 会按照时间顺序来存储 sample，如果 sample 不是顺序收集的，会将其丢弃。 ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:1:2","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"2 Metric 类型 目前，Prometheus 提供了 4 种 Metric 类型。 Counter Gauge Histogram Summary ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:2:0","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"2.1 Counter Counter 代表一个累加数据，通常用于跟踪事件次数，累计时间等。 其特点如下： 值只能从 0 开始增加，不能减少。 重启进程后，只会重置为 0。 通常，Counter 类指标会命名为 “xxx_total”。 # TYPE node_cpu_seconds_total counter node_cpu_seconds_total{cpu=\"0\",mode=\"idle\"} 2.01526517e+06 node_cpu_seconds_total{cpu=\"0\",mode=\"iowait\"} 1640.01 ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:2:1","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"2.2 Gauge Gauge 反映一个瞬时测量值，其值可能随着时间变化上下波动。 其特点如下： 测量值是瞬时值，可以任意变化。 重启进程后，会被重置。 Gauge 是适合记录无规律变化的数据。 # HELP node_load1 1m load average. # TYPE node_load1 gauge node_load1 1.6 # HELP node_load15 15m load average. # TYPE node_load15 gauge node_load15 1.01 ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:2:2","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"2.3 Histogram Histogram 表示直方图，会在一段时间范围内对数据进行采样，并将其计入 bucket 中。 histogram 中有三类值： bucket：值小于 “le” 下，统计到的数据次数 sum：统计值的累计和 count：统计次数 看个例子，下面数据表明了对于 / 的 HTTP 请求的 # HELP prometheus_http_request_duration_seconds Histogram of latencies for HTTP requests. # TYPE prometheus_http_request_duration_seconds histogram prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"0.1\"} 1 prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"0.2\"} 1 prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"0.4\"} 1 prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"1\"} 1 prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"3\"} 1 prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"8\"} 1 prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"20\"} 1 prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"60\"} 1 prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"120\"} 1 prometheus_http_request_duration_seconds_bucket{handler=\"/\",le=\"+Inf\"} 1 prometheus_http_request_duration_seconds_sum{handler=\"/\"} 0.000184525 prometheus_http_request_duration_seconds_count{handler=\"/\"} 1 bucket le=“0.1” - 请求处理时间小于 0.1s 的次数为 1 次 le=“0.2” - 请求处理时间小于 0.2s 的次数为 1 次 … le=\"+Inf\" - 请求处理时间小于 +Inf 的次数为 1 次 sum 所有请求的处理实际的总和为 0.000184525 count 统计的请求次数为 1 次 可以看到，对于 bucket 的统计结果是累计的。 ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:2:3","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"2.4 Summary summary 为概率图，包含了各百分比样本的值，也就是样本的值的分布区间。 summary 包含三类数据： bucket：百分比范围样本的值，“quantile” 表示百分比 sum：统计值的累计和 counter：统计次数 看个示例，下面数据表明了所有 wal_fsync 操作的耗时分布区间： # HELP prometheus_tsdb_wal_fsync_duration_seconds Duration of WAL fsync. # TYPE prometheus_tsdb_wal_fsync_duration_seconds summary prometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.5\"} 0.012352463 prometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.9\"} 0.014458005 prometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.99\"} 0.017316173 prometheus_tsdb_wal_fsync_duration_seconds_sum 2.888716127000002 prometheus_tsdb_wal_fsync_duration_seconds_count 216 bucket quantile=“0.5” - 50% 的 wal_fsync 操作耗时小于 0.012352463 quantile=“0.9” - 90% 的 wal_fsync 操作耗时小于 0.014458005 quantile=“0.99” - 99% 的 wal_fsync 操作耗时小于 0.017316173 sum 所有 wal_fsync 操作耗时总和 2.888716127000002 count 统计 wal_fsync 操作 216 次 可以看到，对于 bucket 的统计结果是累计的。 ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:2:4","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"3 任务模型 在 Prometheus 中，任何被采集的目标（往往是一个 IP:Port）被称为 Intsance。而将相同采集项进行分组，每个组就称为 Job。 看下 prometheus.yml 配置文件来理解这个概念： scrape_configs:- job_name:'prometheus'static_configs:- targets:['localhost:9090']- job_name:'node'static_configs:- targets:['localhost:9100'] scrape_configs.job_name 描述了一个脚本，这里有两个 job，分别表示采集 Prometheus 与 Node。 scrape_configs.static_configs.targets 中就表明了需要采集的 Instance。 Prometheus 拉取一个采集样本时，会自动在时序的基础上添加 \"job\" 与 \"instance\" 两个 label，用于识别被采集的目标。 prometheus_http_requests_total{code=\"200\", handler=\"/-/ready\", instance=\"localhost:9090\", job=\"prometheus\"} Note 如果这两个 label 已经存在了，那么会根据配置文件中的 honor_label 配置来决定如何处理。 对于每个被采集的 Instance，Prometheus 也会有着一些统计数据。 up - 如果 Instance 能够被正常采集，那么值为 1 ，否则为 0 up{instance=\"localhost:9090\", job=\"prometheus\"} 1 scrape_duration_seconds - 采集 Instance 累计的使用时间 scrape_duration_seconds{instance=\"localhost:9090\", job=\"prometheus\"} 0.006962203 scrape_samples_post_metric_relabeling - Instance 的采集数据被 relabel 后，剩余的采集数量累计 scrape_samples_post_metric_relabeling{instance=\"localhost:9090\", job=\"prometheus\"} 723 scrape_samples_scraped - 从该 Instance 采集数据的次数 scrape_samples_scraped{instance=\"localhost:9090\", job=\"prometheus\"} 723 ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:3:0","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Prometheus 学习"],"content":"参考 官方文档 Blog: 一文搞懂 Prometheus 的直方图 ","date":"2021-07-05","objectID":"/posts/cloud_computing/prometheus-learning/basic/:4:0","tags":["prometheus","monitor"],"title":"Prom 学习 - 基本概念","uri":"/posts/cloud_computing/prometheus-learning/basic/"},{"categories":["Kubernetes 学习"],"content":"1 概述 Kubernetes 中所有的资源访问与操作都是通过 API Server 执行的，因此 API Server 有着一套安全机制。 APIServer 的权限检查从大体上分为三个阶段： Authentication ：身份认证，用于验证请求者的身份是否合法。 Authorization ：权限认证，验证请求者是否包含对应资源的控制权限。 Admission Control ：可扩展的通用检查。 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"2 Subject 客户端访问 API Server 通常包含三种途径：kubectl、client 库、REST API。而这些此类请求的对象通常为：UserAccount 和 ServiceAccount。 Note UserAccount 与 ServiceAccount 仅仅是身份，而需要通过授权才能访问 APIServer。 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"2.1 UserAccount UserAccount 是独立于 Kubernetes 之外的用户账号，包括 负责分发私钥的管理员 类似 Keystone 或者 Google Account 这类的数据库 包含用户名密码的文件 Kubernetes 不会存储用户的信息，而是通过其 CA 证书来进行身份限制。证书中的 Subject Common Name 会被作用用户名，例如 “CN=bob”。 2.1.1 创建 UserAccount 下面尝试新建一个 User，让其能够访问 Kubernetes: 使用 Kubernetes 的根证书签发 UserAccount 使用证书： $ openssl genrsa -out shiori.key 2048 $ openssl req -new -key shiori.key -out shiori.csr -subj \"/CN=shiori\" $ openssl x509 -req -in shiori.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out shiori.crt -days 3650 在 Kubernetes 中创建一个新的用户，传入对应的证书与私钥 $ kubectl config set-credentials shiori --client-certificate=./shiori.crt --client-key=./shiori.key --embed-certs=true User \"shiori\" set. 创建一个 context，指定其用户。这样我们接下来来使用的就是新用户来访问 APIServer 了。 $ kubectl config set-context shiori@kubernetes --cluster=kubernetes --user=shiori $ kubectl config use-context shiori@kubernetes ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"2.2 ServiceAccount ServiceAccount 是 Kubernetes 管理的账号，提供给 Pod 里的进程使用，为 Pod 里的进程提供身份证明。 Note 一定要明确 ServiceAccount 对为了 Pod 中进程而设计的 一个 ServiceAccount 的基本定义如下： apiVersion:v1kind:ServiceAccountmetadata:name:local-storage-adminnamespace:kube-systemspec:automountServiceAccountToken:false spec.automountServiceAccountToken 表明不进行自动创建对应 Secret 对象 2.2.1 Service Account Atuh 在 Pod 中访问 Kubernetes APIServer 时，会使用 Kubernetes 内置的 HTTP Token 认证方式 Service Account Auth。 Note 使用 HTTP Bearer Token 的方式 对于 APIServer 的认证，Pod 内进程使用下发的根证书，来验证 APIServer 的证书是否合法。 根证书数据挂载到 Pod 内 /run/secrets/kubernetes.io/serviceaccount/ca.crt，可以看到其和 APIServer 启动使用的根证书内容是一致的。 $ cat /etc/kubernetes/pki/ca.crt $ kubectl exec -it ${POD} -c ${CONTAINER} -- cat /run/secrets/kubernetes.io/serviceaccount/ca.crt 对于 Pod 内进程的认证，通过 HTTP Header 中的 Token 字符串数据，来验证 Pod 内进程。 Token 为Kubernetes Controller 进程用 APIServer 的私钥（–service-account-private-key-file 指定）生成的一个 JWT Secret。 数据会挂载到 Pod 内 /run/secrets/kubernetes.io/serviceaccount/token 文件。 可以看到，使用 ServiceAccount 时涉及到的 Pod 内的三个文件： /run/secrets/kubernetes.io/serviceaccount/ca.crt /run/secrets/kubernetes.io/serviceaccount/token /run/secrets/kubernetes.io/serviceaccount/namespace（Client 使用该 namespace 作为调用 API 的参数） $ kubectl exec -it ${POD} -c ${CONTAINER} -- ls /run/secrets/kubernetes.io/serviceaccount ca.crt namespace token 2.2.2 ServiceAccount 与 Secret 对象 上述的文件都是 Kubernetes Secret 对象传递的，当创建一个 ServiceAccount 时，自动会创建对应的 Secret 对象。而 Secret 对象就包含这三个数据。 $ kubectl get secrets NAME TYPE DATA AGE default-token-4l585 kubernetes.io/service-account-token 3 16d $ kubectl get secrets default-token-4l585 -o yaml apiVersion: v1 data: ca.crt: ... namespace: ... token: ... kind: Secret metadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: a4d19cee-45e3-41ac-af66-ca8b070fa5d9 creationTimestamp: \"2021-06-16T09:25:37Z\" name: default-token-4l585 namespace: tidb-cluster-dev resourceVersion: \"3996078\" uid: a068d2ad-79b9-4b3c-be8c-c6e245c1865f type: kubernetes.io/service-account-token 可以看到，对应的 Secret 对象名为 \u003cservice_account\u003e-token-\u003crandom\u003e，type 为 kubernetes.io/service-account-token。Secret 中也通过两个 annotations 来表明其所属的 ServiceAccount。 2.2.3 使用 ServiceAccount Pod 定义中通过 spec.serviceAccountName 可以指定使用的 ServiceAccount（默认 default）。 spec:# ...serviceAccountName:myserviceaccount 接着，对应的文件就会被挂载到 Pod 内，那么我们使用 Client 库或者 HTTP 访问等方式，就可以访问 APIServer 了： $ TOKEN=$(cat token) \u0026\u0026 curl \"https://kubernetes.default/api/v1/namespaces/mycluster/pods/\" --cacert ./ca.crt -H \"Authorization: Bearer $TOKEN\" { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\": \"1589287\" }, \"items\": [ // … ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"3 Authentication APIServer 安全的第一步检查是 Authentication，用于验证请求的身份。 目前包含以下身份验证方式： X509 客户证书认证：基于 Kubernetes 根证书签名的双向认证方式 HTTP Bearer Token 认证：通过 Bearer Token 识别合法用户 OpenID Connect Token 第三方认证：通过第三方 OIDC 协议进行认证 Webhook Token 认证：通过外部 Webhook 服务进行认证 Authenticating Proxy 认证：通过认证代理程序进行认证 当开启了多个身份认证模块时，只需要任一模块认证通过。APISever 不会保证身份认证模块的运行顺序。 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"3.1 X509 客户证书认证 通过 APIServer 启动参数 \"–client-ca-file=SOMEFILE\"，可以启动客户端证书验证方式。其参数指定了用于验证客户端证书的根证书文件。 证书验证通过后，客户端证书中 Subject Common Name 会被用作请求的用户名，用于后续的授权检查。 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"3.2 HTTP Bearer Token 认证 通过 APIServer 启动参数 \"–token-auth-file=SOMEFILE\"，指定其使用的静态 Token 文件。文件为 CSV 文本格式，每行字段为： # token,user,uid[,groupnames] 31ada4df-abedx-a11z-123z-124sgtszxvr3,join,2,\"group1,group2\" token - Token 字符串 user - 用户名 uid - 用户 ID groupnames - 用户组 通过 HTTP 访问 APIServer 时，其 HTTP Header 中 Token 字段来表明客户身份。APIServer 通过读取该 Token 来保存的 Token 中进行验证，就可以检查其身份，并知晓对应的用户了。 ServiceAccount ServiceAccount 就是使用 Bearer Token 方式认证，只是其 Token 是由 Kubernetes 动态生成的。 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:3:2","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"3.3 OpenID Connect Token 第三方认证 Kubernetes 也支持使用 OpenID Conntect 协议（简称 OIDC） 进行身份验证，不过没有使用 OIDC 的权限管理。 用户通过 OIDC Sever 得到一个合法的 ID Token，请求时传递给 APIServer。APIServer 通过验证该 Token 是否合法来验证用户的身份。 要使用 OIDC Token 认证方式，APIServer 需要配置一些参数，见 配置 API 服务器 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:3:3","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"3.4 Webhook Token 认证 Kubernetes 也支持通过外部 Webhook 认证服务器，配置 HTTP Bearer Token 来实现自定义的用户身份认证功能。 其工作步骤如下： 开启并配置 API Server 的 Webhook Token Authentication 功能。 APIServer 接受到一个需要认证的请求后，从 HTTP Header 中取出 Token 信息，然后将包含该 Token 的 TokenReview 资源以 HTTP POST 方法发送到远程 Webhook 服务进行认证。 APIServer 根据 Webhook 返回的结果判断是否认证成功。 远端 Webhook 服务回复也需要是一个 TokenReview 资源对象，并且 apiVersion 要与 APIServer 发送的 apiVersion 一致。 要使用 Webhook Token 认证方式，APIServer 需要以下启动参数： \"–authentication-token-webhook-config-file\": 指向一个配置文件，文件描述了如何访问远程的 Webhook 服务 \"–authentication-token-webhook-cache-ttl\": 缓存 Webhook 服务返回的结果的时间，默认为 2min \"-authentication-token-webhook-version\": 发送给 Webhook 服务的 TokenReview 资源 API 版本号，“v1beta1” 或 “v1” 配置文件使用 kubeconfig 文件格式： apiVersion:v1kind:Configclusters:- name:name-of-remote-authn-servicecluster:certificate-authority:/path/to/ca.pem # 用来验证远程服务的 CAserver:https://authn.example.com/authenticate# 要查询的远程服务 URL。必须使用 'https'。users:- name:name-of-api-serveruser:client-certificate:/path/to/cert.pem# Webhook 插件要使用的证书client-key:/path/to/key.pem # 与证书匹配的密钥# kubeconfig Contextcurrent-context:webhookcontexts:- context:cluster:name-of-remote-authn-serviceuser:name-of-api-severname:webhook clusters: 设置 Webhook 远程服务 users: APIServer 使用的 Webhook 配置 APIServer 是收到请求，提取出 Token 后，将发送如下 TokenReview 资源对象并发送。 { \"apiVersion\": \"authentication.k8s.io/v1beta1\", \"kind\": \"TokenReview\", \"status\": { \"authenticated\": true, \"user\": { \"username\": \"janedoe@example.com\", \"uid\": \"42\", \"groups\": [ \"developers\", \"qa\" ], \"extra\": { \"extrafield1\": [ \"extravalue1\", \"extravalue2\" ] } } } } 回复中通过 status.authenticated 字段来表明是否授权成功： { \"apiVersion\": \"authentication.k8s.io/v1beta1\", \"kind\": \"TokenReview\", \"status\": { \"authenticated\": true, \"user\": { \"username\": \"janedoe@example.com\", \"uid\": \"42\", \"groups\": [ \"developers\", \"qa\" ], \"extra\": { \"extrafield1\": [ \"extravalue1\", \"extravalue2\" ] } } } } { \"apiVersion\": \"authentication.k8s.io/v1beta1\", \"kind\": \"TokenReview\", \"status\": { \"authenticated\": false } } ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:3:4","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"3.5 Authenticating Proxy APIServer 可以配置为从 HTTP Header（例如 X-Remote-User）来进行用户身份验证。由 Authenticating Proxy 程序设置 HTTP Header 相关的值。 首先，Authenticating Proxy 需要向 APIServer 配置对应的客户端 CA 证书，保证 Proxy 能与 APIServer 进行 HTTPS 连接。 通过以下 APIServer 启动参数配置： –requestheader-client-ca-file: Authenticating Proxy 客户端 CA 证书文件路径 –requestheader-allowed-names: （可选）设置允许的 Common Name 列表 –requestheader-username-headers: 设置用户名的 Header，通常为 “X-Remote-User” –requestheader-group-headers： 设置用户组的 Header，通常为 “X-Remote-Group” –requestheader-extra-headers-prefix: Header 字段前缀用于确定用户一些其他信息，通常为 “X-Remote-Extra-” HTTPS 连接建立后，APIServer 才会校验 HTTP Header 中设置的用户名。一个请求类似于： GET / HTTP/1.1 X-Remote-User: fido X-Remote-Group: dogs X-Remote-Group: dachshunds X-Remote-Extra-Acme.com%2Fproject: some-project X-Remote-Extra-Scopes: openid X-Remote-Extra-Scopes: profile ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:3:5","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"3.6 Anonymous Requests 请求中没有被任何已配置的身份认证方法拒绝，那么会被视为 Anonymous Requests。这类请求会被视为用户 system:anonymous 和 对应的用户组 system:unauthenticated。 1.6 版本后，如果所使用的鉴权模式不是 AlwaysAllow，则匿名访问默认是被启用的。对于匿名请求，ABAC 和 RBAC 要求必须给出显式的权限判定。 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:3:6","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"3.7 User Impersonation 一个用户可以通过 Impersonation Header 来以另一个用户身份执行操作。 用户发起 API 请求，提供自身的认证，同时提供要伪装的头部字段信息。 APIServer 对用户进行身份认证。 APIServer 确认用户有着伪装的权限。 请求的用户信息被替换为伪装的用户信息。 针对伪装的用户进行验证与授权管理。 可以通过一下 HTTP Header 来进行伪装： Impersonate-User：要伪装成的用户名 Impersonate-Group：要伪装成的用户组名。可以多次指定以设置多个用户组 Impersonate-Extra-\u003c附加名称\u003e：一个动态的头部字段，用来设置与用户相关的附加字段 要伪装为某个用户或用户组时，需要有着 “impersonate” 权限，利用 RBAC 设置对应的 Role 如下： apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:impersonatorrules:- apiGroups:[\"\"]resources:[\"users\",\"groups\",\"serviceaccounts\"]verbs:[\"impersonate\"] 使用 kubectl 时，可以通过 –as 参数设置 Impersonate-User，通过 –as-group 参数设置 Impersonate-Group。 $ kubectl drain mynode --as=superman --as-group=system:masters ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:3:7","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"4 Authorization 经过了身份认证后，APIServer 就会进行授权检查的流程，即通过授权策略（Authorization Policy）决定是否有权限进行调用。 APIServer 目前有以下授权策略： AlwaysDeny: 拒绝所有请求，仅用于测试 AlwaysAllow: 允许所有请求 ABAC：基于属性的访问控制 RBAC：基于角色的访问控制 Webhook：通过调用外部的 REST 服务对用户授权 Node：对 kubelet 进行的授权的特权模式 通过 APIServer 启动参数 \"–authorization-mode\" 配置多种授权策略。默认使用 Node 与 RBAC 策略。 --authorization-mode=Node,RBAC 当系统配置了多个授权模式时，Kubernetes 将按顺序使用每个模式。 如果任何鉴权模块批准或拒绝请求，则立即返回该决定，并且不会与其他授权模式协商。 如果所有模块对请求没有意见，则拒绝该请求。 当请求被拒绝时，会返回 HTTP Code 403。 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"4.1 ABAC 授权模式 ABAC（Attribute-Based Access Control） 是基于属性的访问控制。 4.1.1 授权策略文件 集群管理员通过启动参数 \"–authorization-policy-file=SOME_FILENAME\" 指定授权策略文件的路径。 授权策略文件每一行都是一个 Map 类型 JSON 对象，称为 “策略对象”。 {\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"user\": \"alice\", \"namespace\": \"somenamespace\", \"resource\": \"*\", \"apiGroup\": \"*\"}} {\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"group\":\"system:authenticated\", \"nonResourcePath\": \"*\", \"readonly\": true}} apiVersion：有效值为 “abac.authorization.kubernetes.io/v1beta1” kind: 有效值为 “Policy” spec.user: 用户名，来自于 –token-auth-file 文件中记录的 user spec.group: 设置为 “system:authenticated” 时，表示匹配所有已认证请求；设置为 “system:unauthenticated” 时，表示匹配所有未认证去哪个区 spec.readonly: 表明仅用于 get list watch 操作 匹配资源 spec.apiGroup: 表示匹配哪些 API Group spec.namespace: 表示允许访问哪个 namespace 下的资源 spec.resource: 表明要匹配的 API 资源对象 匹配非资源 spec.nonResourcePath: 如果匹配非资源，表明要匹配的请求路径 4.1.2 授权算法 APIServer 收到请求后，识别出请求的策略对象属性。 根据在策略文件中定义的策略，逐条进行匹配，以判断是否允许授权。 如果至少一条成功，那么就通过授权。 4.1.3 示例 Alice 可以对所有资源做任何事情： {\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"user\": \"alice\", \"namespace\": \"*\", \"resource\": \"*\", \"apiGroup\": \"*\"}} Kubelet 可以读写事件： {\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"user\": \"kubelet\", \"namespace\": \"*\", \"resource\": \"events\"}} 任何人都可以对所有非资源路径进行只读请求： {\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"group\": \"system:authenticated\", \"readonly\": true, \"nonResourcePath\": \"*\"}} {\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\", \"kind\": \"Policy\", \"spec\": {\"group\": \"system:unauthenticated\", \"readonly\": true, \"nonResourcePath\": \"*\"}} 4.1.4 对 Service Account 进行授权 ServiceAccount 会自动生成一个 ABAC 用户名，命名规则如下： system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccountname\u003e 因此，可以通过 ABAC 对某个 ServiceAccount 进行访问控制。例如希望 kube-system namespace 中的 “default” 有全部权限，修改策略文件: {\"apiVersion\":\"abac.authorization.kubernetes.io/v1beta1\",\"kind\":\"Policy\",\"spec\":{\"user\":\"system:serviceaccount:kube-system:default\",\"namespace\":\"*\",\"resource\":\"*\",\"apiGroup\":\"*\"}} 当然，需要重启 APIServer 来重新加载策略文件。 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:4:1","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"4.2 RBAC 授权模式 见 K8s 学习 - RBAC 授权机制 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:4:2","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"4.3 Webhook 授权模式 Webhook 模式使用参数 –authorization-webhook-config-file=SOME_FILE 来开启，文件为 kubeconfig 文件格式。 # Kubernetes API 版本apiVersion:v1# API 对象种类kind:Config# clusters 代表远程服务。clusters:- name:name-of-remote-authz-servicecluster:# 对远程服务进行身份认证的 CA。certificate-authority:/path/to/ca.pem# 远程服务的查询 URL。必须使用 'https'。server:https://authz.example.com/authorizeusers:- name:name-of-api-serveruser:client-certificate:/path/to/cert.pem# webhook plugin 使用 certclient-key:/path/to/key.pem # cert 所对应的 keycurrent-context:webhookcontexts:- context:cluster:name-of-remote-authz-serviceuser:name-of-api-servername:webhook user 设置了 API Server 的配置 cluster 设置了远端 Webhook 服务器的配置 4.3.1 请求 授权时，APIServer 会生成一个 SubjectAccessReview 对象，通过 JSON 格式以 HTTP POST 发送给 Webhook 服务器。 SubjectAccessReview 对象中包含了用户访问资源请求动作的描述，以及访问的资源信息。 对于资源的访问请求如下： { \"apiVersion\": \"authorization.k8s.io/v1beta1\", \"kind\": \"SubjectAccessReview\", \"spec\": { \"resourceAttributes\": { \"namespace\": \"kittensandponies\", \"verb\": \"get\", \"group\": \"unicorn.example.org\", \"resource\": \"pods\" }, \"user\": \"jane\", \"group\": [ \"group1\", \"group2\" ] } } spec.resourceAttributes 表明访问的资源的，以及访问的操作 spec.user 请求的用户 spec.group 请求的用户组 对于非资源的访问请求如下： { \"apiVersion\": \"authorization.k8s.io/v1beta1\", \"kind\": \"SubjectAccessReview\", \"spec\": { \"nonResourceAttributes\": { \"path\": \"/debug\", \"verb\": \"get\" }, \"user\": \"jane\", \"group\": [ \"group1\", \"group2\" ] } } spec.nonResourceAttributes 对 API 的访问与操作 Note 因为 apiVersion 使用 “authorization.k8s.io/v1beta1”，因此 APIServer 需要开启该 API Group（–runtime-config=authorization.k8s.io/v1beta1=true）。 4.3.2 回复 Webhook 服务器返回的也是一个 SubjectAccessReview 对象，但是需要设置其中的 status 字段。 允许访问（allowed=true） { \"apiVersion\": \"authorization.k8s.io/v1beta1\", \"kind\": \"SubjectAccessReview\", \"status\": { \"allowed\": true } } 不允许访问（allowed=false），但是如果有其他授权者，继续可以对请求进行授权 { \"apiVersion\": \"authorization.k8s.io/v1beta1\", \"kind\": \"SubjectAccessReview\", \"status\": { \"allowed\": false, \"reason\": \"user does not have read access to the namespace\" } } 不允许访问（allowed=false），同时立刻拒绝其他授权者（denied=true） { \"apiVersion\": \"authorization.k8s.io/v1beta1\", \"kind\": \"SubjectAccessReview\", \"status\": { \"allowed\": false, \"denied\": true, \"reason\": \"user does not have read access to the namespace\" } } ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:4:3","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"4.4 Node 授权模式 Node 授权模式仅仅针对 Subject 为 Node，专门对 kubelet 发起的 API 请求进行授权的管理模式。 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:4:4","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"5 Admission Control 经过身份认证与授权后，请求最后还要通过 Admission Control（准入控制） 的控制链。 Admission Control 有一个准入控制器的插件列表，发送给 APIServer 任何请求都需要经过列表中每个准入控制器的检查。任一一个控制器检查不通过，那么 APIServer 就会拒绝次调用。 准入控制器还能够修改请求参数，来实现一些自动化任务。 通过 APIServer 启动参数，可以配置哪些准入控制器启用，哪些准入控制器关闭： \"–enable-admission-plugins\" 参数配置开启的准入控制器： kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ... \"–disable-admission-plugins\" 用以配置关闭的准入控制器，以此可以关闭默认开启的准入控制器： kube-apiserver --disable-admission-plugins=PodNodeSelector,AlwaysDeny ... 目前，Kubernetes 内置的准入控制器有 30 多个，见：What does each admission controller do? ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"5.1 动态准入控制 除了内置的准入控制器，可以通过 Webhook 方式对接外部的 Admission Webhook 服务。 可以将其分为两类： Mutating Admission Webhook: 针对请求参数进行修改 Validating Admission Webhook：针对请求进程检查 具体部署方式见：动态准入控制 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:5:1","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Kubernetes 学习"],"content":"参考 官方文档：用户认证 Blog: 基于 oAuth2 的 OIDC 原理 学习笔记 ","date":"2021-07-03","objectID":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/:6:0","tags":["k8s","云计算"],"title":"K8s 学习 - 10 - API Server 认证","uri":"/posts/cloud_computing/k8s_learning/10-apiserver-auth/"},{"categories":["Golang"],"content":"Ginkgo 基本用法","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"1 概述 ginkgo 是一个 BDD 框架，Kubernetes 的 E2E 测试使用该框架实现集群的测试。 ginkgo 是集成在 Go 测试框架的，在目录下执行 ginkgo bootstrap 就会构建测试的入口： package ginkgo_test import ( \"testing\" . \"github.com/onsi/ginkgo\" . \"github.com/onsi/gomega\" ) func TestGinkgo(t *testing.T) { // Gomega 连接 Ginkgo // 使得 Gomega 断言能够通知到 Ginkgo RegisterFailHandler(Fail) // 测试入口 RunSpecs(t, \"Ginkgo Suite\") } 接着，我们在同目录下可以创建测试 Spec，然后通过 ginkgo 和 go test 命令就可以执行测试。 ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:1:0","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"2 构建 Spec 2.1 Describe Context It 为了更好的构建测试的结构，ginkgo 提供了三个接口来构建测试项的结构： Describe: 定义一个测试项 Context: 定义一个测试项下的一种情况 It: 真正执行的测试代码 以下面为例，Describe 定义了顶层的测试项与其中一个方面的测试项，两个 Context 为两个测试的情况，而 It 包含了具体的测试代码。 // Root Describe var _ = Describe(\"Book\", func() { // Sub Describe Describe(\"Categorizing book length\", func() { // Context1 Context(\"With more than 300 pages\", func() { It(\"should be a novel\", func() { // 测试代码 + 断言 }) }) // Context2 Context(\"With fewer than 300 pages\", func() { It(\"should be a short story\", func() { // 测试代码 + 断言 }) }) }) }) Note 也可以简单的将 Describe 与 Context 理解为将测试分类，复用一些描述情况。 ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:2:0","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"2.2 BeforeEach AfterEach BeforeEach 会在每个 It 执行之前执行，一般用于测试进行数据的初始化。 // Root Describe var _ = Describe(\"Book\", func() { var ( // 通过闭包在 BeforeEach 和 It 之间共享数据 longBook Book shortBook Book ) BeforeEach(func() { longBook = Book{ Title: \"Les Miserables\", Author: \"Victor Hugo\", Pages: 1488, } shortBook = Book{ Title: \"Fox In Socks\", Author: \"Dr. Seuss\", Pages: 24, } }) // Sub Describe Describe(\"Categorizing book length\", func() { // Context1 Context(\"With more than 300 pages\", func() { It(\"should be a novel\", func() { // 测试代码 + 断言 }) }) // Context2 Context(\"With fewer than 300 pages\", func() { It(\"should be a short story\", func() { // 测试代码 + 断言 }) }) }) }) 相反，AfterEach 会在每个 It 执行完成后执行，用于数据的销毁。 ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:2:1","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"2.3 JustBeforeEach JustAfterEach JustBeforeEach 会在每个 It 执行之前执行，在对应的 BeforeEach 之后执行。 JustBeforeEach 出现主要是为了抽象出初始化数据的逻辑，使得只需要 BeforeEach 提供初始化数据来源。 例如下面例子，过 JustBeforeEach 就可以将原来的两次 BeforeEach 中的 NewBookFromJSON 调用，减少为一次。 var _ = Describe(\"Book\", func() { var ( book Book err error json string ) BeforeEach(func() { // 准备数据 json = `{ \"title\":\"Les Miserables\", \"author\":\"Victor Hugo\", \"pages\":1488 }` }) JustBeforeEach(func() { // 执行数据构建 book, err = NewBookFromJSON(json) }) Describe(\"loading from JSON\", func() { Context(\"when the JSON parses succesfully\", func() { }) Context(\"when the JSON fails to parse\", func() { BeforeEach(func() { // 覆盖数据 json = `{ \"title\":\"Les Miserables\", \"author\":\"Victor Hugo\", \"pages\":1488oops }` }) }) }) }) 对应的，JustAfterEach 在每个 It 之后调用，在 AfterEach 之前调用。 ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:2:2","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"2.4 BeforeSuite AfterSuite BeforeSuite 会在所有的测试执行前执行，AfterSuite 在所有的测试执行后执行： func TestBooks(t *testing.T) { RegisterFailHandler(Fail) RunSpecs(t, \"Books Suite\") } var _ = BeforeSuite(func() { dbClient = db.NewClient() err = dbClient.Connect(dbRunner.Address()) Expect(err).NotTo(HaveOccurred()) }) var _ = AfterSuite(func() { dbClient.Cleanup() }) ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:2:3","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"2.5 SynchronizedBeforeSuite SynchronizedAfterSuite SynchronizedBeforeSuite 用于指定在主进程执行的函数，以及在各个子进程执行的函数。函数都在运行测试之前执行。 Note 如果没有通过 \"–nodes \" 指定进程数量，那么默认就是一个主进程，一个子进程执行测试。 因此，SynchronizedBeforeSuite/SynchronizedAfterSuite 类似于 BeforeSuite/AfterSuite 会被执行。 例如，下面示例在子进程创建前，运行创建数据库。在各个子进程运行测试前，执行创建 client。 var _ = SynchronizedBeforeSuite(func() []byte { // 在主进程中执行 port := 4000 + config.GinkgoConfig.ParallelNode dbRunner = db.NewRunner() err := dbRunner.Start(port) Expect(err).NotTo(HaveOccurred()) return []byte(dbRunner.Address()) }, func(data []byte) { // 在每个子进程中执行 dbAddress := string(data) dbClient = db.NewClient() err = dbClient.Connect(dbAddress) Expect(err).NotTo(HaveOccurred()) }) 相反的，通过 SynchronizedAfterSuite 来进程回滚。 var _ = SynchronizedAfterSuite(func() { // 在每个子进程中执行 dbClient.Cleanup() }, func() { // 在主进程中执行 dbRunner.Stop() }) ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:2:4","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"2.6 By By 函数用于添加一些块文档，如果测试失败时会打印出来。可以将其理解为错误日志。 var _ = Describe(\"Browsing the library\", func() { BeforeEach(func() { By(\"Fetching a token and logging in\") }) It(\"should be a pleasant experience\", func() { By(\"Entering an aisle\") }) }) ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:2:5","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"3 Spec Runner ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:3:0","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"3.1 Pending Spec 定义一个 Spec 或容器时，调用 P/X 前缀的接口，会定义一个 Pending Spec。默认不会执行 PDescribe(\"some behavior\", func() { ... }) PContext(\"some scenario\", func() { ... }) PIt(\"some assertion\") PMeasure(\"some measurement\") XDescribe(\"some behavior\", func() { ... }) XContext(\"some scenario\", func() { ... }) XIt(\"some assertion\") XMeasure(\"some measurement\") 通过命令行参数 –noisyPendings=false 可以将其执行。 ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:3:1","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"3.2 Skiping Spec 通过 Skip 接口，你可以在代码中运行时跳过某个 Spec。 It(\"should do something, if it can\", func() { if !someCondition { // 跳过此 Spec，不需要 Return 语句 Skip(\"special condition wasn't met\") } }) 或者，你可以通过 –skip= 跳过运行匹配的 Spec。 ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:3:2","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"3.3 Focused Specs 通过 F 前缀的接口，可以默认仅仅执行 Focused Spec FDescribe(\"some behavior\", func() { ... }) FContext(\"some scenario\", func() { ... }) FIt(\"some assertion\", func() { ... }) 或者，你可以在执行命令时通过 –focus= 指定运行 Spec。 ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:3:3","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"3.3 Parallel Specs 默认下，Ginkgo 执行测试是串行的，你可以通过 ginkgo -p 开启并行测试，Ginkgo 会自动创建适当数量的进程来并行执行。通过 ginkgo -nodes=N 也可以执行进程数量。 多个 go test 子进程会消费队列中的 Spec 并行执行。 ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:3:4","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"4 Gomega Gomega 库提供了断言的功能， ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:4:0","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"4.1 断言 Ω 与 Expect 都提供断言的功能，完全相同。 通过 Should/To 来进行 “应该” 逻辑的断言，通过 ShouldNot/NotTO/ToNot 进行 “不应该” 逻辑的断言。 Expect(ACTUAL).Should(Equal(EXPECTED)) Expect(ACTUAL).To(Equal(EXPECTED)) Expect(ACTUAL).ShouldNot(Equal(EXPECTED)) Expect(ACTUAL).ToNot(Equal(EXPECTED)) Expect(ACTUAL).NoTo(Equal(EXPECTED)) Should 等函数后跟着 Matcher interface 变量，代表一个表达式。 ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:4:1","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["Golang"],"content":"4.2 Matcher 4.2.1 判断类型与值相等 Equal 使用 reflect.DeepEqual 进行比较。 BeEquivalentTo 会先将 ACTUAL 转换为 EXPECTED 的类型，然后使用 reflect.DeepEqual 进行比较。 Expect(ACTUAL).Should(Equal(EXPECTED)) Expect(ACTUAL).Should(BeEquivalentTo(EXPECTED)) 4.2.2 接口相容 BeAssignableToTypeOf 仅仅用于判断能否将 EXPECTED 赋值给 ACTUAL。常用于判断 interface 是否满足。 4.2.3 空值与零值 BeNil 判断是否为 nil BeZero 判断是否为零值 Expect(ACTUAL).Should(BeNil()) Expect(ACTUAL).Should(BeZero()) 4.2.4 布尔值 BeTrue 判断为 true BeFalse 判断为 false Expect(ACTUAL).Should(BeTrue()) Expect(ACTUAL).Should(BeFalse()) 4.2.5 error 处理 HaveOccurred 判断 error 为 nil Succeed 判断 error 不为 nil MatchError 以判断 error 或者 string 是否相同 Expect(err).ShouldNot(HaveOccurred()) Expect(err).Should(Succeed()) Expect(err).Should(MatchError(\"an error\")) //asserts that err.Error() == \"an error\" Expect(err).Should(MatchError(SomeError)) //asserts that err == SomeError (via reflect.DeepEqual) 4.2.6 channel 处理 BeClosed 判断 channel 已经关闭，会读取 channel 数据来判断 Receive 判断 channel 中能否读取到数据 BeSent 判断 channel 能否无阻塞发送消息 Expect(ch).Should(BeClosed()) Expect(ch).Should(Receive(\u003coptionalPointer\u003e)) Expect(ch).Should(BeSent(VALUE)) 4.2.7 文件处理 BeAnExistingFile 判断文件/目录是否存在 BeARegularFile 判断是否为普通文件 BeADirectory 判断是否为目录 Expect(ACTUAL).Should(BeAnExistingFile()) Expect(ACTUAL).Should(BeARegularFile()) Expect(ACTUAL).Should(BeADirectory()) 4.2.8 字符串处理 ContainSubstring 判断是否包含子串 HavePrefix 判断是否包含前缀 HaveSuffix 判断是否包含后缀 MatchRegexp 进行正则匹配 Expect(ACTUAL).Should(ContainSubstring(STRING, ARGS...)) Expect(ACTUAL).Should(HavePrefix(STRING, ARGS...)) Expect(ACTUAL).Should(HaveSuffix(STRING, ARGS...)) Expect(ACTUAL).Should(MatchRegexp(STRING, ARGS...)) 4.2.9 JSON/XML/YAML 处理 MatchJSON 判断 JSON 是否相同 MatchXML 判断 XML 是否相同 MatchYAML 判断 YAML 是否相同 4.2.10 集合 string array map chan slice 处理 BeEmpty 判断为空 HaveLen 判断长度 HaveCap 判断容量 ContainElement 判断是否包含元素 BeElementOf 判断值是否在集合其中一个 ConsistOf 判断两个集合元素是否相同，不考虑顺序 HaveKey 判断 map 是否包含指定的 key HaveKeyWithValue 判断 map 是否包含指定的 key/value ints := []int{1, 2, 3} m := map[string]string{} Expect(ints).Should(BeEmpty()) Expect(ints).Should(HaveLen(1)) Expect(ints).Should(HaveCap(2)) Expect(ints).Should(ContainElement(3)) Expect(1).Should(BeElementOf(ints) Expect(ints).Should(ConsistOf(1, 2, 3)) Expect(ints).Should(ConsistOf([]int{3, 2, 1})) Expect(m).Should(HaveKey(\"a\")) Expect(m).Should(HaveKeyWithValue(\"a\", \"b\")) 4.2.11 数字/时间处理 BeNumerically 进行数值的比较，包含以下运算符: == 判断数值是否相等 ~ 判断数值是否相似（一定范围内） \u003e \u003e= \u003c \u003c= 比较大小 BeBetween 判断是否在范围内 BeTemporally 进行 time.Time 类型比较，方式与 BeNumerically 相同 a := 1 b := 2 Expect(a).Should(BeNumerically(\"==\", b)) Expect(a).Should(BeNumerically(\"~\", b, 1)) Expect(a).Should(BeNumerically(\"\u003e\", b)) Expect(a).Should(BeNumerically(\"\u003e=\", b)) Expect(a).Should(BeNumerically(\"\u003c\", b)) Expect(a).Should(BeNumerically(\"\u003c=\", b)) Expect(a).Should(BeBetween(0, 10)) 4.2.12 panic Panic 判断函数是否会发生 panic Expect(func(){}).Should(Panic()) 4.2.13 逻辑组合 SatisfyAll/And 进行逻辑与的组合 SatisfyAny/Or 进行逻辑或的组合 Expect(msg).To(And(Equal(\"Success\"), MatchRegexp(`^Error .+$`))) Expect(msg).To(Or(Equal(\"Success\"), MatchRegexp(`^Error .+$`))) ","date":"2021-06-27","objectID":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/:4:2","tags":["Test"],"title":"Ginkgo 学习","uri":"/posts/language/golang/ginkgo-%E5%AD%A6%E4%B9%A0/"},{"categories":["AWS 学习"],"content":"VPC 基本概念，包括：VPC、Subnet、ENI、ACL、Security Group 等","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"1 VPC 与 Subnet 云上的网络是由 VPCVirtual Private Cloud 与 Subnet 组成的： VPC 是针对 Region 级别，其由一个 CIDR Block 来指定 IP 地址范围。 Subnet 是 VPC 网段的子网，AZ 级别资源（无法跨 AZ 覆盖）。 EC2 Instance 启动会加入一个 Subnet，这样才能完成网络的连接。 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:1:0","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"1.1 Default 与 Nondefault 对于每个 Region，都会自动为账户创建一个 Default VPC。Default VPC 默认包含一下资源： Internet Gateway Default Network ACL 每个 AZ 一个 Default Subnet 当启动 Instance 没有指定 VPC 时，就会默认使用 Default VPC 与 Default Subnet。 相反，手动创建的 VPC 与 Subnet 都称为 Nondefault VPC 或者 Nondefault Subnet。 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:1:1","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"1.2 Public 与 Private 能够连接 Internet Gateway 的 Subnet 称为 Public Subnet。反之就称之为 Private Subnet。 同样，如果 VPC 流量可以转发到 Internet Gateway，表明可以连接到外网，称为 Public VPC。反之，流量无法通向外网，称为 Private VPC。 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:1:2","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"2 IP 类型 Private IP - VPC 范围内的内网 IP。 Instance 操作系统层面能够看到的都是 Private IP，Public IP 如要从 AWS 服务中读取。 Public IP - “临时”的公网 IP，Instance 重启后重新分配。 如果 Subnet 配置了 “Auto-assign public IPv4 address” 功能，或者创建 Instance 时指定，那么 Instance 会绑定 Public IP。 Elastic IP - 固定生命周期的公网 IP Elastic IP 独立于 Instance，通过绑定与解绑与 Instance 进行交互。 如果 VPC 有着 IPv6 地址范围，那么 Instance 能够被分配 IPv6 地址，也是公网 IPv6 地址。 Tip 下面所说的 IP 都是指 IPv4，IPv6 会显式说明。 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:2:0","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"3 Route Table VPC 内的路由规则都是由 Route Table 控制。 Main Route Table - VPC 创建后默认创建的全局路由表，所有未绑定 Route Table 的 Subnet 默认会使用 Main Route Table Custom Route Table - 自定义的 Route Table 如果一个 Subnet 绑定了 Custom Route Table，那么进出的流量就由该路由表控制，否则就由 Main Route Table 控制了。 按照 Route Table 关联的对象不同，又分为： Subnet Route Table ：关联到 Subnet Gateway Route Table ：关联到 Internet Gateway Local Gateway Route Table ：关联到 Local Gateway ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:3:0","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"3.1 Route Rule Route Table 由多个 Route Rule 组成。 Destination Target Status Propagated 172.31.0.0/16 local Active No 0.0.0.0/0 igw-3518715d Active No Destination ：要匹配流量的目的地址范围，包括 0.0.0.0/0 ：CIDR 地址范围 pl-id ：Prefix Lists Target ：转发的目标，包括 格式 目标 local Subnet 内部 igw-id Internet Gateway nat-gateway-id NAT Device vgw-id Virtual Private Gateway lgw-id Outposts Local Gateway cagw-id Carrier Gateway pcx-id VPC Peering Connection eigw-id Egress-only Internet Gateway tgw-id Transit Gateway eni-id Network Interface vpce-id Gateway Endpoint vpc-endpoint-id VPC Endpoint（网关路由表使用） Propagated ：是否允许 Virtual Private Gateway 将路由传播到 Route Table Route Table？ 可以发现，并不支持指定地址或者指定 Subnet 的路由，因为 VPC 下默认所有 Subnet 之间都是互通的。 为此，可以理解为 Route Table 是用于 Subnet 内将流量路由到 AWS 资源的。 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:3:1","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"4 Elastic Network Interface ENIElastic Network Interface 是 VPC 网络中的虚拟网卡。无论 Private IP 还是 Public IP，其都是将其绑定到 ENI 上的。 ENI 包含以下属性： 一个或多个 Private IP 一个或多个 Public/Elastic IP，每个 Public IP 都要对应一个 Private IP 一个或多个 IPv6 地址 MAC 地址 一个或多个 Security Group 每个 Instance 的第一个 ENI 称为 Primary Network Interface，可以来自于： 自动创建 - Instance 创建时自动创建的 ENI 指定 - Instance 创建时指定已经存在的 ENI 作为 Primary ENI 每个 Instance 可以被附加上多个 ENI，其数量上限取决与 Instance 类型，具体见 文档。 Note 大多数情况在描述 IP 与 Security Group 时都是直接说 Instance 时，而会忽略 ENI。 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:4:0","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"5 Security Group Security Group 用于定义进出 ENI 数据包的白名单。分为 Inbound rules 与 Outbound rules。 Inbound rules - 入 ENI 数据包的白名单。 默认规则为拒绝所有主动访问流量。需要通过配置 Inbound rules 来一个个打开配置。 Outbound rules - 出 ENI 数据包的白名单。 默认规则为允许所有出的流量。一旦配置了规则了，默认会为拒绝所有出的流量。 一个 Rule 类似于： Type Protocol Port Range Destination/Source Description SSH TCP 22 0.0.0.0/0 Allow SSH Type ：常用的端口，或者自己配置 Protocol ：协议，TCP/UDP/ICMP Port Range ：允许的端口范围，ALL 表示全部端口 Destination/Source ：匹配流量的 目的/源 地址范围 对于 Inbound Rule 就是 Source，对于 Outbound Rule 就是 Destination 特别的注意，Security Group 是有状态的： 对于 Outbound 流量，自动允许对应的 Inbound 流量 对于 Inbound 流量，自动允许对应的 Outbound 流量 \"对应的\"意思 所有 “对应” 就是指 Source IP/Port 与 Destination IP/Port 相反。类似于端口限制型的 NAT。 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:5:0","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"6 Network ACL Network ACL 用于控制出入 Subnet 的流量，也分为 Inbound rules 与 Outbound rules。 Inbound rules - 入 Subnet 数据包的过滤规则，支持 Allow 与 Deny 规则。 Outbound rules - 出 Subnet 数据包的过滤规则，支持 Allow 与 Deny 规则。 当创建 VPC 时，会自动创建一个默认 ACL，而用户可以自己创建 ACL 并绑定到 Subnet 上。 默认 ACL - VPC 创建时自动创建，默认允许所有出入流量。 未绑定 ACL 的 Subnet 自动关联到默认 ACL。 自定义 ACL - 用户手动创建的 ACL，默认拒绝所有出入流量。 与 Security Group 不同，Network ACL 是无状态的，出入数据包都会完全经过 Inbound/Outbound rules 的过滤。 Network ACL 的 Rule 类似于： Rule Number Type Protocol Port Range Destination/Source Allow/Deny 100 Custome TCP TCP 123 0.0.0.0/0 Allow * All traffic All All 0.0.0.0/0 Deny Rule Number ：规则的编号 Type ：常用的端口或者自定义 Protocol ：传输层协议 Port Range ：匹配的端口范围 Destination/Source ：匹配的 目的/源 地址范围，对于 Inbound Rule 就是 Source，对于 Outbound 就是 Destination Allow/Deny ：允许还是禁止 匹配流量时，会按照 Rule Number 从小到大的顺序进行匹配，如果匹配上某一条 Rule （无论 Allow 或 Deny）就不再匹配其他的 Rule 了。 预留 Rule Number 建议以 100 间隔来添加 Rule，为中间预留可插入的编号。 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:6:0","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"5 VPC 中的 DNS 对每个 VPC 会有一个 Route 53 Resolver 作为 DNS 服务器，会每个 Instance 的每个 IP 地址（不包括 IPv6）提供 DNS 主机名，包括内网 IP 与公网 IP。 Private DNS 主机名支持同 VPC 解析到对应的 IP，格式为： 对于 Region us-east-1，格式为 ip-\u003cprivate_ip\u003e.ec2.internal 其他 Region 形式为 ip\u003cprivate_ip\u003e.\u003cregion\u003e.compute.internal Public DNS 主机名支持外网访问，格式为： 对于 Region us-east-1，格式为 ec2-\u003cpublic-ip\u003e.compute-1.amazonaws.com 其他 Region 格式为 ec2-\u003cpublic-ip\u003e.region.compute.amazonaws.com 关闭 DNS 解析 可以通过设置 VPC 的 enableDnsSupport 属性来关闭 DNS 解析功能，通过设置 VPC 的 enableDnsHostnames 属性来不为 Public IP 提供 DNS 主机名。 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:7:0","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"参考 VPC 官方文档 ","date":"2021-06-25","objectID":"/posts/cloud_computing/aws_learning/3-vpc/:8:0","tags":["aws"],"title":"AWS 学习 - 3 - VPC 基本概念","uri":"/posts/cloud_computing/aws_learning/3-vpc/"},{"categories":["AWS 学习"],"content":"Region、AZ、DC","date":"2021-06-23","objectID":"/posts/cloud_computing/aws_learning/1-introduction/","tags":["aws"],"title":"AWS 学习 - 1 - 地理概念","uri":"/posts/cloud_computing/aws_learning/1-introduction/"},{"categories":["AWS 学习"],"content":"1 Region 将所有的资源都放在一个 Data Center（简称 DC）不能保证可用性，如果该数据中心出现问题，那么意味所有的应用程序出现问题。 为此，AWS 在世界各地不同地方构建了许多的 DC，每个地方称为 Region。例如 us-west、us-east、eu-west 等。 所有 Region 之间的网络是可以连通的，但是默认下 Region 之间网络是隔离的。该特性是为了数据处理的合法合规。 在学习一个服务时，很重要的一点就是知晓是 Regional，还是 Zonal。一个 Regional Service 表明是其 Region 下所有 AZ 共享的，由 AWS 保证了 Service 的高可用。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/aws_learning/1-introduction/:1:0","tags":["aws"],"title":"AWS 学习 - 1 - 地理概念","uri":"/posts/cloud_computing/aws_learning/1-introduction/"},{"categories":["AWS 学习"],"content":"2 Available Zone Region 由一组 Available Zone（简称 AZ），AZ 表示一个或一组分离的 DC。例如 us-west-1、us-west-2 等。 一个 Region 下的多个 AZ 之间相隔数十英里，该距离保证多个 AZ 之间通信还是低延时的。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/aws_learning/1-introduction/:2:0","tags":["aws"],"title":"AWS 学习 - 1 - 地理概念","uri":"/posts/cloud_computing/aws_learning/1-introduction/"},{"categories":["AWS 学习"],"content":"3 Region AZ DC 之间关系 三者之间的关系如下图： 一个 Region 包含多个 AZ，各个 AZ 之间物理位置上分离； 一个 AZ 包含一个或多个 IDC； 可以看到，为了容灾，业务应该至少在一个 Region 下的两个 AZ 中部署。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/aws_learning/1-introduction/:3:0","tags":["aws"],"title":"AWS 学习 - 1 - 地理概念","uri":"/posts/cloud_computing/aws_learning/1-introduction/"},{"categories":["AWS 学习"],"content":"4 Edge Location Edge Location 是分布在世界各地的边缘站点，用于支持一些加速用户访问的服务。 例如，CloudFront 与 Route53 都可以部署在 Edge Location。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/aws_learning/1-introduction/:4:0","tags":["aws"],"title":"AWS 学习 - 1 - 地理概念","uri":"/posts/cloud_computing/aws_learning/1-introduction/"},{"categories":["AWS 学习"],"content":"5 管理分区 AWS 按照合规性要求将全球服务分为三个管理分区，每个分区之间是完全独立的。包括： 海外分区 aws.amazon.com - 21 个 Region 组成，每个 Region 之间使用 AWS 自行维护的骨干网络相连； 中国分区 amazonaws.cn - 北京和宁夏两个 Region 组成，但是没有骨干网络相连接； govcloud ","date":"2021-06-23","objectID":"/posts/cloud_computing/aws_learning/1-introduction/:5:0","tags":["aws"],"title":"AWS 学习 - 1 - 地理概念","uri":"/posts/cloud_computing/aws_learning/1-introduction/"},{"categories":["Kubernetes 学习"],"content":"Pod 调度，抢占与驱逐","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"1 概述 无论是基本的副本控制器，还是自定义资源，其控制的底层 Pod 的调度都是都通过 Scheduler 完成的。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"2 Schedule ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"2.1 nodeSelector Pod 的 spec.nodeSelector 可以用于控制 Pod 能被调度到哪些节点上。其内容是一组 kv 键值对，只有节点 label 包含所有设定的 kv，才可以被调度 Pod。 apiVersion:v1kind:Podmetadata:name:nginxlabels:env:testspec:containers:- name:nginximage:nginximagePullPolicy:IfNotPresentnodeSelector:disktype:ssd # 只有 label 包含 disktype:ssd 的节点才能被调度 除了你手动为节点添加 label 外，每个节点会默认添加上一些 label： kubernetes.io/hostname failure-domain.beta.kubernetes.io/zone failure-domain.beta.kubernetes.io/region topology.kubernetes.io/zone topology.kubernetes.io/region beta.kubernetes.io/instance-type node.kubernetes.io/instance-type kubernetes.io/os kubernetes.io/arch ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"2.2 nodeName spec.nodeName 是最简单的选择节点方法，指定 Pod 只能在一个指定节点上运行。 apiVersion:v1kind:Podmetadata:name:nginxspec:containers:- name:nginximage:nginxnodeName:kube-01 # 指定调度到节点 kube-01 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"2.3 affinity 2.3.1 nodeAffinity spec.affinity.nodeAffinity 与 nodeSelector 类似，可以根据节点的 label 来控制 Pod 调度到哪些节点。 目前包含两种类型的节点亲和性： requiredDuringSchedulingIgnoredDuringExecution ：指定调度到的节点必须满足的条件，与 nodeSelector 一样但是表达性更高； preferredDuringSchedulingIgnoredDuringExecution ：指定调度到节点的偏好条件，也就是优先调度到满足条件的节点； 只影响调度 目前两种类型节点亲和性都仅仅影响调度时的选择，而不会驱逐已经运行的 Pod。 apiVersion:v1kind:Podmetadata:name:with-node-affinityspec:affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:# 必须满足条件nodeSelectorTerms:- matchExpressions:- key:kubernetes.io/e2e-az-nameoperator:Invalues:- e2e-az1- e2e-az2preferredDuringSchedulingIgnoredDuringExecution:# 优先级条件- weight:1preference:matchExpressions:- key:another-node-label-keyoperator:Invalues:- another-node-label-valuecontainers:- name:with-node-affinityimage:k8s.gcr.io/pause:2.0 nodeSelectorTerms 下的数组之间是 “或” 关系，也就是满足其中一个条件就可以被调度。 matchExpressions 下的数组之间是 “与” 关系，需要满足所有条件才可以被调度。 weight 字段范围 1-100，如果满足其指定的条件，那么节点优选算分时就会加上 weight 的值。 2.3.2 Pod 亲和性与反亲和性 spec.affinity.podAffinity 亲和性允许根据节点上已经运行的 Pod 的 label 来控制是否调度到该节点。 Pod 亲和性也包含两种类型： requiredDuringSchedulingIgnoredDuringExecution ：必须满足的条件 preferredDuringSchedulingIgnoredDuringExecution ：优选的条件 spec.affinity.podAntiAffinity 与亲和性相反，表明将 Pod 尽量与其他 Pod 分开部署。 对于 Pod 亲和性与反亲和性，判断范围都是针对拓扑域来说的。通过 topologyKey 指定判断拓扑域的 label，具有相同 : 的节点会认为属于用一个拓扑域下： topologyKey:topology.kubernetes.io/zone # 如果两个节点具有相同的 topology.kubernetes.io/zone:\u003cval\u003e 的 label，那么它们属于同一个拓扑域。 所以，节点亲和性的规则为：对将被调度的节点，如果其相同拓扑域下的某个节点运行着满足条件的 Pod，那么就可以调度到该节点。 对应的，节点反亲和性的规则为：对将被调度的节点，如果其相同拓扑域下的某个节点运行着满足条件的 Pod，那么就尽量不要调度到该节点。 apiVersion:apps/v1kind:Deploymentmetadata:name:web-serverspec:selector:matchLabels:app:web-storereplicas:3template:metadata:labels:app:web-storespec:affinity:podAntiAffinity:requiredDuringSchedulingIgnoredDuringExecution:- labelSelector:matchExpressions:- key:appoperator:Invalues:- web-storetopologyKey:\"kubernetes.io/hostname\"# 拓扑域为节点podAffinity:requiredDuringSchedulingIgnoredDuringExecution:- labelSelector:matchExpressions:- key:appoperator:Invalues:- storetopologyKey:\"kubernetes.io/hostname\"containers:- name:web-appimage:nginx:1.16-alpine 上面例子中，podAntiAffinity 表明不要调度到同节点已经运行着 app:web-store 的 Pod 的节点上，podAffinity 表明调度到同节点运行着 app:store 的 Pod 的节点上。通俗点说，该 Pod 不能重复部署在同一个节点，并且每次部署要与 app:store 的 Pod 绑定。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"2.4 taint 与 tolerations 与 affinity 相反，taint 使节点排斥一类特定的 Pod。 为了能使 taint 节点能够被调度到一些特殊的 Pod，可以设置 Pod 的 toleration，表明不在意某些节点的 taint 。 2.4.1 taint 通过 kubectl taint 为节点增加一个 taint： $ kubectl taint nodes node1 key1=value1:NoSchedule 为 node1 添加 key1:value1 的 traint，其触发的效果是不能被调度（NoSchedule） 当然，你也可以为删除某个节点的 taint： kubectl taint nodes node1 key1=value1:NoSchedule- 结尾的 - 号表示是删除一个 taint； 设置的 kv 对用于来判断 toleration 是否匹配 taint。 目前包含几种类型的 effect ： NoSchedule ：不将 Pod 调度到该节点，但是不影响已经运行的 Pod； PreferNoSchedule ：尽量不降 Pod 分配到该节点，是个软性条件； NoExecute ：不将 Pod 调度到该节点，并且会驱逐已经运行并且不能容忍污点的 Pod； Kubernetes 会在一些条件下，自动会节点添加一些内置的污点： node.kubernetes.io/not-ready + NoExecute ：节点为准备好，Ready 为 false； node.kubernetes.io/unreachable + NoExecute ：节点不可达，Ready 为 unknown； node.kubernetes.io/memory-pressure + ：节点存在内存压力； node.kubernetes.io/disk-pressure + NoSchedule ：节点存在磁盘压力； node.kubernetes.io/pid-pressure + NoSchedule ：节点 PID 压力； node.kubernetes.io/network-unavailable + NoSchedule ：节点网络不可用； node.kubernetes.io/unschedulable + NoSchedule ：节点不可调度； node.cloudprovider.kubernetes.io/uninitialized + NoSchedule ：节点未被云平台初始化； DaemonSet 创建的 Pod DaemonSet 创建的 Pod 会自动加上上面两个 NoExecute 的 taint，使得其 Pod 不会被驱逐。 2.4.2 tolerations 当 Pod 设置的 spec.tolerations 能够 “匹配” 节点某个 taint 时，就可以认为该 taint 不存在。 “匹配” 有两个含义： 如果 operator 是 Exist，那么相同的 key 即可。如果 operator 为 Equal，那么 key val 都要相同 effect 相同 apiVersion:v1kind:Podmetadata:name:nginxlabels:env:testspec:containers:- name:nginximage:nginximagePullPolicy:IfNotPresenttolerations:- key:\"example-key\"operator:\"Exists\"effect:\"NoSchedule\" 可以容忍 key 为 “example-key”，effect 为 “NoSchedule” 的 taint； 通过 spec.tolerations.tolerationSeconds 可以指定匹配到容忍的污点后，能够持续容忍的时间。 tolerations:- key:\"key1\"operator:\"Equal\"value:\"value1\"effect:\"NoExecute\"tolerationSeconds:3600# 匹配到 taint 后，3600 内不会被驱逐 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:2:4","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"3 Eviction ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"3.1 节点压力驱逐 kubelet 会监控 CPU、Mem、磁盘空间、文件系统 inode 数量等资源，一旦某个资源消耗达到一个阈值，kubelet 会主动驱逐节点上的一个或多个 Pod，以回收资源。 驱逐时，kubelet 会将 Pod 设置为 Failed 状态，并停止 Pod，而上层的副本控制器可能会在其他地方创建 Pod 来替代。 驱逐的阈值分为： soft eviction thresholds ：达到软阈值后并持续了一段时间没有恢复，就会通过 graceful 的方式驱逐一些 Pod； hard eviction thresholds ：一旦触发阈值，立即通过 force 方式进行驱逐； 3.1.2 配置驱逐参数 驱逐相关的配置参数都需要配置 kubelet 的启动参数来进行配置。 3.1.3 驱逐策略 kubelet 会按照下面参数来决定驱逐 pod 的顺序： Pod 资源使用量是否超过 spec.request； Pod 优先级； Pod 相对于 spec.request 的资源使用情况； 因此，kubelet 会按照下面顺序进行驱逐：。 如果 BestEffort 或者 Burstable Pod 资源使用量超过 request。超出的越多的 Pod 优先被驱逐； 如果都是 Guaranteed 和 Burstable Pod 并小于 request，那么基于 Pod Priority 驱逐。 BestEffort Burstable Guaranteed 是 QosClass。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"3.2 API 驱逐 与节点压力驱逐的不同，API 驱逐是指通过 Eviction API 来进行主动的驱逐，并且停止 Pod 是 graceful。 kubectl drain 就是通过 API 进行驱逐，停止某个节点上的所有 Pod。 API 驱逐会受到 PodDisruptionBudgets 和 terminationGracePeriodSeconds 的控制。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:3:2","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"3.3 污点驱逐 在第 2 部分看到，自定义的污点也会导致 Pod 的驱逐，不能容忍 NoExecute 污点的 Pod 都会被驱逐。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:3:3","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"4 Preemption Pods 可以被提供一个优先级。高优先级的 Pod 会被优先调度，scheduler 甚至会尝试抢占低优先级的 Pod，来让高优先级的 Pod 先运行。 要使用优先级与抢占功能： 创建 PriorityClass； Pod 或者 Pod template 定义中指定 spec.priorityClassName 为一个特定的 PriorityClasses。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"4.1 PriorityClass PriorityClass 是一个 non-namespaced 资源，其包含一个 value 来描述优先级。 Kubernetes 内置两个 PriorityClass ：system-cluster-critical system-node-critical，表明是系统关键的组件、 一个基本的 PriorityClass 优先级如下： apiVersion:scheduling.k8s.io/v1kind:PriorityClassmetadata:name:high-priorityvalue:1000000globalDefault:falsedescription:\"This priority class should be used for XYZ service pods only.\" value ：优先级值，32 位整型，越大表示优先级越高。 globalDefault ：是否是系统默认优先级，没有指定 PriorityClass 的 Pod 使用默认优先级； 如果系统没有设置 globalDefault，那么默认优先级是 0。 description ：文本描述； ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:4:1","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"4.2 Pod 优先级 创建 Pod 时通过指定 spec.priorityClassName 来指定一个特定的 PriorityClass。 apiVersion:v1kind:Podmetadata:name:nginxlabels:env:testspec:containers:- name:nginximage:nginximagePullPolicy:IfNotPresentpriorityClassName:high-priority 当 Pod 优先级设置后，scheduler 会按照优先级对 pending Pods 进行排序，高优先级的 pending Pod 优先于低优先级的进行处理。 如果高优先级的 Pod 无法被调度到节点，那么 scheduler 才会继续调度到低优先级 Pod。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:4:2","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"4.3 抢占 当 scheduler 发现一个 Pod 无法被调度到任何节点时，就会触发抢占的逻辑。scheduler 会尝试计算：是否移除某个节点的一个或多个低优先级的 Pod，使得节点能够满足被调度的条件。 如果能够找到该节点，新 Pod 状态信息中的 nominatedNodeName 为被设置为节点名，使得用户可以看到抢占信息。 之后，节点上被低优先级的 Pod 会被驱逐（graceful stop 30s）。因此，这里会导致新 Pod 调度到该节点之间有一个需要等待的时间差。 所以，Nominated Node 这不代表新 Pod 必定会调度到该节点，也许驱逐期间出现别的节点满足调度条件，那么就会被调度。 如果新 Pod 与将被驱逐的 Pod 之间有 pod affinity 关系，那么抢占后亲和性关系就不再会被满足，因此 scheduler 不会选择这样的节点来进行抢占。同样，推荐在同优先级或者高优先级的 Pod 间设置 pod affinity。 同样，针对拓扑域下的 pod affinity，也会有上述的问题，因此 scheduler 不会进行跨节点抢占。 ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:4:3","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"参考 Blog：k8s 节点资源预留与 pod 驱逐 官方文档：Scheduling, Preemption and Eviction ","date":"2021-06-23","objectID":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - 9 - Schedule Preemption Eviction","uri":"/posts/cloud_computing/k8s_learning/9-schedule-preemption-eviction/"},{"categories":["Kubernetes 学习"],"content":"API 版本管理规则与扩展方式","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"1 概述 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"1.1 组织对象的方式 Kubernetes 中，组织对象的方式，就是按照 Group、Version、Resource 三个层级。 Group 用以来对 API 进行分组（分类）； Version 用以对相同的 API 进行版本控制； Resource 代表着一个具体的资源对象的 API； etcd 中如何组织对象 这不是在 etcd 中组织资源对象的方式，etcd 中还是按照资源类型，以及命名来进行分类存储 $ alias edctl=\"etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key $edctl get /registry --prefix --keys-only /registry/pingcap.com/tidbinitializers/mycluster/mycluster-init /registry/pingcap.com/tidbmonitors/mycluster/mycluster /registry/pods/kube-system/coredns-7c7788d75c-cggn5 … 这三个层次，也会体现在资源定义的 yaml 文件中： apiVersion:batch/v2alpha1 # Group/Versionkind:CronJob # Resource# … 而 Kubernetes 就是通过比较 Group Version Resource，再加上资源对象的 name 来寻找一个资源。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:1:1","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"1.2 REST 风格 Kubernetes 系统大部分情况下，API 定义和标准都符合 HTTP REST 标准，即通过 POST、PUT、GET、DELETE 等 HTTP method 来进行对象的增删改查操作。 目前，Kubernetes 使用 OpenAPI 文档格式生成接口文档。你可以通过 https://\u003cmaster_ip\u003e:\u003cmaster_port\u003e/openapi/v2 来访问 API 文档。 # 获取 TOEKN $ TOKEN=$(kubectl describe secrets $(kubectl get secrets -n kube-system |grep admin |cut -f1 -d ' ') -n kube-system |grep -E '^token' |cut -f2 -d':'|tr -d '\\t'|tr -d ' ') # 获取 APIServer 地址 $ APISERVER=$(kubectl config view |grep server|cut -f 2- -d \":\" | tr -d \" \") $ curl -H \"Authorization: Bearer $TOKEN\" $APISERVER/openapi/v2 -k | jq | more { \"swagger\": \"2.0\", \"info\": { \"title\": \"Kubernetes\", \"version\": \"v1.21.1\" }, \"paths\": { # ... ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:1:2","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"2 版本控制 Kubernetes 对于版本的控制体现在 URL 中，每个 API 对应的 URL 基于 /apis/\u003cgroup\u003e/\u003cversion\u003e/namespaces/\u003cns\u003e/\u003cresource\u003e 的格式构建，所以不同的版本有着不同的 URL。 对于一个特定的版本，例如 v1，包含三种级别的版本： Alpha ：预览版本 version 以 \u003cv\u003ealpha\u003cnr\u003e 格式，例如 v1alpha1。 表明软件是不稳定，软件可能会有 Bug，某些新增特性支持可能随时被删除，某些新增 API 可能会出现不兼容性更改。 因此仅仅使用于测试集群，不适合生产环境。 Beta ：测试版本 version 以 \u003cv\u003ebeta\u003cnr\u003e 格式，例如 v2beta3。 表明软件经过测试，并且特性都是安全的。如果特性与 API 发生兼容性更改，那么会提供迁移的说明。 GA ：稳定版本 verison 以 \u003cv\u003e 格式，例如 v1。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"3 API Groups 为了更容易扩展 API，Kubernetes 将 API 分组为多个逻辑集合，称为 API Groups。 当前支持两类的 API Groups： Core Groups（核心组） 又称为 Legacy Groups，大部分核心资源对象都在该组里，例如 Container、Pod、Service 等。 其 URL 路径前缀为 /api/v1。其 Group 为空，因此使用 spec.apiVersion : v1。 其他 API Groups URL 路径前缀为 /apis/\u003cGroup\u003e/\u003cVersion\u003e，使用 spec.apiVersion: \u003cGroup\u003e/\u003cVersion\u003e。 常见的分组包括： apps/v1 ：主要包含与用户发布、部署有关资源，包括 Deployments，RollingUpdates，ReplicaSet。 extensions/\u003cVersion\u003e ：扩展 API 组，包括 DaemonSet、ReplicaSet，Ingresses。 batch/\u003cVersion\u003e ：批处理和作业任务的对象，包括 Job。 autoscaling\u003cVersion\u003e ：HPA 相关资源对象。 certificate.k8s.io/\u003cVersion\u003e ：集群证书操作相关资源对象。 rbac.authorization.k8s.io/v1 ：RBAC 权限相关资源对象。 policy/\u003cVersion\u003e ：Pod 权限性相关的资源。 其他 CRD 定义的分组 Kubernetes 内置的所有 API Group 可以见 API 参考文档。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"3.1 启用或禁用 API Group 所有 API Group 默认情况都是可用的。可以通过 API Server 启动配置开启用或禁用某个 API 组。 例如 –runtime-config=batch/v1=false 表明禁用 batch/v1 下的所有 API。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"4 对象 Kubernetes API 所有资源类型都是以 对象 方式看待，并且以 REST 方式提供 HTTP API 接口。 每个对象的 API 以以下格式表示： non-namespace 资源： GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/\u003cResource\u003e ：返回资源集合 GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/\u003cResource\u003e/\u003cName ：操作指定 name 的资源对象 namespace 资源： GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/\u003cResource\u003e ：返回所有 namespace 下资源对象 GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/namespaces/\u003cNamespace\u003e/\u003cResource\u003e ：返回指定 namespace 下的所有资源对象 GET /apis/\u003cGroup\u003e/\u003cVersion\u003e/namespaces/\u003cNamespace\u003e/\u003cResource\u003e/\u003cName\u003e ：返回 namespace 下指定 name 的资源对象 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"4.1 watch 对象 基于 etcd 的机制，每个 Kubernetes 对象都有一个 resourceVersion 字段，代表着资源在 etcd 中存储的版本。 因此，在 client 的 watch 连接断开，重新连接后，可以通过指定 resourceVersion 来得到断连期间其对象所有的更改。 GET /api/v1/namespaces/test/pods?watch=1\u0026resourceVersion=10245 --- 200 OK Transfer-Encoding: chunked Content-Type: application/json { \"type\": \"ADDED\", \"object\": {\"kind\": \"Pod\", \"apiVersion\": \"v1\", \"metadata\": {\"resourceVersion\": \"10596\", ...}, ...} } { \"type\": \"MODIFIED\", \"object\": {\"kind\": \"Pod\", \"apiVersion\": \"v1\", \"metadata\": {\"resourceVersion\": \"11020\", ...}, ...} } ... 当然，etcd 只会保存一定时间内的资源变更历史（默认 5min）。如果请求的历史版本不存在，那么会返回 HTTP Code 410 Gone。 所以 client 必须能够处理 410 错误，清理本地对象缓存，调用 list 操作，重新建立 watch。 Reflector 官方提供了封装好这些功能的代码组件，例如 Go 中 Reflector 组件已经实现了这些逻辑 为了解决历史窗口太短的问题，为 event 引入了 bookmark event 的概念。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:4:1","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"4.2 查询对象 一般情况下，查询对象只需要使用 HTTP GET URL 就可以对象集合或者指定对象。 不过有时候，当你查询大量的对象时，可能返回的数据很大，使得浪费网络资源。从 1.9 开始，Kubernetes 支持分段查询对象。 在进行查询请求时，可以指定 limit 和 continue 参数。 查询 500 个 Pod，指定 “limit=500”。 GET /api/v1/pods?limit=500 --- 200 OK Content-Type: application/json { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\":\"10245\", \"continue\": \"ENCODED_CONTINUE_TOKEN\", ... }, \"items\": [...] // returns pods 1-500 } 继续前面调用，查询剩余的 Pod，指定 “continue=ENCODED_CONTINUE_TOKEN” 表明继续上一次查询。 GET /api/v1/pods?limit=500\u0026continue=ENCODED_CONTINUE_TOKEN --- 200 OK Content-Type: application/json { \"kind\": \"PodList\", \"apiVersion\": \"v1\", \"metadata\": { \"resourceVersion\":\"10245\", \"continue\": \"\", // continue token is empty because we have reached the end of the list ... }, \"items\": [...] // returns pods 1001-1253 } 注意，当你使用 continue 继续查询时，查询的永远是同一个 resourceVersion 的对象。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:4:2","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"4.3 对象表现形式 默认 Kubernetes 都是通过 JSON 格式来进行数据的传输。通过下面方式，允许 client 使用 protobuf 形式进行数据传输。 请求数据时，添加头部 Accept: application/vnd.kubernetes.protobuf 表明期望返回 protobuf 类型数据； 发送数据时，通过指定 Content-Type: application/vnd.kubernetes.protobuf 表明传输的是一个 protobuf 类型数据； 不过，部分 CRD 或者 API 扩展加入的资源不支持 Protobuf，可以使用 Accept 头部指定多种类型来允许回退。 Accept: application/vnd.kubernetes.protobuf, application/json ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:4:3","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"4.4 对象的删除 资源对象删除经过两个阶段： finalization 终止 资源的 metadata.deletionTimestamp 被设置，然后随机顺序执行 Finalizers。 delete 删除 当所有 Finalizers 执行结束后，资源才从 etcd 中被真正删除。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:4:4","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"5 访问控制 Kubernetes 对 API 有着三种访问控制方式： 认证Authentication 授权Authorization 准入控制Admission Control 这里仅仅会介绍三种方式的概念，更多的细节后面才深入去学习。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"5.1 Authentication Authentication 用于验证请求者是否是合法的，也就是检查身份。 每次收到请求时，APIServer 都会通过令排链进行认证，某一个认证成功即可： x509 处理程序 ：认证 HTTP 请求的证书是否有效； bearer token 处理程序 ：验证 token 是否有效； 基本认证处理程序 ：确保 HTTP 请求的基本认证凭证与本地的状态匹配； ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:5:1","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"5.2 Authorization Authorization 用于验证请求者是否由对应操作的权限，也就是检查权限。 APIServer 会组合一系列授权方法，只要有一个授权者批准了操作，那么请求就可以继续。 使用哪些授权方法，可以在 APIServer 启动时通过 --authorization_mode 参数设置 目前支持的几种授权方法： webhook ：允许集群外的 HTTP(s) 服务进行授权； ABAC ：执行静态文件中定义的策略； RBAC ：通过 RBAC 机制进行授权，这允许管理员进行动态的配置； Node ：确保 kubelet 只能访问自己节点上资源； ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:5:2","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"5.3 Admission Control Admission Control 是作为对授权模块的补充，用于拦截请求以确保其符合集群的期望与规则。 准入控制仅仅对于对象的创建、删除、更新、连接请求其作用，对于对象读取操作不起作用。 只有所有的准入控制器都通过，请求才能继续。如果任一准入控制器检查不通过，请求就会被立即拒绝。 准入控制是最后的关卡，一旦通过后，资源对象就会写入 etcd 中。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:5:3","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"6 扩展 API Kubernetes 提供了很完善的 API 扩展机制，使得你不需要修改 Kubernetes 的代码，可以动态的添加你自己的资源对象。 目前主要包含两种扩展 API 的方式： CRD ：复用 Kubernetes 的 API Server。用户只需要定义 CRD，并实现一个 CRD Controller，就能够通过 Kubernetes 管理自定义资源对象。 API Aggregate ：用户需要编写额外的 API Server，对资源进行更细粒度的控制。 CRD 相关见文章：CRD ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:6:0","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"6.1 API Aggregate 通过 API Aggregate 机制，能够将用户扩展的 API 注册到 kube-apiserver 上。 为了实现该机制，kube-apiserver 中引入了一个 API 聚合层，可以将 API 的访问请求转发到用户提供的 API Server 上，由用户 API Server 完成对请求的处理。 6.1.1 开启 API Aggregate 功能 为了让 kube-apiserver 开启 API Aggregate 功能，需要配置一些启动参数，具体见文档：启用-kubernetes-apiserver-标志。 6.1.2 注册 APIService 资源对象 用户需要创建一个 APIService 资源对象，来注册自己的 API Service。 下面示例表明，将 /apis/custom.metrics.k8s.io/v1beta1 的转发到名为 custom-metrics-server 的 Service。 apiVersion:apiregistration.k8s.io/v1kind:APIServicemetadata:name:v1beta1.custom.metrics.k8s.iospec:insecureSkipTLSVerify:truegroup:custom.metrics.k8s.iogroupPriorityMinimum:1000versionPriority:15service:name:custom-metrics-servernamespace:custom-metricsversion:v1beta1 spec.service ：指定将请求转发到的 Service spec.version ：API 的 version spec.group ：API 的 group 当然，APIService 仅仅负责将请求转发到 Service，而 Service 后端的 Pod 就需要用户自己部署了。 ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:6:1","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"参考 Blog：Kubernetes API 的版本控制，分组，对象，访问控制 官方文档：Kubernetes API Concepts Blog: what-happens-when-k8s ","date":"2021-06-12","objectID":"/posts/cloud_computing/k8s_learning/8-api/:7:0","tags":["k8s","云计算"],"title":"K8s 学习 - 8 - API","uri":"/posts/cloud_computing/k8s_learning/8-api/"},{"categories":["Kubernetes 学习"],"content":"CRD 概念与使用","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"1 概述 Kubernetes 不仅仅是一个编排框架，更是提供了极大的扩展性，可以看做一个资源框架。你可以基于 k8s 提供的种种功能，来满足你应用的需要。 其中 CRD 就是允许你来自定义 Resource，可以不修改 Kubernetes 代码来实现类似于 NetworkPolicy 这样的资源。 Note 可以理解 Kubernetes 提供了基础的框架，Pod Service Volume 等都是最基础的组件，而我们可以在这些组件之上进行再次抽象与组合，将其赋予一个特定的概念 CustomResource 以贵司 TiDB Operator 来举个例子，Operator 用于管理 TiDB 的集群，其核心组件包括 PD、TiDB、TiKV 程序。这些核心组件在最底层都是以 Pod 方式运行的，并且是多副本形式。而我们将所有的 Pod + Service 等组合在一起，构成了 TiDBCluster 这个 CustomResource。 所以，当你想部署 TiDB 集群时，只需要修改 TiDBCluster 这个资源的配置，然后提交到 Kubernetes 中。TiDB Operator（CRD Controller）就会根据 TiDBCluster 来操作，使得集群按照期望状态部署。 为什么能够做到？ Kubernetes 底层架构被尽可能的进行拆分与解耦，这样使得上层可以有很高的扩展性。 对于使用自定义资源，最基本要涉及到： CustomResourceDefinition ：一种 Resource 类型，为了让 k8s 能够认识到你的自定义资源，需要先提交 CRD； CustomResource Controller ：管理 CR 的程序，以 Pod 方式运行，并通过 k8s API 来管理 CR 以及执行操作； CustomResource ：你的自定义资源，就像 Pod Service 这种资源一样使用； 比喻 我习惯以编程的方式来看到这 3 个： CRD 是类定义，告诉编译器类型； CR 基于类创建的对象； CR Controller 是代码逻辑； ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"2 CRD CRD 是使用的 CustomResource 的基础，能够让 k8s 认识到你的自定义的资源。 基本的 CRD 定义如下（来自官网）： apiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:crontabs.stable.example.comspec:group:stable.example.comversions:- name:v1beta1# Each version can be enabled/disabled by Served flag.served:true# One and only one version must be marked as the storage version.storage:true- name:v1served:truestorage:falsescope:Namespacednames:# plural name to be used in the URL: /apis/\u003cgroup\u003e/\u003cversion\u003e/\u003cplural\u003eplural:crontabssingular:crontabkind:CronTabshortNames:- ctadditionalPrinterColumns:- name:Spectype:stringdescription:The cron spec defining the interval a CronJob is runjsonPath:.spec.cronSpec metadata.name ：命名，必须是 \u003cplural\u003e.\u003cgroup\u003e； spec group ：API Group，用以 REST API 注册（/apis/\u003cgroup\u003e/\u003cversion\u003e），基本以 URL 方式； versions ：版本号，用以 REST API 注册（/apis/\u003cgroup\u003e/\u003cversion\u003e）； scope ：资源的范围，Namespaced 或者 Cluster； names plural ：CRD 的复数命名； singular ：CRD 的单数命名； kind ：CR 的名字，定义 CR 时使用； shortNames ：简短的名称，用以 kubectl 时简写； additionalPrinterColumns ：kubectl 打印出的额外信息，从 CR 的 spec 中获取相关的信息； ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"2.1 URL 创建 CRD 后，会自动在 API Server 建立其对应的 Endpoint URL，使得可以通过 HTTP 方式来查询与操作该 CR。 其 URL 为 /apis/\u003cgroup\u003e/\u003cversion\u003e/namespaces/\u003cns\u003e/\u003cplural\u003e/…。 例如，示例中的 API endpoint 为 /apis/stable.example.com/v1/namespaces/*/crontabs/… ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"2.2 Validation spec.validation 用以用户在提交 CR 时，对其定义进行合法性检查。 该功能需要配置 kube-apiserver 的 –feature-gates=CustomResourceValidation=true。 我们对上面的示例对其增加两个检查： apiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:crontabs.stable.example.comspec:group:stable.example.comversion:v1scope:Namespacednames:plural:crontabssingular:crontabkind:CronTabshortNames:- ctvalidation:# openAPIV3Schema is the schema for validating custom objects.openAPIV3Schema:properties:spec:properties:cronSpec:type:stringpattern:'^(\\d+|\\*)(/\\d+)?(\\s+(\\d+|\\*)(/\\d+)?){4}$'replicas:type:integerminimum:1maximum:10 spec.cronSpec 必须是匹配正则表达式的字符串 spec.replicas 必须是从 1 到 10 的整数 更多的 openAPIV3Schema 检查语法见 OpenAPI v3 schemas。 ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"2.3 Defaulting 与 Nullable 在 OpenAPI v3 validation schema 下，允许你为某些项指定默认值。 apiVersion 必须是 apiextensions.k8s.io/v1 下面示例为 spec.cronSpec 指定了一个默认值，而 spec.image 默认为 null 值： apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:crontabs.stable.example.comspec:group:stable.example.comversions:- name:v1served:truestorage:trueschema:# openAPIV3Schema is the schema for validating custom objects.openAPIV3Schema:type:objectproperties:spec:type:objectproperties:cronSpec:type:stringpattern:'^(\\d+|\\*)(/\\d+)?(\\s+(\\d+|\\*)(/\\d+)?){4}$'default:\"5 0 * * *\"image:type:stringnullable:truereplicas:type:integerminimum:1maximum:10default:1scope:Namespacednames:plural:crontabssingular:crontabkind:CronTabshortNames:- ct ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"2.4 Subresources subresouces 支持设置 status 和 scale 两类： status ：API URL 启动 /status 路径，请求后得到对应资源对象的当前 status scale ：API URL 启用 /scale 路径，使得 Kubernetes 控制器（如 HPA）能够控制 CRD 对象。用户通过 kubectl scale 也可以对 CRD 对象进行缩扩容操作。 前提 当然，scale 的前提是你的 CustomResource 能够支持多副本的形式。 示例如下： # ...spec:group:stable.example.comversions:- name:v1subresources:status:{}scale:specReplicasPath:.spec.replicasstatusReplicasPath:.status.replicaslabelSelectorPath:.status.labelSelector scale specReplicasPath ：从 CR 获取期望副本数量的路径； statusReplicasPath ：从 CR 获取当前副本数量的路径； labelSelectorPath ：从 CR 获取 Label Selector 的路径； ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:2:4","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"2.5 Categories spec.names.categories 可以将资源分组，通过使用 kubectl get \u003ccategories\u003e 来获取一组资源。 apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:crontabs.stable.example.comspec:# … names:# categories 是定制资源所归属的分类资源列表categories:- all 上面例子将 CR 分类到 “all” 类别中，使得 kubectl get all 可以获取到该类别下的所有已经创建的资源。 ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:2:5","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"2.6 Finalizer Finalizer 会在删除 CR 时自动被调用，可以实现对 CR 的清理工作。 apiVersion:\"stable.example.com/v1\"kind:CronTabmetadata:finalizers:- stable.example.com/finalizer finalizers ：设置 Finalizer，仅仅是一个名称，用于 Controller 判断 当用户请求删除资源对象时，Kubernetes 会先将 metadata.deletionTimestamp 的值，然后将其标记为开始删除。设置该值后，不断的轮询检查是否所有的 finalizers 被移除。 对于旁路的 Controller，发现 metadata.deletionTimestamp 被设置，知道了资源对象正在删除流程，于是开始执行其负责的的清理工作。执行完成后，从 finalizers 中移除对应的一项。 Note Controler 可以读取 metadata.deletionGracePeriodSeconds 的值，用以得到期望的优雅删除时间是多久。 所有 finalizers 移除后，Kubernetes 才将其真正的资源对象删除。 ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:2:6","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"3 CR 在提交 CRD 后，我们就可以创建 CR 来提交了。当然，CR 中的 spec 相关的属性 k8s 是不会使用的，而是在 Controller 中解析并使用。 基于上面示例，创建一个 CronTab 资源： apiVersion:\"stable.example.com/v1\"kind:CronTabmetadata:name:my-new-cron-objectspec:cronSpec:\"* * * * /5\"image:my-awesome-cron-image 当我们提交资源后，其资源就保存在了 Kubernetes 中，可以查看与删除。但是，我们并没有注册 CronTab 的 Controller，所以没有基于该资源进行系统上的管理。 ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"4 CR Controller 上面看到，CR 与 CRD 都是静态的资源，而 CR Controller 就是基于已经创建的 CR 来进行实际的系统操作，使得整个系统是向期望的状态发展。 Controller 需要我们自行编写其逻辑，然后注册到 Kubernetes 中。 ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"4.1 Controller 模式 不论是内置的 Controller，还是你需要编写的自定义的 Controller，都需要遵循 控制器模式Controller Pattern 的方式实现。 我理解的控制器模式是：Controller 不断永久循环执行着 Controll Loop，而每一个 Loop 都是基于当前实际的资源状态（status），进行系统操作，向期望的资源状态（spec）靠拢。 所以整个逻辑是基于状态的，而不是基于事件的，这也就是 声明式 APIDeclarative API 与 命令式 APIImperative API 的区别。 ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:4:1","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"4.2 工作原理 Kubernetes 提供了资源状态的存储功能，而 Controller 需要实现的就是基于对资源的更变的控制逻辑。 一个 Controller 的工作原理，可以使用以下流程图表示： 整体上看，Controller 实现分为三个组件： Informer ：提供特定对象的存储，事件触发功能，即 List 与 Watch 功能； WorkQueue ：对象变更事件通知队列，通过队列来防止阻塞 Informer； Control Loop ：控制循环，基于事件来执行对应的操作； 4.2.1 Informer Informer 是 Kubernetes 提供的代码模块，针对于特定的 API 对象，包含几个职责： 同步 etcd 中对应 API 对象，将所有对象缓存到本地； 某个 API 对象的任何变更，都可以触发对象变更事件； 触发/检测到对象变更事件时，触发注册的 ResourceEventHandler 回调，即 AddFunc UpdateFunc DeleteFunc 回调； 针对于 etcd 的 List 与 Watch 机制，Informer 的同步机制也包含两种： 使用 Watch 的事件增量同步：当 APIServer 有对象的创建、删除或者更新，Informer.Reflector 模块会收到对应事件，解析后放入 Delta FIFO Queue； 使用 List 的定期周期全量同步：经过一定周期，Informer 会通过 List 来进行本地对象的强制更新。 该更新操作会强制触发“更新事件”，从而调用 UpdateFunc 回调 主动判断 ResourceVersion 所以 UpdateFunc 要通过对象的 ResourceVersion 来判断，是否对象有着真正的更新。 同时，Informer 中异步的不断读取 Delta FIFO Queue 中事件，并触发其注册的回调函数（AddFunc UpdateFunc DeleteFunc）。 ","date":"2021-06-10","objectID":"/posts/cloud_computing/k8s_learning/7-crd/:4:2","tags":["k8s","云计算"],"title":"K8s 学习 - 7 - CRD","uri":"/posts/cloud_computing/k8s_learning/7-crd/"},{"categories":["Kubernetes 学习"],"content":"StatefulSet 概念与使用","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"1 概述 Pod 在设计的理念上是无状态的，Pod 可以在任意时刻被销毁，可以在任意时刻被在别的节点创建相同的副本。 Deployment 与 RelicSets 就是基于这个理念设计的，它们仅仅保证 Pod 的副本数量，而不关心其他。 对于大多数程序，其都是需要 “状态” 的，也就是重启能够恢复之前的数据。这个状态可能包括： 存储 网络 启停顺序 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"1.1 “状态” 是什么 对于存储很好理解，大多数程序会将数据保存到磁盘或者云盘，而重启后重新读取数据，来恢复状态。所以，Pod 重启前后读取到的存储要是一样的。 对于网络，基于 Service 的存在，一般的后端服务可以不用关系其所处于的网络环境。但是对于分布式程序或者 p2p 程序，每个程序是需要知道其他程序的网络地址的。所以，Pod 重启前后的网络地址要是一样的。 还有一点是基于 Pod 之间的关系，有时候多个 Pod 实例之间的启停顺序也应该是一样的。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:1:1","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"1.1 如何保存 “状态” 对于存储，PVC 是与 Pod 生命周期不耦合的，所以我们让 Pod 重启前后都使用同一个 PVC，那么其数据就是相同的。所以问题变为了：建立 Pod 与 PVC 的映射关系。 对于网络，Pod 的 IP 不是恒定的，这个是 Pod 的基本概念，无能更改。所以问题变为了：要找到一个可以代表这个 Pod 的恒定网络地址。 对于启动顺序，要保证 Pod 之间的运行顺序，那么需要认得每一个 Pod，也就是 Pod 的命名必须是有规范与固定的。 所有的问题都由固定的 Pod 命名来解决，Pod 会按照 0-N 的方式来进行命名，而这样 Pod0 可以固定到 Pod0 PVC，固定到 Pod0 DNS 域名，启动顺序按照 Pod0 Pod1 … 来启动。 这就是 StatefulSet 做的事情，总结一下： 固定的持久化存储，通过 PVC； 固定的网络标识，通过 Headless Service 使得 \u003cpodname\u003e.\u003cservice\u003e.\u003cnamespace\u003e 与固定命名的 Pod 绑定； 按照编号进行有序的启动与停止； ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:1:2","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"2 StatefulSet 基础 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"2.1 定义 StatefuleSet 基本定义如下： apiVersion:apps/v1kind:StatefulSetmetadata:name:webspec:serviceName:\"nginx\"replicas:2selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:k8s.gcr.io/nginx-slim:0.8ports:- containerPort:80name:webvolumeMounts:- name:wwwmountPath:/usr/share/nginx/htmlvolumeClaimTemplates:- metadata:name:wwwspec:accessModes:[\"ReadWriteOnce\"]resources:requests:storage:1GistorageClassName:shared-ssd-storage serviceName ：使用的 Headless Service 命名； replicas ：副本数量； selector ：Pod selector，决定要管理哪些 Pod； template ：Pod template； volumeClaimTemplates ：创建的 PVC 模板； metadata.name ：创建的 PVC 基础命名，命名以 \u003cname\u003e-\u003cpodname\u003e 格式； spec ：与创建一个 PVC 的 spec 相同； ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"2.2 Status StatefulSet 会将升级与版本信息记录到 status 字段： status:collisionCount:0currentReplicas:3currentRevision:my-cluster-69545dcd7dobservedGeneration:1readyReplicas:3replicas:3updateRevision:my-cluster-69545dcd7dupdatedReplicas:3 currentReplicas ：当前版本的 Pod 数量； currentRevision ：当前版本的标识； updatedReplicas ：升级完成的 Pod 数量； updateRevision ：需要升级的版本标识； 而在对应的 Pod label 中，controller-revision-hash 会保存着其对应的 Revision，也就是创建其 Pod 的 StatefulSet 添加的： metadata:labels:controller-revision-hash:my-cluster-69545dcd7d 通过对比 Pod 的 controller-revision-hash label 与 StatefulSet 保存的 updateRevision 就可以判断这个 Pod 是否已经升级过了。因此，StatefulSet 可以不断对未升级的 Pod 执行升级操作。 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"2.3 使用 注意事项： PV 或者 StorageClass 需要预先创建，不会自动创建与删除； 虽然 PVC 是由 StatefulSet 自动创建的，但是不会进行自动删除，这是为了保留数据； Headless Service 需要预先创建，不会自动创建与删除； 删除 StatefulSet 不会使其清理所有的 Pod，应该通过将 .spec.replicas 设置为 0 来让其清理所有的 Pod； ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"2.4 Pod 管理策略 通过 spec.podManagementPolicy 可以设置管理的 Pod 的策略： OrderedReady ：默认策略，按照 Pod 的顺序依次创建或停止，前一个 Pod 完成后才会进行下一个 Pod 的操作； Parallel ：所有 Pod 创建或停止都是并行的，也就是说你不需要启停顺序的特性； ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:2:4","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"2.5 Pod 升级策略 通过 spec.updateStrategy 可以设置 Pod 的升级策略，也就是当你更新 spec.template 的镜像版本时，触发的操作。 # …spec:updateStrategy:rollingUpdate:partition:3type:RollingUpdate type 指定升级策略 updateStrategy ：指定 RollingUpdate 时控制滚动升级行为 目前支持两种策略： OnDelete （默认）：需要升级时，并不会自动删除旧版本 Pod，需要用户手动停止旧版本的 Pod，才会创建对应新版本 Pod。 RollingUpdate ：自动删除旧版本的 Pod，并创建新版本 Pod。管理方式依赖于 Pod 管理策略。 2.5.1 Partitions 设置 spec.updateStrategy.rollingUpdate.partition 可以对 Pod 进行分区管理。只有序号大于或等于的 Pod 才会进行滚动升级，其他 Pod 保持不变（即使被删除后重新创建）。 # 设置 partition 为 1 $ kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":3}}}}' statefulset \"web\" patched # 更新 StatefulSet $ kubectl patch statefulset web --type='json' -p='[{\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/image\",\"value\":\"gcr.io/google_containers/nginx-slim:0.7\"}]' statefulset \"web\" patched # 验证更新，可以看到从 web-1 web-2 都升级了，而 web-0 没有变化 kubectl get pods -n mytest NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 3m2s web-1 1/1 Running 0 30s web-2 1/1 Running 0 50s ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:2:5","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"总结 StatefulSet 在原理上设计的让人感觉很优雅，仅仅从固定的 Pod 出发来实现 “有状态” 这个复杂的事情。 不过也许还有好多场景 StatefulSet 也无法满足，可能需要更多的开发。 需要弄清楚以下几点： 为了需要有状态应用 如何为 Pod 实现有状态 StatefulSet 基本定义与使用 StatefulSet 的 Pod 管理策略 StatefulSet 的升级策略 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"参考 《Kubernetes 指南》 ","date":"2021-06-09","objectID":"/posts/cloud_computing/k8s_learning/6-statefulset/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 6 - StatefulSet","uri":"/posts/cloud_computing/k8s_learning/6-statefulset/"},{"categories":["Kubernetes 学习"],"content":"1 概念 在 API 访问控制 中提到，Kubernetes 支持的授权机制有多种，其中 RBAC 是最常用的授权方式。RBAC 基于角色访问控制，全称 Role-Base Access Control。 对于理解 RBAC，首先需要知道三个关键的概念： Role ：角色，代表一组对 Kubernetes API 对象操作的权限。 Subject ：被作用者，包括 User、Group、ServiceAccount 见 K8s 学习 - API Server 认证 RoleBinding ：定义 Role 与 Subject 的映射关系； 因此，我们会预先创建一些 Role，然后创建 Subject 时，定义 RoleBinding 来表明对 Subject 的权限控制。 为什么这么设计？ 通过 RoleBinding，实现了 Role Subject 之间的解耦。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"2 Role 与 ClusterRole ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"2.1 Role Role 就是一个 Kubernetes 资源对象，并且是一个 namespaced Resouce，因此其 Role 控制的权限范围也只能是所属的 namespace 下。 基本定义如下： kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:namespace:defaultname:pod-readerrules:- apiGroups:[\"\"]resources:[\"pods\"]verbs:[\"get\",\"watch\",\"list\"] metadata.namespace ：指定 Role 所属的 namespace rules.apiGroups ：表明针对哪些 API Group 生效，为空表示 core API Group； rules.resources ：表明针对哪些 Resource 生效； rules.verbs ：允许执行的操作； 示例中 rules 定义的规则就是：允许对 “mynamespace” 下的所有 Pod 对象，执行 GET、Watch、List 操作。 能够支持限制的操作为：“get”, “list”, “watch”, “create”, “update”, “patch”, “delete”，“deletecollection” 。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"2.2 ClusterRole ClusterRole 是一个 non-namespaced Resource，所以是针对所有 namespace 的资源生效，也可以实现控制 non-namespaced Resource 的权限。 基本定义如下： kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:# \"namespace\" omitted since ClusterRoles are not namespacedname:secret-readerrules:- apiGroups:[\"\"]resources:[\"secrets\"]verbs:[\"get\",\"watch\",\"list\"] rules.apiGroups ：表明针对哪些 API Group 生效，为空表示 core API Group； rules.resources ：表明针对哪些 Resource 生效； rules.verbs ：允许执行的操作； 示例中 rules 定义的规则是：允许对所有 namespace 下的 所有 Secret 对象，执行 GET、Watch、List 操作。 2.2.1 聚合 ClusterRole 多个 ClusterRole 可以聚合为一个新的 ClusterRole，用于简化 ClusterRole 的管理工作。 apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:monitoringaggregationRule:clusterRoleSelectors:- matchLabels:rbac.example.com/aggregate-to-monitoring:\"true\"rules:[]# rules 规则会自动被设置 aggregationRule 字段设置了匹配 ClusterRole 的规则，当一个 ClusterRole 被匹配到后，其 rules 自动会填充相关的规则。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"2.3 对非资源对象限制 某些 Kubernetes API 包含下次子资源，例如 Pod 的日志。 # ...rules:- apiGroups:[\"\"]resource:[\"pods\",\"pods/log\"]verbs:[\"get\",\"list\"] ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"2.4 对指定资源对象限制 通过 rules.resourceName 字段可以限制对指定资源的权限： # ...rules:- apiGroups:[\"\"]resource:[\"configmap\"]resourceName:[\"mu-configmap\"]verbs:[\"update\",\"get\"] 当然，因为是对指定的资源，因此无法限制 list、watch、create 或 deletecollections 操作。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:2:4","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"2.5 内置 Role 与 ClusterRole 默认下，Kubernetes 已经内置了许多个 Role 与 ClusterRole，它们都是以 system: 开头命名。 所有系统默认的 ClusterRole 和 RoleBinding 都会使用 label kubernetes.io/bootstrappiong=rbac-defaults 来标记。 每次集群启动时，APIServer 会更新默认的集群 Role 缺失的权限，也会更新默认 Role 绑定的 Subject。这样可以防止一些破坏性的更改，保证集群升级情况下相关内容能够及时更新。 使用内置 Role 与 ClusterRole 已经完全可以满足大部分的使用场景。 内置的 Role 的含义见 Default roles and role bindings。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:2:5","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"3 RoleBinding 与 ClusterRoleBinding ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"3.1 RoleBinding RoleBinding 用于绑定一个 Role 与多个 Subject，通过 RoleBinding 才能让某个 Subject 有 namespace 相关资源的访问权限。 # This role binding allows \"jane\" to read pods in the \"default\" namespace.kind:RoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:read-podsnamespace:defaultsubjects:- kind:Username:janeapiGroup:rbac.authorization.k8s.io- kind:ServiceAccountname:defaultroleRef:kind:Rolename:pod-readerapiGroup:rbac.authorization.k8s.io Tip RoleBinding 也可以引用 ClusterRole，将其权限限制在了 RoleBinding 所属的 namespace 下。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"3.2 ClusterRoleBinding ClusterRoleBinding 让 Subject 有着整个集群相关资源的访问权限。 3.2.1 ClusterRole 聚合 在 ClusterRole 中可以通过 aggregationRule 来与其他 ClusterRole，也就是可以自动组合多个 ClusterRole 为一个。 kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:monitoringaggregationRule:# 通过 label selector 方式自动组合clusterRoleSelectors:- matchLabels:rbac.example.com/aggregate-to-monitoring:\"true\"rules:[]# Rules 由组合起来的 ClusterRole 自动填充---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:monitoring-endpointslabels:rbac.example.com/aggregate-to-monitoring:\"true\"# These rules will be added to the \"monitoring\" role.rules:- apiGroups:[\"\"]resources:[\"services\",\"endpoints\",\"pods\"]verbs:[\"get\",\"list\",\"watch\"] ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:3:2","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"参考 官方文档：使用 RBAC 鉴权 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/5-rbac/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 5 - RBAC 授权机制","uri":"/posts/cloud_computing/k8s_learning/5-rbac/"},{"categories":["Kubernetes 学习"],"content":"PV PVC StorageClass 概念与使用","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"1 PV PVPersistent Volume 代表一个实际可用的后端存储（也可能不是后端，而是 Local PV）。大多数情况下，PV 是一个网络文件系统，或者分布式存储，或者云厂商的云盘。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"1.1 PV 的定义 PV 的定义仅仅描述了存储的属性： apiVersion:v1kind:PersistentVolumemetadata:name:pv0003spec:capacity:storage:5GivolumeMode:FilesystemaccessModes:- ReadWriteOncepersistentVolumeReclaimPolicy:RecyclestorageClassName:slowmountOptions:- hard- nfsvers=4.1nfs:path:/tmpserver:172.17.0.2 capacity：描述存储的大小； volumeMode：存储的使用类型，包括： Filesystem 文件系统：使用时会通过 Mount 挂载到 Pod 某个目录。也支持自动为 device 创建文件系统 Block 块设备：通过 device 方式提供给 Pod accessModes ：设备在实际使用时会先 mount 到宿主机上，accessMode 决定了是否允许多节点复用，并其读写权限，包括： ReadWriteOnce：只允许一个节点挂载使用，并提供读写； ReadOnlyMany：允许多个节点挂载使用，提供读权限； ReadWriteMany：允许多个节点挂载使用，提供读写权限； 不同的 PV 类型对其 accessMode 支持程度不同，具体见 AccessMode。 storageClassName ：指定其所属的 StorageClass。 也可以不指定 StorageClass，那么只有不指定 class 的 PVC 才可以绑定该 PV。 persistentVolumeReclaimPolicy ：ReclaimPolicy 指定了当其绑定的 PVC 删除时，该如何处理 PV。 目前的回收策略包括： Retain ：保留 PV，PV 会变为 Released 状态； Recycle ：自动删除 PV 数据； Delete ：同时删除后端磁盘（AWS EBS、GCE PD 等云盘也会被删除）； 目前，NFS 和 HostPath 类型的 PV 仅仅支持 Recycle，云厂商磁盘可以支持 Delete。 mountOptions ：当 PV 挂载到节点上时，添加的附加选项。 针对不同类型的 PV，这个选项是不同的，具体见 Mount Points。 nodeAffinity ：PV 可以设定节点亲和性，来限制只有特定的节点可以使用其 PV。 Note nodeAffinity 在使用 Local PV 时必定要使用。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:1:1","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"1.2 PV 生命周期 PV 生命周期包括 5 个阶段： Provisioning ：即 PV 的创建，可以直接创建 PV（静态方式），也可以使用 StorageClass 动态创建； Binding ：将 PV 分配给 PVC； Using ：Pod 通过 PVC 使用该 Volume，并可以通过准入控制 StorageObjectInUseProtection 阻止删除正在使用的 PVC； Releasing ：Pod 释放 Volume 并删除 PVC； Reclaiming ：回收 PV，可以保留 PV 以便下次使用，也可以直接从云存储中删除； Deleting ：删除 PV 并从云存储中删除后段存储； 对应 5 个阶段，一个 PV 可能处于以下状态： Available：PV 已经创建，并且没有 PVC 绑定； Bound：PVC 绑定了该 PV； Released：PVC 已经被删除，而该 PV 还没有被回收； Failed：PV 自动回收失败； ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:1:2","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"2 PVC PVCPersistent Volume Claim 描述一个 Pod 对 PV 的需求，是基于 namespace 下的。 为什么需要 PV PVC？ 感觉还是为了解耦，解耦使得 Pod 与存储形成了多对多的关系。 最直观的好处，一个 PV 可以通过多个 PVC 进行绑定，使得数据可以被多个 Pod 共享使用。也可以预定义多个 PV，而通过 PVC 可以直接由调度去选择合适的 PV 进行绑定。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"2.1 PVC 定义 2.1.1 Spec PVC 定义中主要是描述了对 PV 的需求，也就是告诉调度如何选择一个合适的 PV。 apiVersion:v1kind:PersistentVolumeClaimmetadata:name:myclaimspec:accessModes:- ReadWriteOncevolumeMode:FilesystemvolumeName:foo-pvresources:requests:storage:8GistorageClassName:slow # 指定绑定的 StorageClassselector:matchLabels:release:\"stable\"matchExpressions:- {key: environment, operator: In, values:[dev]} accessModes ：访问模式，与 PV 的访问模式要匹配 volumeMode ：使用模式，与 PV 的访问模式要匹配 volumeName ：指定绑定的 PV name，或者 Bind 后也会填充为对应的 PV name resources ：Pod 所需求的资源 selector：进一步来过滤选择的 PV，只有满足匹配条件的 PV 才可以被绑定 storageClassName ：指定所属的 StorageClass，只有相同 StorageClass 的 PV PVC 才可以绑定 如果没有指定 StorageClass，如果系统设置了 Default StorageClass 则使用，否则只能绑定没有设置 StorageClass 的 PV。 不指定 StorageClass 在没有指定 storageClassName 下，行为是有多种情况的，见 Class。 2.1.2 Status status:accessModes:- ReadWriteOncecapacity:storage:446Giphase:Bound accessModes ：访问模式 capacity ：可使用的容量 phase ：PVC 的阶段，包括： Pending ：没有绑定 PV； Bound ：已经绑定到 PV； Lost ：已经绑定到 PV，但是 PV 丢失了； ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"2.2 PVC 的使用 在 Pod 定义或者 Pod template 中，使用 pvc 类型 volume 来指定使用的 PVC。 apiVersion:v1kind:Podmetadata:name:mypodspec:containers:- name:myfrontendimage:nginxvolumeMounts:# 添加 volume 挂载- mountPath:\"/var/www/html\"name:mypdvolumes:- name:mypd # volume 类型为 PVCpersistentVolumeClaim:claimName:myclaim # 指定使用的 PVC ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"3 StorageClass PV 的创建方法有两种： 静态创建Static Provision，由管理员手动创建所有支持的 PV； 动态创建Dynamic Provision，定义 StorageClass，按 PVC 来创建合适的 PV； 所以明确，StorageClass 是根据 PVC，来创建/回收 PV 的。 而创建与回收的 Driver 就称为 Provisioner，不同类型的 PV 的创建与回收方式都是不同的，所以有着许多的 Provisioner 实现。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"3.1 StorageClass 定义 StorageClass 中大多数属性是其创建出 PV 的模板。 apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:standardprovisioner:kubernetes.io/aws-ebs # 指定 Provisionerparameters:type:gp2reclaimPolicy:RetainallowVolumeExpansion:truemountOptions:- debugvolumeBindingMode:Immediate provisioner ：指定 Provisioner 的实现，这也就决定了其创建 PV 的类型。 其内置支持的 Provisioner 见 provisioner。 当然也可以使用自定义的 Provisioner。 reclaimPolicy ：针对 PV 的回收策略，这也就决定了其创建 PV 的回收策略。默认为 Delete。 allowVolumeExpansion ：是否允许通过修改 PVC 对象的属性，来对使用的 PV 进行扩容。 大多数云存储都支持该选项，具体见 allow-volume-expansion。 mountOptions ：指定其创建的 PV 的 Mount Options。 volumeBindingMode ：决定了何时进行将 PV mount 到节点上。 Immediate 表明一旦创建 PVC 后，就需要绑定 PV。 WaitForFirstConsumer 将 PV 与 PVC 的绑定延后，等到 Pod 创建并使用 PVC 时，才会去绑定对应的 PV。 Local PV 必须使用 WaitForFirstConsumer 因为 Local PV 只有等到 Pod 创建并调度时，才能决定使用本地的 PV，所以必须使用 WaitForFirstConsumer。 allowedTopologies ：在使用云服务的存储时，其 PV 是有 “访问位置”（拓扑域） 的限制的，而就需要满足节点上的 Pod 仅仅使用特定位置的 PV。 所以在 WaitForFirstConsumer 模式下，通过 allowedTopologies 使得仅仅在特定的位置来创建 PV，从而节点的 Pod 可以正常的访问 PV。 parameters ：提供给 Provisioner 创建 PV 的参数，最多支持 512 个参数，长度不能超过 256KiB。 不同 Provisioner 支持不同参数，见 parameters。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"4 PV PVC StorageClass 之间的关系 下面一张图描述了 PV PVC StorageClass 之间的关系： ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"5 Local PV ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"5.1 手动创建使用 LocalPV 在自己测试环境下，往往没有一个云存储可以作为 PV 使用，而使用 Local PV，使得 Pod 使用的 PV 来自于本地的目录或者块设备。 先明确 Local PV 如何定义，即 PV 类型为 hostPath，表明实际存储设备来自于宿主机的一个目录。 同时，只有调度到该节点的 Pod 才能使用这个 PV，所以要设定 PV 的 spec.nodeAffinity，仅仅允许该节点使用该 PV： apiVersion:v1kind:PersistentVolumemetadata:name:example-pvspec:capacity:storage:5GivolumeMode:FilesystemaccessModes:- ReadWriteOncepersistentVolumeReclaimPolicy:DeletestorageClassName:local-storage # 指定 storageClassNamelocal:# local 类型表明使用 Local PVpath:/mnt/disks/vol1nodeAffinity:# 设定节点亲和性，使得只能本地节点使用 PVrequired:nodeSelectorTerms:- matchExpressions:- key:kubernetes.io/hostnameoperator:Invalues:- node-1 # 仅仅允许 node-1 使用该 PV 而为了让 PVC 与 PV 绑定推迟到 Pod 创建后才决定绑定的 PV，我们需要一个 StorageClass，即使其并不能够支持动态创建 PV： kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:local-storageprovisioner:kubernetes.io/no-provisioner # no-provisioner 不支持动态创建 PVvolumeBindingMode:WaitForFirstConsumer # 让 PVC 绑定推迟 而在使用时候，我们只需要一个普通的 PVC，指定好 spec.storageClassName 后，就可以使用 LocalPV 了。 kind:PersistentVolumeClaimapiVersion:v1metadata:name:example-local-claimspec:accessModes:- ReadWriteOnceresources:requests:storage:5GistorageClassName:local-storage # 指定 StorageClass，来绑定 PV ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:5:1","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"5.2 local pv static provisioner local pv static provisioner 将上面的步骤进行了简化，其启动了一个 DaemonSet，根据配置好的目录，自动生成对应目录的 Local PV。 所以带来的好处就是，我们不需要为一个个节点去创建对应的 PV 了，有 Pod 自动负责该事。 5.2.1 原理 local pv static provisioner 为创建一个 DamonSet，在每个节点创建 Daemon Pod。 每个节点上的 Daemon Pod 会根据其 ConfigMap 配置好的 discovery dir，在目下查找可以作为 PV 的子目录。 在 discovery dir 下挑选目录有两个条件： 目录必须是一个挂载点，也就是 mountinfo 中可以看到的； 为什么必须是挂载点？ 我猜测这是为了通过挂载点得到目录容量大小，从而填充 PV 容量相关属性。 满足其 ConfigMap 中指定的目录匹配方式 namePattern。 5.2.2 示例 而为了使用 LocalPV，我们也需要定义一个 StorageClass，让创建出的 LocalPV 能够属于正确的 StorageClass。 所以使用的步骤为： 每个节点上准备作为 LocalPV 的目录，该目录必须是一个挂载点，且位于 discovery dir 下； 创建 StorageClass，作为后续创建出的 Local PV 的 StorageClass； 定义并创建 ConfigMap，为了将相关的信息传递给 provisioner 的 DaemonSet Pod； 定义并创建 ServiceAccount，因为后续的 Daemon Pod 需要创建 Local PV 结构，所以需要相关的权限； 定义并创建 DaemonSet，根据 config map 信息，其 Pod 里程序会自动创建 Local PV，其定义就与上述的 PV 一致； 看一下其 StorageClass、ConfigMap 和 DaemonSet 定义： apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:fast-disks # name 要与下面 ConfigMap 相同provisioner:kubernetes.io/no-provisionervolumeBindingMode:WaitForFirstConsumerreclaimPolicy:Delete # 支持 Delete Retain---apiVersion:v1kind:ConfigMapmetadata:name:local-provisioner-config namespace:default data:storageClassMap:| fast-disks:# 上面创建的 StorageClass 名字hostDir:/mnt/fast-disks # 宿主机 discovery dirmountDir:/mnt/fast-disks # Daemon Pod 中查找的 discover dirblockCleanerCommand:- \"/scripts/shred.sh\"- \"2\"volumeMode:FilesystemfsType:ext4namePattern:\"*\"# 查找子目录的匹配方式---apiVersion:apps/v1kind:DaemonSetmetadata:name:local-volume-provisionernamespace:defaultlabels:app:local-volume-provisionerspec:selector:matchLabels:app:local-volume-provisioner template:metadata:labels:app:local-volume-provisionerspec:serviceAccountName:local-storage-admincontainers:- image:\"k8s.gcr.io/sig-storage/local-volume-provisioner:v2.4.0\"imagePullPolicy:\"Always\"name:provisioner securityContext:privileged:trueenv:- name:MY_NODE_NAMEvalueFrom:fieldRef:fieldPath:spec.nodeNamevolumeMounts:- mountPath:/etc/provisioner/config name:provisioner-configreadOnly:true- mountPath:/mnt/fast-disks name:fast-disksmountPropagation:\"HostToContainer\"volumes:- name:provisioner-config # 能够访问到 ConfigMapconfigMap:name:local-provisioner-config - name:fast-disks # 能够查看到 discovery dirhostPath:path:/mnt/fast-disks 等到自动创建出 Local PV 后，后续的流程其实就是使用一个 Local PV 的流程了。 ","date":"2021-06-08","objectID":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/:5:2","tags":["k8s","云计算"],"title":"K8s 学习 - 4 - PV PVC StorageClass","uri":"/posts/cloud_computing/k8s_learning/4-pv-pvc-storageclass/"},{"categories":["Kubernetes 学习"],"content":"1 Pod 的 DNS ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/3-dns/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 3 - DNS","uri":"/posts/cloud_computing/k8s_learning/3-dns/"},{"categories":["Kubernetes 学习"],"content":"1.1 Pod 的 DNS 域名 对每个 Pod 来说，CoreDNS 会为其设置一个 \u003cpod_ip\u003e.\u003cnamespace\u003e.pod.\u003cclust-domain\u003e 格式的 DNS 域名。 # 访问 POD IP DNS $ nslookup 192-168-166-168.tidb-cluster-dev.pod.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: 192-168-166-168.tidb-cluster-dev.pod.cluster.local Address 1: 192.168.166.168 my-tidb-cluster-dev-tidb-0.my-tidb-cluster-dev-tidb-peer.tidb-cluster-dev.svc.cluster.local 对于 Deployment 或 DaemonSet 类型创建的 Pod，CoreDNS 会为其管理的每个 Pod 设置一个 \u003cpod_ip\u003e.\u003cdepolyment/daemonset_name\u003e.svc.\u003ccluster_domain\u003e 格式的 DNS 域名。 ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/3-dns/:1:1","tags":["k8s","云计算"],"title":"K8s 学习 - 3 - DNS","uri":"/posts/cloud_computing/k8s_learning/3-dns/"},{"categories":["Kubernetes 学习"],"content":"1.2 自定义 hostname 和 subdomain 默认情况下，Pod 内容器的 hostname 被设置为 Pod 的名称。因此，使用副本控制器时 Pod 名称会包含随机串，因此 hostname 无法固定。 可以通过 spec.hostname 字段定义容器环境的 hostname，通过 spec.subdomain 字段定义容器环境的子域名。 spec:hostname:webapp-1subdomain:mysubdomain Pod 创建成功后，Kubernetes 为其设置 DNS 域名为 \u003chostname\u003e.\u003csubdomain\u003e.\u003cnamespace\u003e.svc.\u003ccluster_domain\u003e。这时通过部署一个 Headless Service 就可以在 DNS 服务器中自动创建对应 DNS 记录。这样就可以通过该 DNS 域名来访问 Pod。 实际上，StatefulSet 就是通过这种方式来使用 Headless Service 的。查看一个 StatefulSet 管理的 Pod： $ kubectl get po my-tidb-cluster-dev-pd-0 -o yaml# ...spec:hostname:my-tidb-cluster-dev-pd-0# 固定 pod 的 hostnamesubdomain:my-tidb-cluster-dev-pd-peer # 在 StatefulSet 中定义的 Headless Service name ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/3-dns/:1:2","tags":["k8s","云计算"],"title":"K8s 学习 - 3 - DNS","uri":"/posts/cloud_computing/k8s_learning/3-dns/"},{"categories":["Kubernetes 学习"],"content":"1.3 自定义 DNS 配置 通过 Pod 定义中的 spec.dnsPolicy 可以定义使用的 DNS 策略。 目前包含以下的 DNS 策略： Default ：从 Node 继承 DNS ClusterFirst ：与配置的集群 domain 后缀不匹配的 DNS 查询，都会转发到从 Node 继承的上游服务器 ClusterFirstWithHostNet ：如果 Pod 使用 hostNetwork 运行，DNS 应该使用 host None ：忽略 Kubernetes DNS 设置，由用户通过 spec.dnsConfig 字段进行配置 通过 spec.dnsConfig 字段进行更加细节的 DNS 相关配置。 spec:dnsPolicy:\"None\"dnsConfig:nameservers:- 8.8.8.8searches:- ns1.svc.cluster-domain.example- my.dns.search.suffixoptions:- name:ndotsvalue:\"2\"- name:edns0 Pod 创建后，容器内的 /etc/resolv.conf 文件会被设置： nameserver 8.8.8.8 search ns1.svc.cluster-domain.example my.dns.search.suffix option natods:2 eth0 ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/3-dns/:1:3","tags":["k8s","云计算"],"title":"K8s 学习 - 3 - DNS","uri":"/posts/cloud_computing/k8s_learning/3-dns/"},{"categories":["Kubernetes 学习"],"content":"2 Service 中的 DNS Service DNS 相关已经在 Service 一文中说明，这里仅仅简单提及一下。 每个 Service 会自动对应一个 DNS 域名，命名方式为 \u003cservice\u003e.\u003cnamespace\u003e.svc.\u003ccluster_domain\u003e，cluster_domain 默认为 cluster.local。 通过改变 namespace 可以访问不同 namespace 下的 Service ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/3-dns/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 3 - DNS","uri":"/posts/cloud_computing/k8s_learning/3-dns/"},{"categories":["Kubernetes 学习"],"content":"3 Node 本地 DNS 缓存 集群中的 DNS 服务都是通过 “kube-dns” 的 Service 提供的，所有容器都可以通过其 ClusterIP 地址去进行 DNS 域名解析。 为了缓解 DNS 服务的压力，Kubernetes 引入了 Node 本地 DNS 缓存，使得 DNS 域名解析可以在 Node 本地缓存，不用每次跨主机去 CoreDNS 进行解析。 使用 Node 本地 DNS 缓存后，Pod 进行 DNS 解析的流程如下： Node 本地 DNS 缓存的功能是通过部署一个 DaemonSet 实现的，其 Pod 运行的 k8s.gcr.io/k8s-dns-node-cache 镜像实现了 DNS 缓存功能。 具体部署方式见文档：在 Kubernetes 集群中使用 NodeLocal DNSCache ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/3-dns/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 3 - DNS","uri":"/posts/cloud_computing/k8s_learning/3-dns/"},{"categories":["Kubernetes 学习"],"content":"4 CoreDNS 从 1.11 版本开始，Kubernetes 集群的 DNS 服务由 CoreDNS 提供，其用 Go 实现的一个高性能、插件式、易扩展的 DNS 服务器。 ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/3-dns/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 3 - DNS","uri":"/posts/cloud_computing/k8s_learning/3-dns/"},{"categories":["Kubernetes 学习"],"content":"4.1 配置 CoreDNS CoreDNS 的主要功能是通过插件系统实现的，CoreDNS 实现了一种链式插件结构，将 DNS 逻辑抽象为一个个插件，能够组合灵活配置。 常见的插件如下： loadbalance ：提供基于 DNS 负载均衡功能 loop ：检查 DNS 解析出现的循环问题 cache ：提供前端缓存功能 health ：对 Endpoint 进行健康检查 kubernetes ：从 Kubernetes 中读取 zone 数据 etcd ：从 etcd 中读取 zone 记录，可用于自定义域名记录 file ：从 RFC11035 格式文件读取 zone 数据 hosts ：使用 /etc/hosts 文件或者其他文件读取 zone 数据，可用于自定义域名记录 auto ：从磁盘中自动加载区域文件 reload ：定时重新加载 Corefile 配置文件内容 forward ：转发域名查询到上游 DNS 服务器上 prometheus ：为 Prometheus 系统提供采集性能指标数据 URL pprof ：在 URL 路径 /debug/pprof 提供性能数据 log ：对 DNS 查询进行日志记录 errors ：对错误信息进行日志记录 默认下，CoreDNS 的 Pod 会使用 coredns ConfigMap 提供对 CoreDNS 的配置。因此，你可以通过配置此 ConfigMap 进行 CoreDNS 的配置。 $ k get configmaps coredns -n kube-system -o yaml # ... kind: ConfigMap data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/3-dns/:4:1","tags":["k8s","云计算"],"title":"K8s 学习 - 3 - DNS","uri":"/posts/cloud_computing/k8s_learning/3-dns/"},{"categories":["Kubernetes 学习"],"content":"参考 《Kubernetes 权威指南》 ","date":"2021-06-07","objectID":"/posts/cloud_computing/k8s_learning/3-dns/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - 3 - DNS","uri":"/posts/cloud_computing/k8s_learning/3-dns/"},{"categories":["Kubernetes 学习"],"content":"1 概述 首先我们需要明确 Service 出现的背景：因为 Pod 设计上是无状态的，可以说没有固定的 IP，所以在其他组件访问某一个网络服务时，需要一个 “固定” 的地址。 这个固定的地址，就需要由 Service 来提供。同样，因为访问的地址固定了，Service 也可以提供负载均衡这样的功能。 最简单的例子，有一个 Web 后端服务，有着 3 个 Pod 运行。而前端想访问该后端服务，想要的只是一个固定的域名或者地址，而不关系后端服务的 Pod 会怎样被调度。 为什么这样设计 可以感觉，这就是分层。上下层之间只通过固定的地址通信，而不关心层内部是怎样运行的。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"2 Service 基础 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"2.1 定义 Service 基本的定义如下： apiVersion:v1kind:Servicemetadata:name:stringnamespace:stringlabels:- name:stringannotations:- name:stringspec:selector:[]type:stringclusterIP:stringsessionAffinity:stringports:- name:stringprotocol:stringport:inttargetPort:intnodePort:intstatus:loadBalancer:ingress:ip:stringhostname:stringtopologyKeys:- \"key\"externalName:string spec.selector ：用于 Service 选择被代理的 Pod； spec.type ： Service 类型，见 Service 的类型； spec.clusterIP ：固定的地址，为空那么随机提供； spec.sessionAffinity ： 设置负载均衡策略； spec.ports ：提供需要代理的协议，源端口，目的端口，宿主机端口（NodePort 类型）； spec.status ：使用 LoadBalancer 类型下，提供相关参数； spec.topologyKeys ：控制流量转发的拓扑控制，优先将流量转发到相同 key 的 Node 上的 Pod； spec.externalName ：ExternalName 类型 service 代理的集群外的服务域名； ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"2.2 负载均衡策略 k8s 默认提供两种负载均衡策略： RoundRobin ：轮询模式，将请求轮询到后端各个 Pod。默认模式 SessionAffinity ：基于客户端 IP 地址进行会话保持的模式。 即第一次将某个客户端发起请求到后端某个 Pod，之后相同客户端发起请求都会被转发到对应 Pod。 将 spec.sessionAffinity 指定为 “ClientIP”，就表明了开启 SessionAffinity 策略。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"2.3 支持的网络协议 目前 Service 支持如下网络协议： TCP: 默认网络协议，可用于所有类型 Service UDP: 可用于大多数类型 Service。LoadBalancer 类型取决于云服务是否支持 HTTP: 取决于云服务是否支持 PROXY: 取决于云服务是否支持 SCTP 从 1.17 版本开始，可以为 Service 和 Endpoint 资源对象设置 spec.ports[].AppProtocol 字段，用于表示后端服务在某端口停的应用层协议类型。 # ...spec:ports:- port:8080targetPort:8080AppProtocol:HTTP ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:2:3","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"3 服务发现 Kubernetes 提供了两种机制供客户端以固定的范式获取后端 Service 的访问地址：环境变量和 DNS。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"3.1 环境变量方式 如果 Pod 在 Service 之后创建，那么集群中同 namespace Service 地址信息会通过 ENV 传递给容器（不包括 Headless Service）。 相关的环境变量包括： \u003cservice\u003e_PORT=\u003cproto\u003e://\u003cclusterip\u003e:\u003cport\u003e \u003cservice\u003e_PORT_\u003cport\u003e_\u003cproto\u003e=\u003cproto\u003e://\u003cclusterip\u003e:\u003cport\u003e \u003cservice\u003e_PORT_\u003cport\u003e_\u003cproto\u003e_ADDR=\u003cclusterip\u003e \u003cservice\u003e_PORT_\u003cport\u003e_\u003cproto\u003e_PORT=\u003cport\u003e \u003cservice\u003e_PORT_\u003cport\u003e_\u003cproto\u003e_PROTO=\u003cproto\u003e \u003cservice\u003e_SERVICE_HOST=\u003cclusterip\u003e \u003cservice\u003e_SERVICE_PORT=\u003cport\u003e \u003cservice\u003eSERVICE_PORT\u003cNAME\u003e=\u003cport\u003e 默认相关的环境变量使用的都是 service.spec.port[0] 信息。如果使用的多 port 代理，需要使用 xxx_PORT_xxx 相关环境变量。 环境变量默认还会提供 kubernetes 的 service 地址，用于 Pod 来访问 APIServer。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"3.2 DNS 方式 所有 Service 都会自动对应一个 DNS 域名，其命令方式为 \u003cservice\u003e.\u003cnamespace\u003e.svc.cluster.\u003ccluster_domain\u003e。由 CoreDNS 作为集群的默认 DNS 服务器，提供域名解析服务。 如果 Service 定义中设置了 spec.ports[].name，那么该端口号也会有一个域名：`\u003cport_name\u003e....svc.\u003ccluster_domain\u003e。 # 提供了 _http._tcp.\u003cname\u003e.\u003cnamspace\u003e.svc.local 的 DNS 域名spec:ports:- protocol:TCPport:8080targetPort:8080name:http 当执行 nslookup \u003cservice\u003e 时，自动在当前的 namespace 下访问。通过 nslookup \u003cservice\u003e.\u003cnamespace\u003e 可以跨 namespace 进行 DNS 解析。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:3:2","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"4 Service 类型 Service 核心的功能是：代理。代理涉及到两个点：访问代理的前端，被代理的后端。 针对这个前后端的不同，Service 分为了 4 种类型： ClusterIP（默认）：代理一组后端 Pod，提供一个固定的内部地址 ClusterIP + Port，也称为 VIP； NodePort ：NodePort 在 ClusterIP 基础上，会将 ClusterIP + Port 映射到 Node 上的某个端口，使得集群外部可以访问内部服务； LoadBalancer ：LoadBalancer 在 NodePort 上，将一部分 Node 的端口映射到一个 IP 上，使得外部网络可以访问 Node； ExternalName ：代理集群外部服务的 DNS 域名，而不是通过 selector 来选择集群内部后端 Pod； 可以看到，ClusterIP NodePort LoadBalancer 是针对访问代理的前端做了区分，而 ExternalName 是在被代理后端的不同。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"4.1 ClusterIP ClusterIP 是最基本的 Service，通过一个 VIP + Port 来在集群内部代理了一组 Pod 的 Port。始终要记得，VIP 是在集群内部才能使用。 apiVersion:v1kind:Servicemetadata:name:service-pythonspec:ports:- port:3000# 入口端口protocol:TCP # 代理协议targetPort:443# 目标端口selector:run:pod-python # 仅代理匹配的 Podtype:ClusterIP 上述定义创建了一个随机选择 IP，代理 TCP 3000 -\u003e 443 端口的 Service。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:4:1","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"4.2 NodePort NodePort 是对 ClusterIP 的增强，增加一个宿主机上端口到代理源端口的转发，使得集群外部也可以访问集群内部的服务。 port-forward 当你执行 kubectl port-forward svc/xxx 命令时，其实也是增加了一个宿主端口到 Service 源端口转发，和 NodePort 一样。 通过 service.spec.ports[].nodePort 指定映射的宿主机端口： apiVersion:v1kind:Servicemetadata:name:service-pythonspec:ports:- port:3000protocol:TCPtargetPort:443nodePort:30080selector:run:pod-pythontype:NodePort 上述定义在 ClusterIP 基础上，增加了宿主机 30080 的端口转发。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:4:2","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"4.3 LoadBalancer 在云厂商环境下，Node 都是在云的托管集群中的，所以外网访问 k8s 集群内的路径为：“外网 -\u003e 云集群 -\u003e k8s 集群”。而 NodePort 仅仅解决了 “云集群 -\u003e k8s 集群” 这个问题。 因此，LoadBalancer Service 在 NodePort 基础上，提供了云厂商需要的负载均衡信息，而云厂商根据该信息设置好 “外网 -\u003e 云集群” 的转发路径。 apiVersion:v1kind:Servicemetadata:name:service-pythonspec:ports:- port:3000protocol:TCPtargetPort:443nodePort:30080selector:run:pod-pythontype:LoadBalancerexternalTrafficPolicy:Local spec.externalTrafficPolicy spec 中定义仅仅是约定的规范，不同厂商所需要的更加细节的 LoadBalancer 的参数，大多数是通过 service.metadata.annotations 来提供： metadata:name:my-serviceannotations:service.beta.kubernetes.io/aws-load-balancer-access-log-enabled:\"true\"# Specifies whether access logs are enabled for the load balancerservice.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval:\"60\"# The interval for publishing the access logs. You can specify an interval of either 5 or 60 (minutes).service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name:\"my-bucket\"# The name of the Amazon S3 bucket where the access logs are storedservice.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix:\"my-bucket-prefix/prod\"# The logical hierarchy you created for your Amazon S3 bucket, for example `my-bucket-prefix/prod` 对于 LoadBalancer，更多的信息见官方文档：LoadBalancer。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:4:3","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"4.4 ExternalName 前面三种 Service 代理的后端都是集群内部的 Pod，而 ExternalName 不再是代理 Pod，而是将请求域名重定向另一个域名。 因此，ExternalName Service 不再提供代理的功能，而是提供了域名重定向的功能。 kind:ServiceapiVersion:v1metadata:name:service-pythonspec:ports:- port:3000protocol:TCPtargetPort:443type:ExternalNameexternalName:remote.server.url.com 通过 serivce.spec.externalName 指定被代理的域名，而集群内部的 Pod 就可以通过该 Service 访问外部服务了。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:4:4","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"5 Endpoint 当创建一个 Service 后，会根据 service.spec.selector 自动来匹配作为后端的 Pod。实际上，会对应一个 Endpoints 对象来代表其匹配到的代理目标，而其每一个被代理的 Pod 称之为 Endpoint。 $ kubectl get endpoints -A NAMESPACE NAME ENDPOINTS AGE default kubernetes 172.16.4.169:6443 3d1h kube-system kube-dns 10.36.0.4:53,10.36.0.5:53,10.36.0.4:53 + 3 more... 3d1h 当然，你也可以不提供 service.spec.selector，也不使用 ExternalName Service，而指定创建一个 Endpoints 对象。这样可以实现，Service 只代理指定的地址： apiVersion:v1kind:Servicemetadata:name:plat-devspec:ports:- port:3306protocol:TCPtargetPort:3306---apiVersion:v1kind:Endpointsmetadata:name:plat-devsubsets:- addresses:- ip:\"10.5.10.109\"ports:- port:3306 同名的 Endpoint 与 Service 自动被认为是相绑定的。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"6 Headless Service Service 会自动发现一组 Pod，并提供代理服务与负载均衡。不过有时候，Pod 中程序并不想使用 Service 的代理功能，而是仅仅想让 Service 作为一个服务发现的作用，例如，peer2peer 程序想要知道有哪些对端的程序。 通过 Service 的定义看，这种情况也就是不需要 service.spec.clusterIP，但是需要 service.spec.selector。这种特殊的 Service 被称为 Headless Service。 apiVersion:v1kind:Servicemetadata:name:mysql-balance-svcnamespace:mysql-spacespec:ports:- port:3308protocol:TCPtargetPort:3306clusterIP:None # clusterIP 指定为 None 表明不需要selector:name:mysql-balance-podpublishNotReadyAddresses:false spec.clusterIP: None 表明其为 Headless Service sepc.publishNotReadyAddresses ：为 true 时，即使 Pod 还不是 Ready 状态，也会提供 DNS 记录 service.spec.selector 让 Service 匹配到后端 Pod，并创建对应的 Endpoints。 当我们直接使用 Service DNS 进行解析时，会得到 DNS 系统返回的全部 Endpoint 地址。 $ nsloopup mysql-balance-svc.mysql-space.svc.cluster.local Server: 169.169.0.100 Address: 169.169.0.100#53 Name :mysql-balance-svc.mysql-space.svc.cluster.local Address: 10.0.95.13 Name :mysql-balance-svc.mysql-space.svc.cluster.local Address: 10.0.95.12 Name :mysql-balance-svc.mysql-space.svc.cluster.local 通过 Headless Service，然后设置 Pod 的 hostname 与 subdomain，就可以实现通过固定 DNS 记录访问某个 Pod。也就是 StatefulSet 使用 Headless Service 的方式。 这里提及一下，StatefulSet Pod 对应的 DNS 记录为：\u003cpodname\u003e.\u003cservice\u003e.\u003cnamespace\u003e.svc.cluster.local。 DNS 记录不是由 Headless Service 提供的 要注意，StatefulSet Pod 的 DNS 记录不是由 Headless Service 提供的。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:6:0","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"7 EndpointSlice 与 Service Topology 由前面知道，Service 后端是一组 Endpoint，随着集群规模的扩大，Endpoint 数量不断的增长，使得 kube-proxy 需要维护非常多的负载分发规则。 通过 EndpointSlice 与 Service Topology 配合，可以让 kube-proxy 仅仅转发部分的节点，实现对 Endpoint 的分片管理。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:7:0","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"7.1 EndpointSlice 1.16 版本引入了 EndpointSlice 机制，包括 EndpointSlice 资源对象和 EndpointSlice Controller。 EndpointSlice 就是代表着一组 Endpoint，kube-proxy 可以使用 EndpointSlice 中的 Endpoint 进行路由转发。 kube-proxy 开启 EndpointSlice kube-proxy 默认仍然使用 Endpoint 对象，通过设置其启动参数 –feature-gates=“EndpointSlice=true” 来让其使用 EndpointSlice 对象。 默认情况下，创建一个 Service 后，就会存在对应的 EndpointSlice 对象，包含匹配到的 Endpoint 对象。 $ kubectl get endpointslices.discovery.k8s.io NAME ADDRESSTYPE PORTS ENDPOINTS AGE my-tidb-cluster-dev-discovery-sm8ns IPv4 10262,10261 192.168.54.139 2d3h my-tidb-cluster-dev-pd-dsktv IPv4 2379 192.168.135.18,192.168.166.186,192.168.54.140 2d3h my-tidb-cluster-dev-pd-peer-n6wbg IPv4 2380 192.168.135.18,192.168.166.186,192.168.54.140 2d3h ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:7:1","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"7.2 Service Topology Service Topology 可以根据业务需求对 Node 进行分组，设置有意义的指标值来标识 Node 是 “近” 或者 “远”。对于公有云环境来说，通常会进行 Zone 或 Region 的划分。 通过 Service Topology 就实现了对 Endpoint 进行分组，也就是创建多个 EndpointSlice，进行分片管理。 开启 Service Topology 通过设置 kube-apiserver 和 kube-proxy 启动参数 –feature-gates=“ServiceTopology=true,EndpointSlice=true 开启。 通过 Service 定义上的 spec.topologyKeys 字段来进行 Service 流量控制。转发流量时，会去按照 topologyKeys 字段顺序匹配 Node 的 label，同样的 key 对应的 value 算作匹配成功，那么才能将流量转发到该 Node 上。 spec:topologyKeys:- \"kubernetes.io/hostname\"# 匹配 Node 与要转发的 Node，该 key 的 label 相同才可以转发流量---spec:topologyKeys:- \"topology.kubernetes.io/zone\"# 优先转发到同 zone- \"topology.kubernetes.io/region\"# 其次转发到同 region- \"*\"# 最后随意转发 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:7:2","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"8 Ingress Service 提供了基于 4 层的代理，也就是基于 IP + Port 的代理。而 Ingress 出现就是为了支持 7 层的代理，典型的就是支持 HTTP/HTTPS 协议的代理。 Ingress 类似于 nginx 的配置，提供应用层的路由，将流量路由给基于传输层的 Service，而由 Service 将流量路由给底层的 Pod。 不过 Ingress 类似于 Service，仅仅是一个规则的定义，其代理的实现依赖于 Ingress Controller。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:8:0","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"8.1 Ingress Controller Ingress Controller 基于定义好的 Ingress 来实现实际的路由，可以理解为就是实际上的 nginx。 Ingress Controller 是不包含在 controller manager 默认启动的 controller 的，需要手动进行 controller。 当然，目前有着许多种的 Ingress Controller 的实现，具体见 Ingress Controller。 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:8:1","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"8.2 Ingress 定义 Ingress 的配置和配置 nginx 类似，基于 HTTP path 路径进行路由的配置。 看一个示例定义： apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-resource-backendspec:ingressClassName:istiotls:- hosts:[]secretName:secretdefaultBackend:service:name:server-nameport:name:port-namenumber:80resource:apiGroup:k8s.example.comkind:StorageBucketname:static-assetsrules:host:\"\"- http:paths:- path:/iconspathType:ImplementationSpecificbackend:resource:apiGroup:k8s.example.comkind:StorageBucketname:icon-assets spec.ingressClassName - 指定使用的 Ingress Controller 的名字，为保持兼容 annotation kubernetes.io/ingress.class 依旧可以使用 spec.tls - 证书对应的 secret 与允许的 hosts spec.defaultBackend - 用于配置默认的后端，当 rules 中所有都不满足时，就会使用 default 路由 service - 转发到一个 Kubernetes Service name - service 的名字 port - 转发给 service 的 port，可以是名字或者端口号 resource - 转发给 Ingress 对象同 Namespace 下的一个 Resource（与 service 选项互斥） spec.rules - 定义路由策略 host - 匹配请求的 host，可以是精确匹配或者通配符匹配 http - http 层的多个匹配路径 ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:8:2","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"参考 《Kubernetes in Action》 《Kubernetes 权威指南》 Blog: 详解 k8s 4 种类型 Service ","date":"2021-06-06","objectID":"/posts/cloud_computing/k8s_learning/2-service/:9:0","tags":["k8s","云计算"],"title":"K8s 学习 - 2 - Service","uri":"/posts/cloud_computing/k8s_learning/2-service/"},{"categories":["Kubernetes 学习"],"content":"Pod 基本概念","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"1 Pod 的进程模型 Pod 是一组容器的集合，而为什么实现上需要多个容器？据传，因为开发 Borg 项目的工程师发现，部署的应用往往都是一个进程组，而不是一个进程。换个角度说，部署的业务往往有着多个不同的进程，并且往往需要部署到同一个机器上。 容器是 “单进程模型”，在云上代表的是一个进程。而 Pod 是 “进程组模型”，在云上代表着就是进程组。 “单进程模型” “单进程模型” 不代表容器只有一个进程，而是指容器无法管理多进程的能力。 你可以将其理解为，Pod 实现了 Supervisor/goreman 这样的功能，将多个进程进行分组管理，这对于业务应用的部署是非常有用的。 所以 Kubernetes 是一个操作系统，管理着 Pod “进程组”。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:1:0","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"1.1 Pause 容器 在创建 Pod 时，会先创建一个 pause 容器，也称为 根容器 或者 infra 容器。 pause 容器使用 k8s.gcr.io/pause 镜像，永远处于暂停，占用着很少的资源。 为什么需要 pause 容器？ 因为容器共享资源的特殊性。容器在需要共享 namespace 时，需要先创建一个 namespace，然后让其他的容器加入这个 namespace。 这带来的一个问题就是：如果容器 A 先启动，容器 B 加入。那么容器 A 异常重启后，是容器 A 加入容器 B，还是重新启动容器 B 再次加入容器 A。 很难选择，实现起来也很复杂，因为两个容器有着前后顺序的依赖性，而不是“平等的”。 Kubernetes 使用一个占用极少资源，非业务容器的 pause 容器首先启动。然后在其 pause 容器配置好 namespace。接着其他的业务容器加入到 pause 容器的 namespace。 因为 pause 容器占用极少资源，也没有任何逻辑，所以其他业务容器重启后，只需要加入 pause 容器的 namespace 即可，不需要担心 namespace 发生变化。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:1:1","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"1.2 容器共享的资源 Pod 内容器默认共享 network uts ipc namespace，所以容器间的网络是相同的。 Pod 可以配置开启共享 PID namespace，使得容器之间可以看到对方的进程。 共享 pid namespace 设置 Pod 定义的 spec.shareProcessNamespace 为 true 来共享 pid namespace。 Pod 通过 Volume 的抽象概念来进行共享存储，将其块设备或者目录提供到每个容器内的不同路径。也就是说，每个容器看到的目录路径或者块设备是独立的，但是都对应同一个宿主机的目录或块设备。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:1:2","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"2 Dynamic Pod 与 Static Pod ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:2:0","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"2.1 Dynamic Pod 大多数情况，我们会通过 ReplicaSet 或者 Deployment 这样的副本控制器来创建 Pod，甚至通过编写 Pod 的定义来手动部署 Pod。这种情况下，Pod 是由 Kubernetes 控制面管理的，可以通过 APIServer 启停它。 这样的 Pod 被称为 动态 PodDynamic Pod。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:2:1","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"2.2 Static Pod 另一类是特殊的 Pod，被称为 静态 PodStatic Pod。 静态 Pod 是由本地 kubelet 创建的，仅仅在 kubelet 的 Node 上运行。Kubernetes 控制面可以看见，但是无法管理它。kubelet 也不会其进行健康检查。 创建静态 Pod 有着两种方式：配置文件 和 HTTP 方式。 2.2.1 配置文件方式 在 kubelet 的配置文件中社会中 staticPodPath 为一个目录，kubelet 会扫描该目录，读取目录下的 .yaml 或 .json 文件，创建对应的 Pod。 另一种方式 也可以使用 kubelet 启动参数 --pod-manifest-path 指定目录，但是这是即将废弃的参数。 你在 APIServer 可以看到该 Pod，但是如果你进行删除操作，该 Pod 会一直进入 Pending 状态，但是不会真正被停止。 删除该 Pod 只能通过删除目录下的对应 .yaml 文件。 2.2.2 HTTP 方式 设置 kubelet 的启动参数 --manifest-url，kubelet 会定期从该 URL 地址下载文件，然后以 yaml 或 json 格式解析，然后创建对应的 Pod。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:2:2","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"3 Pod 的生命周期 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:3:0","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"3.1 Pod phase Pod 的 status.phase 属性表明了 Pod 的生命周期阶段： status:phase:Running Pod 生命周期包含如下阶段： 状态 含义 Pending Pod 被创建后到 Pod 内所有容器成功创建前，都属于 Pending 阶段，包括了下载镜像期间。 Running Pod 内的所有容器被成功创建，至少一个容器还在运行（即使其他容器异常重启）。 Succeeded Pod 内所有容器成功执行并退出（退出码为 0），并且 Pod 不再会被重新拉起。 Failed Pod 内所有容器都已经停止，并且至少一个容器是异常退出的（退出码非 0）。 Unknown 无法得知 Pod 的状态。大多数情况是 Node 失联导致。 STATUS of kubectl 你会发现通过 kubectl get pods 输出的 STATUS 可能并不是上述的 phase，而是 Pod 中容器状态的一个原因，即 status.containerStatuses[].state.\u003c\u003e.reason，这是为了直观输出 Pod 的异常原因。 通过 kubectl get pods -o yaml 可以看到 status.phase。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:3:1","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"3.2 Container State Kubernetes 会跟踪 Pod 下每个容器的状态，位于 status.containerStatuses.state 字段。 容器可能的状态有： 状态 含义 Waiting 等待运行。可能由于正在拉取 image，或者等待 init container 运行等情况。 Running 容器正在运行中。 Terminated 容器运行结束，无论正常退出还是异常退出。 通过 status.containerStatuses.state 你可以看到为啥容器处于该状态。 3.2.1 容器声明周期回调 Kubernetes 提供两个容器生命周期的回调： postStart ：在容器创建后立即异步执行，因此不保证在容器的 entrypoint 前执行。 但是 kubelet 会等待 postStart 后执行完，才将容器的状态变为 Running。 preStop ：在 kubelet 发送容器停止信号前，执行的操作。 Note 注意，preStop 仅仅在 kubelet 停止容器的情况下才会执行（因为这是 kubelet 执行的操作），而容器自己退出的情况下不会执行。 使用回调的示例如下： apiVersion:v1kind:Podmetadata:name:lifecycle-demospec:containers:- name:lifecycle-demo-containerimage:nginxlifecycle:postStart:httpGet:path:/port:80preStop:exec:command:[\"/bin/sh\",\"-c\",\"nginx -s quit; while killall -0 nginx; do sleep 1; done\"] 可以看到，回调可以配置两种执行方式： httpGet ：执行一次 HTTP GET 请求； exec ：执行命令行； ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:3:2","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"3.3 Pod Condition Pod 的 status.conditions 表明了 Pod 的一些细节情况。 默认包含以下的 condition： Conditon 含义 PodScheduled Pod 是否已经被调度到某个节点。 Initialized 所有 Init Container 是否成功执行。 ContainersReady 所有容器是否是就绪的（运行中）。 Ready Pod 是否能够提供服务，即能否加入到 Service 的 Endpoints 中。 Ready 状态受到 Readiness Probe 的控制。 status:conditions:- lastProbeTime:nulllastTransitionTime:\"2021-06-11T07:11:46Z\"status:\"True\"type:Initialized- lastProbeTime:nulllastTransitionTime:\"2021-06-12T15:32:39Z\"message: 'containers with unready status:[pd]'reason:ContainersNotReadystatus:\"False\"type:Ready- lastProbeTime:nulllastTransitionTime:\"2021-06-12T15:32:39Z\"message: 'containers with unready status:[pd]'reason:ContainersNotReadystatus:\"False\"type:ContainersReady- lastProbeTime:nulllastTransitionTime:\"2021-06-11T07:11:46Z\"status:\"True\"type:PodScheduled 可以看到每个 Condition 都包含了如下信息： type ：condition 类型； status ：condition 是否正常，包括 True False Unknown； lastProbeTime ：上一次检查该 condition 的时间； lastTransitionTime ：上一次 condition status 变化的时间； reason ：上一次 condition status 变化的原因； message ：上一次 conditon status 变化的原因信息； 3.3.1 自定义 condition 除了默认的 condition 外，可以通过 spec.readinessGates 来自定义 condition。 自定义 condition 往往由一个 controller 控制，通过自定义 controller 来主动将自定义 condition 设置为 true。 自定义 condition 仅仅会影响到 Ready condition，只有所有自定义 condition 为 true 后，Ready condition 才能变为 true。 apiVersion:v1kind:Podmetadata:name:centosspec:readinessGates:- conditionType:\"www.example.com/feature-1\" ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:3:3","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"3.4 Pod 重启策略 通过 spec.restartPolicy 指定 Pod 的重启策略，其可能的值为： Always ：容器退出时，kubelet 自动重启该容器； OnFailure ：容器退出且退出码非 0 时，kubelet 自动重启容器； Never ：任何容器提出，kubelet 都不重启容器； kubelet 重启容器的时间间隔以指数的方式增长（1n、2n、4n …），最长延迟 5min。如果容器运行了 10min 没有退出，则间隔时间会被重置。 重启时间间隔单位 重启间隔时间是以 kubelet handle 周期为单位。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:3:4","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"4 Probe Probe 以旁路的方式，周期性地检测 Pod 容器的情况，检测失败是及时更新 Pod 的状态，使得上层感知后进行一些恢复操作。 目前包含三种探针：LivenessProbe、ReadinessProbe 和 StartupProbe。 容器维度 要明确，Probe 是以容器为对象的，而不是 Pod。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:4:0","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"4.1 LivenessProbe LivenessProbe 用于周期性判断一个容器是否是存活的，如果 LivenessProbe 探测失败，那么 kubelet 将会 “杀掉” 该容器。 容器停止后，后续的操作是由 重启策略 控制的。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:4:1","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"4.2 ReadinessProbe ReadinessProbe 用于周期性判断一个容器是否是 Ready，从而影响 Pod 的 Ready condition 是否为 True。 只有 Pod Ready condition 为 True 时，Service 才可以将其 Pod 加入到 Endpoints，从而该 Pod 才能接受请求并处理。 同样，当 Pod Ready condition 从 True 变为 False 时，Service 会将其 Pod 从 Endpoints 移除。 为何还需要 Readiness Gate？ 因为 ReadinessProbe 是一种被动探测的方式，而 Readiness Gate 提供了一种主动修改的方式。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:4:2","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"4.3 StartupProbe StartupProbe 仅仅在容器启动后会运行，并且成功后不会再次运行，直到容器重新启动。 StartupProbe 是为了解决有些容器的启动情况很慢的情况，这种情况只需要进行一次成功的探测，所以不适合 ReadyinessProbe 的周期性语义。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:4:3","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"4.4 使用 Probe 三种探针都可以使用三种探测方式： exec ：执行一个 shell 命令，退出码为 0 表明容器健康； spec:containers:- name:livenessimage:k8s.gcr.io/livenesslivenessProbe:exec:command:- cat- /tmp/healthy tcpSocket ：连接 TCP 的一个 IP 与 Port，连接成功表明容器健康； spec:containers:- name:goproxyimage:k8s.gcr.io/goproxy:0.1ports:- containerPort:8080readinessProbe:tcpSocket:port:8080 httpGet ：发送一个 HTTP Get 请求，状态码 [200, 400) 表明容器健康； spec:containers:- name:livenesslivenessProbe:httpGet:path:/healthzport:8080httpHeaders:- name:Custom-Headervalue:Awesome 每种探测器也包含如下的配置参数，用以控制探测的行为： initialDelaySeconds ：容器启动后多久开始进行探测。 仅仅适用于 LivenessProbe 与 ReadinessProbe。 periodSeconds ：执行探测的时间间隔。默认 10s。 timeoutSeconds ：每次探测的超时时间。默认 1s。 successThreshold ：探测失败后，经过几次探测成功后，才将其变为健康状态，默认值为 1。 仅仅适用于 StartupProbe。 failureThreshold ：连续探测失败多少次后，才将其容器视为不健康状态，默认值为 3。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:4:4","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"5 Init Container 大多数应用在启动前都需要进行一些初始化的操作。Kubernetes 提供了 init container 来进行 Pod 的初始化操作。 init container 也是普通的容器，但是仅仅只运行一次就结束。多个 init container 的情况下，按照定义的顺序运行，并且只有前面的运行成功后才会运行后面的。 某个 init container 运行失败后，其 Pod 的启动流程会终止。而其后的操作就是受到 重启策略 的控制。 init container 在 spec.initContainers 中定义： apiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappspec:containers:- name:myapp-containerimage:busybox:1.28command:['sh','-c','echo The app is running! \u0026\u0026 sleep 3600']initContainers:- name:init-myserviceimage:busybox:1.28command:['sh','-c',\"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]- name:init-mydbimage:busybox:1.28command:['sh','-c',\"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"] init container 与应用容器的区别如下： 启动顺序。init container 全部启动结束后，Kubernetes 才会初始化 Pod 各种信息，然后启动应用容器 资源限制，见：Resource。 init container 不能设置 ReadinessProber。 Pod 重新启动时，init container 将会重新运行。所以 init container 应该是幂等的。 ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:5:0","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["Kubernetes 学习"],"content":"6 Ephemeral Container 有时由于容器崩溃或者容器镜像不包含 debug 工具，使得通过 kubectl exec 方式不能很好的进行调试。v1.18 版本开始，新添加一个 kubectl debug 命令，用于创建 ephemeral container 来进行 debug。 开启特性 需要开启 EphemeralContainers feature gate 使用 kubectl debug 的示例如下： $ kubectl debug -it ephemeral-demo --image=busybox --target=ephemeral-demo –image ：临时容器使用的镜像； –target ：加入的 Pod 中特定容器的 namespace； 也可以复制一个 Pod，然后添加临时容器来进行调试： $ kubectl debug myapp -it --copy-to=myapp-debug --container=myapp -- sh –copy-to ：将 pod 复制一份新命名的 pod； 你可以让临时容器和应用容器共享 pid namespace，这样可以进行类似 strace 这样的进程排查。 kubectl debug myapp -it --image=ubuntu --share-processes --copy-to=myapp-debug –share-processes ：使用 –copy-to 时，开启 pid namespace 共享； ","date":"2021-06-05","objectID":"/posts/cloud_computing/k8s_learning/1-pod/:6:0","tags":["k8s","云计算"],"title":"K8s 学习 - 1 - Pod","uri":"/posts/cloud_computing/k8s_learning/1-pod/"},{"categories":["网络"],"content":"SSL/TLS 总结","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 之前一直对 TLS 与证书理解的不清晰，这里尝试进行总结一下。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:0:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"1 密码套件 SSL 在握手的第一步就是确认所使用的密码套件Cipher Suite，使得首先 client 与 server 使用相同的算法进行通信。其中包含三个组件： 非对称加密算法：表明传输秘钥使用的非对称加密算法，例如 RSA、DHE_RSA 等； 对称加密算法：加密握手后，传输数据使用的加密算法，例如 AES_128_CBC 等； 数据摘要算法：数据校验使用的算法，例如 SHA256 等； 例如，Server 端传输 TLS_RSA_WITH_AES_256_CBC_SHA256 算法套件，表明其询问使用 RSA + AES_256_CBC + SHA256 三个算法组件。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:1:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"1.1 对称加密算法 对称加密算法的作用很简单：使用同一个秘钥，发送端进行数据加密，接收端进行数据解密。 相比于非对称加密，加解密速度快，所以会用于传输数据的加解密。 但是，对称加密无法解决秘钥传输问题，一旦双方交换秘钥时泄露，那么数据的安全性就无法保障了。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:1:1","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"1.2 非对称加密算法 这里不会说明非对称加密算法的算法原理，而是想弄清楚为什么使用非对称加密算法。 首先，我们明确非对称加密算法的作用。非对称加密算法包含两个密码： 公钥：可以公开的秘钥； 私钥：不可公开，自身持有的秘钥； 作用很简单，对于非对称加密，用公钥加密的数据只能用私钥解开，用私钥加密的数据只能用公钥解开。 这样的作用在哪？因为对于对称加密，交换的秘钥一旦泄露，那么加密的数据就会被破解。而对于非对称加密，需要交换的往往只有公钥，而公钥被泄露后，那么使用公钥加密的数据还是无法被破解的，那么还是能保证了单向的数据安全性。 但是，非对称加密算法加解密速度比较慢，所以在 TLS 中仅仅用于交换秘钥。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:1:2","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"1.3 摘要算法 摘要算法不是用于数据加密的，而是用于数据的校验。无论多大的数据，经过摘要算法最后得到固定长度的数据。而不同的数据得到的摘要结果是不同的。 因此，我们比较两个数据的摘要结果，就可以确定两个数据是否相同，也就是数据检验。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:1:3","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2 SSH 在 TLS 之前，先说一下 SSH，github 与 ssh 登录都使用了 SSH 协议。 SSHSecure Shell 是建立在应用层上基础上的网络安全协议（注意，其没有使用 TLS 协议）。 SSH 使用非对称加密技术 RSA 加密了所有传输的数据。使用了两种验证方式： 基于口令的安全验证：只要知道需要登录的机器的账号与密码，就可以登录。所有传输的数据都会被加密，但是无法防止“中间人”攻击。 基于秘钥的安全验证：提前创建公钥与私钥，把公钥放在服务器上。在客户端要登录服务器时，会发送其本地保存的公钥。服务端在指定目录查找对应公钥是否匹配。 可以看到，服务器使用公钥来进行客户端的验证，因此 Github 与 SSH 登录时都需要将本地生成的公钥先提前配置。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:2:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2 SSL/TLS ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:3:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2.1 SSL 与 TLS TLSTransport Layer Security 是 SSLSecure Sockets Layer 的升级版，下面是其发展的历史： 协议 创建时间 RFC SSL 1.0 - SSL 2.0 1995 SSL 3.0 1996 RFC 6101 TLS 1.0 1999 RFC 2246 TLS 1.1 2006 RFC 4346 TLS 1.2 2008 RFC 5246 TLS 1.3 2019 RFC 8446 而 HTTPS 就是在 TCP 建立连接后，先进行 TLS 协议握手，然后使用加密传输。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:3:1","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2.2 TLS 握手过程 看一下具体的握手过程： ClientHello：客户端发起建立 TLS 连接请求（TCP 连接已经建立） client 发送的消息中，会表明其自身支持的 TLS 以及密码套件，让 server 可以选择合适的算法。 即包括几个重要信息： 支持的 TLS 最高版本； 支持的密码套件，按照优先级会进行排序； 支持的压缩算法； （可选）session id，通过 session 重用可以跳过一些握手过程； 随机串，后序生成密码使用，使得每次握手得到的密码都是不一样的； ServerHello：服务端选出合适的密码套件 server 根据 client 的 hello 消息，在自己的证书中查找合适的证书（证书里包含了非对称加密算法和摘要算法），并挑选出合适的对称加密算法，也就得到了密码套件。同时，也会返回一个随机串。 如果 TLS 版本，或者密码套件无法匹配，那么直接会连接失败。 Certificate：服务端发送自己的证书，让 client 检查 ServerKeyExchange（可选）：发送非对接加密 premaster 生成所需的参数（如果算法需要的话） 对于一些非对称加密算法（DHE_RSA），需要 server 传递一些特殊的参数，用以生成【准密码（premaster）】，因此需要传递一些特殊参数。 CertificateRequest（可选）：如果服务端开启双向认证，那么就发送请求，要求 client 提供证书 在内部服务的 HTTPS 中，有时候 server 需要去验证 client 是否是内部的，也就是需要进行双向认证，这就需要 server 去检查 client 的证书。 当然，大部分 Web 服务不需要，而 client 认证是通过账号密码来决定的。 ServerHelloDone：服务端表明结束 Certificate（可选）：client 发送自己的证书 如果第 5 步发生，那么 client 需要发送自己的证书。就算 client 没有证书，也要发送空的消息，让 server 决定是否继续。 ClientKeyExchange：验证完毕证书后，生成 premaster，通过证书中公钥加密后发送 client 验证完毕 server 证书后，生成非对称加密算法的 premaster，然后通过证书的公钥发送。 如果算法需要，会使用第 4 步接收的相关参数进行生成。 CertificateVerify（可选）：发送 client 校验码 + 私钥加密校验码的数据，让 server 尝试解密并验证，来验证 client 证书确实是属性 client 的 如果第 7 步执行，那么为了防止 client 发送一个不属于自身的证书，所以需要进行一次非对称加密通信，使得 server 确保证书是属于 client 的（client 拥有着私钥）。 Finished ：根据密码套件，双方算出 master 密码，并且使用对称加密算法进行一次通信，来验证密码无误。 根据加密套件 + premaster，并且使用 client 随机串 + server 随机串，client server 各独立生成 master 密码，并且是一样的。 接着 client 与 server 都会使用缓存的校验码，使用 master 密码进行对称加密，然后发送给对方，让对方使用 master 密码进行解密。这样来验证 master 密码双方使用的是一样的。 TLS 完成后，client server 就得到了相同的 master 密码，作为后续对称加密通信的密码。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:3:2","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"2.3 为何安全 在上面的流程中可以看到，首先 client server 使用了非对称加密算法来进行 premaster 的交换（第 8 步使用公钥加密后发送），而最终的 master 密码生成需要使用 premaster 作为一个参数。 可以简单地理解，双方生成密码的一个重要参数使用了非对称加密算法进行传输，保证了数据的安全，这样最后各自独立生成的 master 密码就不会泄露。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:3:3","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"3 证书 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:4:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"3.1 原理 我们从申请证书的过程看： 生成 申请（签名）文件，申请文件中包含了要使用的公钥，以及一些申请者的信息：地点、组织等。 将申请文件通过安全的方式（私下）发送给 CA。 CA 就是一个用于分发证书的权威机构。 CA 使用自己的私钥，对申请文件进行签名（加密），得到了证书。 可以看到，最后得到的证书中包含了： 加密后的公钥 申请者的信息 CA 信息 证书年限 等 在浏览器与服务器中，会内置世界上所有 CA 的根证书，也就是包含了 CA 的公钥。通过证书公钥对 server 证书进行解密，并且判断其中的信息，就可以判断出该证书是否是 CA 签署的。 Note xx.crt 是证书文件，xx.crs 是申请文件 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:4:1","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"3.2 自签名 当我们内部服务，仅仅需要自签名时，其实就是自己生成了称为了一个 CA，得到了根证书与私钥。 而后使用私钥给 server 签证书，将根证书内置给 client 使用，就可以进行自签名的 TLS 通信。 下面进行一次自签名的过程： 生成根证书与私钥 openssl genrsa -out root.key 2048 openssl req -new -x509 -days 3650 -key root.key -out ca.crt 生成 server 证书与私钥，并使用根证书私钥签名 openssl req -newkey rsa:2048 -nodes -keyout server.key -out server.csr openssl x509 -req -extfile \u003c(printf \"subjectAltName=DNS:caliper.xycloud.com\") -days 3650 -in server.csr -CA root.crt -CAkey root.key -CAcreateserial -out server.crt （可选）如果需要双向认证，那么要生成 client 的证书，并使用相同的根证书签名。 openssl req -newkey rsa:2048 -nodes -keyout client.key -out client.csr openssl x509 -req -days 3650 -in client.csr -CA root.crt -CAkey root.key -CAcreateserial -out client.crt ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:4:2","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"3.2 总结 换个角度看，为了实现自签名，双向 TLS，两边需要的文件 server 端需要 server.key - server 端私钥 server.crt - 通过 server.csr，由 CA 签名后的证书 certfile := filepath.Join(global.ConfDir(), \"server.crt\") keyfile := filepath.Join(global.ConfDir(), \"server.key\") err := s.server.ListenAndServeTLS(certfile, keyfile) client 端需要 client.key - client 端私钥 client.crt - 通过 client.csr，由 CA 签名后的证书 ca.crt - CA 证书，因为是自签名 Note 如果是单向 TLS，那么就不需要 client 私钥与证书，CA 证书还是需要内置。 而验证就是 client 使用公钥正常解密数据，然后检查下数据。 所以证书，其实就是给 server “盖了一个章”，表明其是权威机构认证的。而 client 内置的根证书使得 client 可以认到这个权威机构。 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:4:3","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["网络"],"content":"参考 Blog：SSL/TLS 及证书概述 ","date":"2021-05-01","objectID":"/posts/net/tls-%E6%80%BB%E7%BB%93/:5:0","tags":["网络"],"title":"SSL/TLS 总结","uri":"/posts/net/tls-%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"VFS 的实现","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Linux 存储架构中的 VFS 层的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:0:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"1 VFS 整体模型 虚拟文件系统Virtual File System 是在用户进程与文件系统之间的一个抽象层，使得用户进程无需关系文件系统的实现，而通过相同的 IO 接口就可以进行数据的读写。 当用户程序对一个 fd 调用 IO 系统调用时，VFS 根据其对应的文件系统，调用其各个函数的实现。 可以将其想象问，你定义了一个 Golang interface VFS，而不同的文件系统实现了其 interface 的操作。上层只需要调用 VFS 的各个接口即可。 Tip “一切的抽象层都是为了解耦” ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:1:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2 VFS 的通用文件模型 为了管理不同的文件系统，VFS 需要一个通用的文件模型来抽象整个文件系统的模型，而不同的文件系统需要通过自身的实现来满足这个模型。 VFS 的通用文件模型主要包含 4 个对象： superblock：保存文件系统的基本元数据，一个文件系统有着一个 superblock 对象，包含文件系统类型、大小、状态等信息； inode：保存一个文件或目录相关的元信息，代表着一个文件或目录，包含文件访问权限、文件类型、大小等（不包括文件名）； file：进程打开文件后，文件的表示，进程所看见的文件； dentry：保存文件名（目录名）与其 inode 的对应关系，用于加速文件的查询； ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.1 inode inode 是针对文件系统维度而言的，是每个文件或目录在文件系统的唯一标识，并包含其相关的元信息。可以想到，inode 需要持久化在具体的存储设备上，用以机器重启后恢复文件信息。 看一下 inode 的结构，可以知晓其对应的包含的信息： // fs.h struct inode { umode_t i_mode; kuid_t i_uid; kgid_t i_gid; unsigned int i_flags; const struct inode_operations *i_op; struct super_block *i_sb; struct address_space *i_mapping; #ifdef CONFIG_SECURITY void *i_security; #endif /* Stat data, not accessed from path walking */ unsigned long i_ino; /* * Filesystems may only read i_nlink directly. They shall use the * following functions for modification: * * (set|clear|inc|drop)_nlink * inode_(inc|dec)_link_count */ union { const unsigned int i_nlink; unsigned int __i_nlink; }; dev_t i_rdev; loff_t i_size; struct timespec64 i_atime; struct timespec64 i_mtime; struct timespec64 i_ctime; spinlock_t i_lock; /* i_blocks, i_bytes, maybe i_size */ unsigned short i_bytes; u8 i_blkbits; u8 i_write_hint; blkcnt_t i_blocks; #ifdef __NEED_I_SIZE_ORDERED seqcount_t i_size_seqcount; #endif /* Misc */ unsigned long i_state; struct rw_semaphore i_rwsem; unsigned long dirtied_when; /* jiffies of first dirtying */ unsigned long dirtied_time_when; struct hlist_node i_hash; struct list_head i_io_list; /* backing dev IO list */ #ifdef CONFIG_CGROUP_WRITEBACK struct bdi_writeback *i_wb; /* the associated cgroup wb */ /* foreign inode detection, see wbc_detach_inode() */ int i_wb_frn_winner; u16 i_wb_frn_avg_time; u16 i_wb_frn_history; #endif struct list_head i_lru; /* inode LRU list */ struct list_head i_sb_list; struct list_head i_wb_list; /* backing dev writeback list */ union { struct hlist_head i_dentry; struct rcu_head i_rcu; }; atomic64_t i_version; atomic64_t i_sequence; /* see futex */ atomic_t i_count; atomic_t i_dio_count; atomic_t i_writecount; #if defined(CONFIG_IMA) || defined(CONFIG_FILE_LOCKING) atomic_t i_readcount; /* struct files open RO */ #endif union { const struct file_operations *i_fop; /* former -\u003ei_op-\u003edefault_file_ops */ void (*free_inode)(struct inode *); }; struct file_lock_context *i_flctx; struct address_space i_data; struct list_head i_devices; union { struct pipe_inode_info *i_pipe; struct block_device *i_bdev; struct cdev *i_cdev; char *i_link; unsigned i_dir_seq; }; __u32 i_generation; void *i_private; /* fs or device private pointer */ } __randomize_layout; i_mode：文件类型，文件、目录等，也包括一些权限信息； i_uid、i_gid：文件所属的 UID 与 GID； i_flags：文件状态标记； i_op：inode 操作集合，不同的文件系统注册不同的操作； i_sb：指向所属文件系统的 superblock 结构； i_mapping：inode 对应的地址空间（address_space），对于 mmap 很重要； i_ino：inode 索引号； i_nlink：inode 引用计数； i_rdev：inode 表示设备文件时，为设备的 dev number（major+minor）； i_size：inode 对应的文件大小； i_atime、i_mtime、i_ctime：文件最后访问时间、文件内容最后修改时间、inode 最后变化时间（权限、所有者等） i_bytes：文件占用存储设备的字节数（考虑稀疏文件）； i_blocks：文件占用存储设备的块数（考虑稀疏文件）； i_hash：inode 所存在的 hash 表； i_lru：inode 构成的 LRU list； i_fop：文件操作函数集合，不同文件系统不同实现； 从上面可以看到，任何文件系统上的每一个单元（普通文件、目录、设备、管道等等），在 VFS 都由一个 inode 来抽象。 因此，对于每个进程，所有的操作都是相同的，即基于文件系统某个“文件”的增删改查等等，接口是统一的。而各个文件系统就要实现自身的 i_op、i_fop 操作。 例如，对于网络 IO，也是通过 VFS 层操作的，而需要网络子系统实现一个走网络的 i_fop 操作。 Note “一切皆文件” 2.1.1 文件系统对 inode 操作 看一下文件系统对 inode 的操作 inode_operations： // fs.h struct inode_operations { struct dentry * (*lookup) (struct inode *,struct dentry *, unsigned int); const char * (*get_link) (struct dentry *, struct inode *, struct delayed_call *); int (*permission) (struct inode *, int); struct posix_acl * (*get_acl)(struct inode *, int); int (*readlink) (struct dentry *, char __user *,int); int (*create) (struct inode *,struct dentry *, umode_t, bool); int (*link) (struct dentry *,struct inode *,struct dentry *); int (*unlink) (struct inode *,struct dentry *); int (*symlink) (struct inode *,struct dentry *,const char *); int (*mkdir) (struct inode *,struct dentry *,umode_t); int (*rmdir) (struct inode *,struct dentry *); int (*mknod) (struct inode *,struct dentry *,umode_t,dev_t); int (*rename) (struct inode *, struct dentry *, struct inode *, struct dentry *, unsigned int); int (*setattr) (struct dentry *, struct iattr *); int (*getattr) (const struct path *, struct kstat *, u32, uns","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:1","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.2 superblock superblock 包含一个具体文件系统的全局信息。 struct super_block { struct list_head s_list; /* Keep this first */ dev_t s_dev; /* search index; _not_ kdev_t */ unsigned char s_blocksize_bits; unsigned long s_blocksize; loff_t s_maxbytes; /* Max file size */ struct file_system_type *s_type; const struct super_operations *s_op; const struct dquot_operations *dq_op; const struct quotactl_ops *s_qcop; const struct export_operations *s_export_op; unsigned long s_flags; unsigned long s_iflags; /* internal SB_I_* flags */ unsigned long s_magic; struct dentry *s_root; struct rw_semaphore s_umount; int s_count; atomic_t s_active; char s_id[32]; /* Informational name */ uuid_t s_uuid; /* UUID */ unsigned int s_max_links; fmode_t s_mode; const struct dentry_operations *s_d_op; /* default d_op for dentries */ /* s_inode_list_lock protects s_inodes */ spinlock_t s_inode_list_lock ____cacheline_aligned_in_smp; struct list_head s_inodes; /* all inodes */ spinlock_t s_inode_wblist_lock; struct list_head s_inodes_wb; /* writeback inodes */ } __randomize_layout; s_list ：所有存在的文件系统以 list 方式组织； s_dev ：文件系统对应的设备的 dev 编号； s_blocksize_bits、s_blocksize ： s_maxbytes ：可以创建的最大文件大小； s_type ：文件系统类型； s_op ：文件系统操作，包含 mount、umount 等操作的实现； dq_op ：文件系统 quota 功能的实现； s_magic ：文件系统 magic number，每类文件系统有着唯一的 magic number； s_root ：root 目录，如果是存储文件系统的话； s_uuid ：文件系统 UUID，每个文件系统有着单机唯一的 UUID； s_inodes ：所有 inode 的组成的链表； s_inodes_wb ：所有等待 writeback 的 inode 组成的链表； ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:2","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.3 file file 是文件在进程中的表示，当进程通过 open() 打开一个文件得到文件描述符后，内核就会有一个 file 对象来表示这个“打开”。 如下图所示，每个进程所打开的每个 fd，就对应这一个 file 结构对象。 看下具体的 file 对象： // fs.h struct file { union { struct llist_node fu_llist; struct rcu_head fu_rcuhead; } f_u; struct path f_path; struct inode *f_inode; /* cached value */ const struct file_operations *f_op; spinlock_t f_lock; enum rw_hint f_write_hint; atomic_long_t f_count; unsigned int f_flags; fmode_t f_mode; struct mutex f_pos_lock; loff_t f_pos; struct fown_struct f_owner; const struct cred *f_cred; struct file_ra_state f_ra; u64 f_version; #ifdef CONFIG_SECURITY void *f_security; #endif /* needed for tty driver, and maybe others */ void *private_data; #ifdef CONFIG_EPOLL /* Used by fs/eventpoll.c to link all the hooks to this file */ struct list_head f_ep_links; struct list_head f_tfile_llink; #endif /* #ifdef CONFIG_EPOLL */ struct address_space *f_mapping; … } __randomize_layout __attribute__((aligned(4))); /* lest something weird decides that 2 is OK */ f_owner 包含处理该文件的进程有关信息，例如进程的 pid； f_ra 表示预读相关的操作，包含预读文件数据的大小、预读的方式等； f_mode 为打开文件时传递的方式，例如 readonly rw 等模式； f_flags 为 open 系统调用时传递的额外的 flag 标志； f_path 用于维护两个信息： 文件名与 inode 之间的关联（dentry）； 文件所在文件系统的信息（vfsmount）； f_op 为对应 inode 的文件操作函数； f_mapping ：指向文件对应的 inode 的地址空间映射，即 inode -\u003e i_mapping； file.f_path 属性包含文件对应的 dentry 对象，dentry 中包含着对应的 inode 指针，因此完成了 file -\u003e inode 的映射。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:3","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.4 dentry dentry 表示的是一个文件名与其对应的 inode 的关系，dentry 组成的树型结构构建起了某个文件系统的目录树。 每个文件系统有着一个没有父 dentry 的根目录，被文件系统对应的 superblock 所引用。 必须要清楚，磁盘等持久化存储设备上并没有存储 dentry。dentry 树型结构的构建是在 VFS 读取目录、文件的数据后，对应创建并构建的。也就是说，dentry 是一个缓存结构，用于运行时构建出树型组织结构，来加速文件、目录的查找。 看下其主要的数据结构： // dcache.h struct dentry { /* RCU lookup touched fields */ unsigned int d_flags; /* protected by d_lock */ seqcount_spinlock_t d_seq; /* per dentry seqlock */ struct hlist_bl_node d_hash; /* lookup hash list */ struct dentry *d_parent; /* parent directory */ struct qstr d_name; struct inode *d_inode; /* Where the name belongs to - NULL is * negative */ unsigned char d_iname[DNAME_INLINE_LEN]; /* small names */ /* Ref lookup also touches following */ struct lockref d_lockref; /* per-dentry lock and refcount */ const struct dentry_operations *d_op; struct super_block *d_sb; /* The root of the dentry tree */ unsigned long d_time; /* used by d_revalidate */ void *d_fsdata; /* fs-specific data */ union { struct list_head d_lru; /* LRU list */ wait_queue_head_t *d_wait; /* in-lookup ones only */ }; struct list_head d_child; /* child of parent list */ struct list_head d_subdirs; /* our children */ /* * d_alias and d_rcu can share memory */ union { struct hlist_node d_alias; /* inode alias list */ struct hlist_bl_node d_in_lookup_hash; /* only for in-lookup ones */ struct rcu_head d_rcu; } d_u; } __randomize_layout; d_parent：父目录的 dentry，根目录 dentry 指向自身； d_name：文件名/目录名； d_inode：文件名对应文件/目录的 inode； d_op：文件系统 dentry 操作的实现； d_sb：dentry 所属的文件系统的 superblock 结构； d_lockref：dentry 引用计数； d_alias：链表元素，不同 dentry 表示相同文件时，会通过链表链接所有 dentry。例如，使用硬链接将两个不同名称对应同一个文件时，会发生这种情况； 2.4.1 dentry 操作 dentry_operations 结构保存了指向特定文件系统对 dentry 需要实现的操作。 // dcache.h struct dentry_operations { int (*d_revalidate)(struct dentry *, unsigned int); int (*d_weak_revalidate)(struct dentry *, unsigned int); int (*d_hash)(const struct dentry *, struct qstr *); int (*d_compare)(const struct dentry *, unsigned int, const char *, const struct qstr *); int (*d_delete)(const struct dentry *); int (*d_init)(struct dentry *); void (*d_release)(struct dentry *); void (*d_prune)(struct dentry *); void (*d_iput)(struct dentry *, struct inode *); char *(*d_dname)(struct dentry *, char *, int); struct vfsmount *(*d_automount)(struct path *); int (*d_manage)(const struct path *, bool); struct dentry *(*d_real)(struct dentry *, const struct inode *); } ____cacheline_aligned; d_revalidate：检查内存中各个 dentry 对象构建的结构，是否符合当前文件系统的情况。这对于网络文件系统至关重要。 因为网络文件系统不直接关联到内核与 VFS，所有的数据都要通过网络 IO 来收集。该函数用于保证 dentry 结构与实际情况的一致性。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:4","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.5 模型总结 首先从内核角度看，一个已经挂载的文件系统就是由 superblock + inode 组成的。文件系统信息保存在 superblock 中，每个文件由 inode 表示。 对于一个文件的读写，就是找到对应文件的 inode，然后调用底层文件系统的 read/write 接口。一个目录 inode 对应的数据，就是其子文件或者目录的命名。 因为文件的读写增删是非常频繁的，为了在目录树中更快速的查找 inode，内核维护了 dentry 结构树，dentry 包含了文件名与 inode 的映射关系，使得不用依靠读写磁盘来查找文件对应的 inode。 从进程角度看，每个进程可以打开多文件，因此使用 file 结构代表了每个进程打开的文件，从 file -\u003e dentry -\u003e inode 来找到其对应的文件。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:2:5","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3 文件操作实现 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:3:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.1 挂载 文件系统的挂载有 mount 系统调用触发，当文件系统挂载到一个目录时，挂载点的内容就被替换为了被挂载的文件系统中的根目录的内容。前一个目录消失，无法访问，当挂载点被取消挂载后又会重新出现，并且数据不会改变。 每个挂载的文件系统都对应内核中的 mount 结构对象： struct vfsmount { struct dentry *mnt_root; /* root of the mounted tree */ struct super_block *mnt_sb; /* pointer to superblock */ int mnt_flags; } __randomize_layout; struct mount { struct hlist_node mnt_hash; struct mount *mnt_parent; struct dentry *mnt_mountpoint; struct vfsmount mnt; union { struct rcu_head mnt_rcu; struct llist_node mnt_llist; }; #ifdef CONFIG_SMP struct mnt_pcp __percpu *mnt_pcp; #else int mnt_count; int mnt_writers; #endif struct list_head mnt_mounts; /* list of children, anchored here */ struct list_head mnt_child; /* and going through their mnt_child */ struct list_head mnt_instance; /* mount instance on sb-\u003es_mounts */ const char *mnt_devname; /* Name of device e.g. /dev/dsk/hda1 */ struct list_head mnt_list; struct list_head mnt_expire; /* link in fs-specific expiry list */ struct list_head mnt_share; /* circular list of shared mounts */ struct list_head mnt_slave_list;/* list of slave mounts */ struct list_head mnt_slave; /* slave list entry */ struct mount *mnt_master; /* slave is on master-\u003emnt_slave_list */ struct mnt_namespace *mnt_ns; /* containing namespace */ struct mountpoint *mnt_mp; /* where is it mounted */ union { struct hlist_node mnt_mp_list; /* list mounts with the same mountpoint */ struct hlist_node mnt_umount; }; struct list_head mnt_umounting; /* list entry for umount propagation */ #ifdef CONFIG_FSNOTIFY struct fsnotify_mark_connector __rcu *mnt_fsnotify_marks; __u32 mnt_fsnotify_mask; #endif int mnt_id; /* mount identifier */ int mnt_group_id; /* peer group identifier */ int mnt_expiry_mark; /* true if marked for expiry */ struct hlist_head mnt_pins; struct hlist_head mnt_stuck_children; } __randomize_layout; mnt_parent：挂载点位于父文件系统挂载； mnt_mountpoint：挂载点，也就是父文件系统中挂载点对应的 dentry； mnt：包含代表的文件系统的挂载属性： mnt_root：文件系统根目录的 dentry； mnt_sb：文件系统对应的 superblock； mnt_flags：挂载标志； mnt_count：挂载的引用计数； mnt_ns：所属的 mount namespace； mnt_devname：挂载的设备名称，例如 /dev/sda； 挂载子树相关数据结构 …； mnt_id：该挂载的唯一 id； mnt_group_id：同组的挂载点的集合，即 peer group id； 3.1.1 共享子树 普通的挂载理解起来并不难，但是涉及到 bind mount 与 mount namespace 后，事情开始不一样了起来。想象几个场景： 如果系统新添加了磁盘，因为容器间 mount namespace 隔离了挂载信息，如果容器想用这个磁盘，是否只能手动进行挂载。 Shared subtrees 可以帮助我们解决这个问题。 共享子树Shared subtrees 就是一种控制子挂载点能否在其他地方被看到的技术，只会在 bind mount 和 mount namespace 中被使用到。 这个概念比较绕，并且也不会常常用到，可以先跳过。 首先，需要先知道两个概念： peer group：一个或多个挂载点的集合，同组的挂载之间会共享挂载信息，也就是可以看到。 目前有两种情况会让两个挂载点属于同一个 peer group： 使用 mount –bind，并且挂载的 propagation type 为 share，这样源目录和目标目录属于同一个 peer group（前提是源是一个挂载点，而不是普通文件或者普通目录）。 当创建新的 mount namespace 时，新 namespace 会拷贝一份老 namespace 的挂载点信息，于是新的和老的 namespace 里面的相同挂载点就会属于同一个 peer group。 propagation type：每个挂载点都有一个 type 标志，由它决定同一个 peer group 里的其他的挂载点下面是不是也会创建和移除相应的挂载点。 目前包含四种类型的 propagation type： MS_SHARED：挂载信息在同一个 peer group 的不同挂载点之间共享传播； MS_PRIVATE：挂载信息不共享，设置 private 的挂载点不属于任何的 peer group； MS_SLAVE：传播是单向的，同一个 peer group 中，master 的挂载点下面发生变化会共享到 slave。反之，slave 下变化不会影响 master； MS_UNBINDABLE：和 MS_PRIVATE 相同，只是这种类型挂载点不能作为 bind mount； 可以看到，peer cgroup 的概念就是 mount 结构中的 mnt_group_id，各个不同的 type 的子挂载，会被保存在不同的 mount 的链表中。 有个大神给出了清晰的示例与解释，见：Shared subtrees ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:3:1","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.2 查找 inode 查找 inode 应该是最常见的操作了，当你要打开文件或者进入目录，内核第一个任务就是要根据其路径，来查找到对应的 inode。 因为 dentry 的存在，查找自然分为了：通过 dentry 缓存查找，以及通过实际的读文件系统查找。 先看下通过 dentry 缓存查找： 解析文件路径，然后不断查找对应的 dentry。 例如 /a/b/c，先通过 root dentry 查找 a 对应的 dentry，接着通过 a dentry 查找 b，不断循环。 因为 dentry 只是缓存，所以需要使用 d_revalidate() 来检查 dentry 是否是有效地。 返回有效 dentry 的 inode。 当对应 dentry 不存在，或者无效之后，就会通过文件系统 inode 操作 lookup() 接口，通过底层的文件系统进行查找，并将其重新记录到 dentry 中。 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:3:2","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"参考 Blog：linux 的 VFS 详解 ","date":"2021-03-13","objectID":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/:4:0","tags":["linux","storage"],"title":"Linux 存储架构 - VFS 总结","uri":"/posts/linux/storage/linux-%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84-vfs%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"Golang 常见调试方法归纳总结","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Golang 调试方法的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:0:0","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1 go pprof Go pprof 是 Go 性能分析工具，在程序运行的过程中，可以记录运行的：CPU、Mem、goroutine 等情况。基本定位 Go 程序的问题第一反应就是使用 go pprof。 pprof 像基本的采集一样，分为采集、上报、分析三个部分。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:1:0","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.1 采集 Go 标准库提供个各个数据的采集接口，可以主动开启 pprof 并将数据写入到 io.writer 中。 其包括： pprof.Profiles() ：返回支持的 profile 的快照。包括：goroutine、threadcreate、heap、allocs、block、mutex； trace.Start() / trace.Stop() ：开启各个事件的追踪，包括 goroutine 状态变更、GC 事件、mheap 大小变更等； runtime 包函数； 不过大多数情况下，我们通过使用 net/http/pprof 就行了，其注册了各个 HTTP 请求的处理函数，而其函数使用前面的接口进行的数据的采集。 // net/http/pprof/pprof.go func init() { http.HandleFunc(\"/debug/pprof/\", Index) http.HandleFunc(\"/debug/pprof/cmdline\", Cmdline) http.HandleFunc(\"/debug/pprof/profile\", Profile) http.HandleFunc(\"/debug/pprof/symbol\", Symbol) http.HandleFunc(\"/debug/pprof/trace\", Trace) } /debug/pprof/ ：引导界面，包含各个子类别的访问； /debug/pprof/cmdline ：输出程序启动的命令行； /debug/pprof/profile ：输出大多数 profile 结果，调用 pprof.Profiles()； /debug/pprof/trace ：一定时间内 go runtime 事件的追踪，调用 trace.Start()； ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:1:1","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.2 上报 前面看到，net/http/pprof 包就 init 注册对应的 HTTP 请求处理函数，而我们只要在程序中导入 net/http/pprof 并启动一个 HTTP Server 就可以使用其提供的上报的功能。 package main import ( \"net/http\" _ \"net/http/pprof\" ) func main() { // … // 启动对应的 HTTP Server // 这里我的程序中 main 函数后面会阻塞, 所以用协程启动 server go func() { if err := http.ListenAndServe(\"0.0.0.0:12300\", nil); err != nil { panic(err) } }() // … } 1.2.1 浏览器方式 浏览器打开指定的地址的 /debug/pprof/ 页面，可以看到引导页面： 各个支持的 profile 都有简要的解释，这里翻译一下： allocs ：过去所有的内存分配的样本； block ： cmdline ：输出程序执行的命令； goroutine ：所有 goroutine 当前执行的上下文； heap ：heap 内存使用的样本； mutex ： profile ：一定时间内的 CPU profile，可以通过参数指定采集多少时间； threadcreate ：创建新的系统线程的函数执行上下文，也许在 cgo 中很有用； trace ：一定时间内程序的执行 trace，可以通过参数指定采集时间。 当你点击其中的某一项时，就会得到一个用于 profile 的数据文件，通过 go tool profile 或者 go tool trace 等命令可以将其可视化。 Tip 也许你点击 heap 或者其他会发现浏览器打开了一个新的页面，这是浏览器直接将获取的数据展示了出来，但是其数据可读性很差。 你可以使用 curl 保存数据文件，或者直接通过 go tool profile \u003caddr\u003e/debug/profile/heap 来获取文件并解析。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:1:2","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.3 解析 通过 go tool pprof 可以解析 pprof 得到的数据文件。go tool pprof \u003cfile\u003e 执行后会进入一个交互式命令行，其中通过各个命令可以进行数据文件的解析与展示。 常用的命令包括： top \u003cN\u003e ：展示前 N 个数据最大的项； list \u003cpackge\u003e.\u003cfunc\u003e ：列出某个函数的指标； traces ：展示各个 goroutine 的调用栈，以及对应的指标； web ：通过浏览器打开解析后的图片； 1.3.1 heap 我们先来看一下 heap 项的数据文件，下载后通过 go tool pprof 的 web 命令打开： $ go tool pprof http://10.9.195.138:12300/debug/pprof/heap 弹出的浏览器的图片中，展示各个函数的申请的 heap 占用。其中，框越大就代表使用的越多，所以我们很快可以找到内存使用最多的函数。 1.3.2 alloc 对于 heap 项展示的是一瞬间的 heap 使用的快照。但是有一些情况从 heap 项的数据中看不出来，例如一直在快速申请无用内存，而导致 GC 频繁触发。这种情况虽然整体程序内存占用不大，但是是频繁 GC 之后的假象。 而 alloc 就是用于展示申请内存大小，这与 heap 展示当前占用内存大小是不一样的。 $ go tool pprof http://10.9.195.138:12300/debug/pprof/allocs 1.3.3 goroutine Go 自带内存回收，所以一般不会发生内存泄漏的情况。而大部分所说的内存泄漏，都是由于 goroutine 泄漏导致的 goroutine 结构占用内存过多。 如果直接点击浏览器的 goroutine 项中，会展示出所有 goroutine 的上下文。 但是这不利于分析整体的 goroutine 情况，还是通过 go tool profile 命令来看： $ go tool pprof http://10.9.195.138:12300/debug/pprof/goroutine 在解析后的图片中，可以看到各个函数创建的 goroutine 的数量，因此可以很快找到 goroutine 泄漏的情况（图片不是泄漏，只是展示一下）。 1.3.4 profile profile 项用于展示各个函数的 CPU 执行时间，当你发现程序 CPU 使用异常时，这项非常关键。 默认情况会程序在收到情况后，采集 30s 内的各个函数的 CPU 执行时间。你可以通过设置参数来调整该时间。 $ go tool pprof http://10.9.195.138:12300/debug/pprof/profile?seconds=30 一眼就可以看到执行时间最长的函数流： 1.3.5 trace 与前面几个不同，trace 可以用于跟踪程序的执行情况。go runtime 会上报特定的 trace 事件，而 trace 将其收集并展示出来。 $ curl http://10.9.195.138:12300/debug/pprof/trace\\?seconds\\=30 \u003e trace.out $ go tool trace trace.out 执行命令后，浏览器会弹出一个页面（必须是 Chrome），包含几个分析项，包括： 项目 作用 View trace 按照时间维度观察各个事件； Goroutine analysis 查看各个 G 的统计信息，包括执行时间，阻塞时间等； Network blocking profile 网络接口阻塞情况； Synchronization blocking profile：用户态阻塞情况，包括 select、chan 阻塞等； Syscall blocking profile 系统调用导致的阻塞情况，最常见的就是 IO 系统调用； Scheduler latency profile 各个函数的执行时间情况； User defined task User defined regions Minimum mutator utilization 展示出了 GC 外，应用能够获得的 CPU 资源的最小比例； (1) View trace 选择 “View trace”，会出现一个以时间为横轴的图片。 按住 W 放大后可以看到，其中每个 P 执行的 G 的时间段与事件（事件执行是个竖线）都会展示出来。各个 G 直接存在事件，点击 Flow events 后还会展示出事件流。 上面 G76 这个 Goroutine，经过了 P0 执行 -\u003e 系统调用阻塞 -\u003e P2 执行 点击其中一个段，可以看到该 G 的执行的统计信息。 Title：G 编号以及执行的函数 Start：启动时间； Wall Duration：执行时间； Start Stack Trace：G 开始执行的函数栈； End Stack Trace：G 切换前或者结束的函数栈； (2) Goroutine analysis 选择 “Goroutine analysis” 后，点击一个具体的 G，可以看到其执行时间的统计。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:1:3","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2 delve pprof 是程序整体的运行情况的数据，如果想对程序执行进行单步调试，就需要使用 delve。 delve 类似于 gdb，但是 gdb 是针对于线程的，无法识别 goroutine。而 delve 是专门用于 Go 程序的，可以做到针对不同 goroutine 打断点，单步执行等操作。 ","date":"2021-02-18","objectID":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/:2:0","tags":["Golang","Golang 开发"],"title":"[WIP] Go 调试方法总结","uri":"/posts/language/golang/go-%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"Linux 网络收发包内核过程总结","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Linux 网络收发包一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:0:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"1 背景知识 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:1:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"1.1 中断 中断终止 CPU 执行流，立即执行必要的处理程序。分为两个类型： 同步中断和异常：由 CPU 自身产生。例如程序崩溃、缺页异常； 异步中断：由 IO 设备产生，任意时间可能发生； 两种中断发生后，CPU 会切换至内核态，并执行 中断处理程序interrupt handler。 对于异步中断，因为会停止当前执行的进程，所以内核要确保中断处理程序尽快完成，尽快返还 CPU。同样，也会阻塞 IO 设备的下一次中断，Linux 将异步中断分为了 硬件中断 与 中断下半部。 1.1.1 硬件中断 硬件中断hardware interrupt 指硬件设备发起信号中断 CPU 执行，CPU 立即进行中断的处理。 实际上，为了不长时间阻塞与中断处理，硬中断往往启动到的是一个通知的作用。例如网卡收到数据包，并通过 DMA 写入 ringbuffer 后，会通过硬中断通知 CPU 从 ringbuffer 读取并处理数据。 通过 /proc/interrupts 可以看到硬中断的触发次数： $ cat /proc/interrupts # … CPU0 100: 15 IR-PCI-MSI 35651584-edge p2p1-TxRx-0 101: 0 IR-PCI-MSI 35651585-edge p2p1-TxRx-1 102: 0 IR-PCI-MSI 35651586-edge p2p1-TxRx-2 103: 0 IR-PCI-MSI 35651587-edge p2p1-TxRx-3 104: 0 IR-PCI-MSI 35651588-edge p2p1-TxRx-4 105: 0 IR-PCI-MSI 35651589-edge p2p1-TxRx-5 106: 0 IR-PCI-MSI 35651590-edge p2p1-TxRx-6 第一行：IRQ 编号； 第二行：每个 CPU 的中断次数； 第三行：中断的类型； 第四行：？？； 第五行：中断发起的设备名； 平时可能只需要关注第五行设备名称就行，因为可能要过滤出网卡对应队列的中断，然后将其绑定触发的 CPU，见 网卡多队列。 1.1.2 中断下半部 中断下半部bottom half 包含三种处理方式： 软中断 softirq：固定的 32 个接口，只留给对时间要求最严格的下半部使用。 查看 /proc/softirqs 文件 可以看到目前支持的软中断： 命名 含义 HI TIMER 定时中断 NET_TX 网络发送 NET_RX 网络接收 BLOCK IRQ_POLL TASKLET tasklet 软中断扩展 SCHED 内核调度 HRTIMER RCU RCU 锁 tasklet：因为软中断只有固定的 32 个，为了支持扩展，tasklet 基于软中断时间，在不同处理器上运行，并且支持通过代码动态注册； 工作队列 work queue：将一个中断的部分工作推后，可以实现一些 tasklet 不能实现的工作（比如可以睡眠）。 一些内核线程会不断处理工作队列的数据，其运行在进程上下文中，并且可以睡眠以及被重新调度。目前，kworker 内核线程 负责处理这个工作。 对于软中断与 tasklet，如果大量出现时，为了不一直进行 CPU 中断，内核会唤醒 ksoftirqd 内核线程进行异步的处理。每个处理器有一个 ksoftirqd/n 线程，n 为 CPU 编号。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:1:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2 网卡层 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:2:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.1 网卡多队列 网卡与系统传输数据包通过两个环形队列：TX ring buffer、RX ring buffer，也称为 DMA 环形队列。平时所说的设置网卡多队列指的就是设置这个环形队列的数量。 当网卡收到帧时，会通过哈希来决定将帧放在哪个 ring buffer 上，然后通过硬中断通知其对应的 CPU 处理。 默认下，处理环形队列数据由 CPU0 负责，可以通过配置 中断亲和性，或者通过开启 irqbalance service 将中断均衡到各个 CPU 上。 2.1.1 配置网卡队列 通过 ethool -l/-L \u003cnic\u003e 命令查看与配置网卡的队列数，通常配置的与机器 CPU 个数一样（如果网卡支持的话）： $ ethtool -l eth0 Channel parameters for eth0: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 8 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 7 $ ethtool -L eth0 combined 8 设置后，你在 /sys/class/net/\u003cnic\u003e/queues/ 可以看到对应的收发队列目录： $ ls /sys/class/net/eth0/queues/ rx-0 rx-1 rx-2 rx-3 rx-4 rx-5 rx-6 rx-7 tx-0 tx-1 tx-2 tx-3 tx-4 tx-5 tx-6 tx-7 在 /proc/interrupts 中也可以看到各个队列对各个 CPU 的中断次数： $ cat /proc/interrupts | grep eth0 # … 这里只打印了一个 CPU 中断 CPU0 62: 0 IR-PCI-MSI 524288-edge eth0 63: 0 IR-PCI-MSI 524289-edge eth0-TxRx-0 64: 0 IR-PCI-MSI 524290-edge eth0-TxRx-1 65: 1766 IR-PCI-MSI 524291-edge eth0-TxRx-2 66: 0 IR-PCI-MSI 524292-edge eth0-TxRx-3 67: 0 IR-PCI-MSI 524293-edge eth0-TxRx-4 68: 0 IR-PCI-MSI 524294-edge eth0-TxRx-5 69: 0 IR-PCI-MSI 524295-edge eth0-TxRx-6 70: 0 IR-PCI-MSI 524296-edge eth0-TxRx-7 通过 ethtool -g/-G \u003cnic\u003e 可以查看与配置 ring buffer 的长度： $ ethtool -g eth0 Ring parameters for eth0: Pre-set maximums: RX: 4096 RX Mini: 0 RX Jumbo: 0 TX: 4096 Current hardware settings: RX: 256 RX Mini: 0 RX Jumbo: 0 TX: 256 2.1.2 配置中断亲和性 如果你发现 CPU0 的中断很高，那么就很有可能所有网卡队列的中断都打到了 CPU0 上。可以通过 /proc/irq/\u003cid\u003e/smp_affinity_list 查看指定编号的中断的对应允许的 CPU。 $ for i in {62..70}; do echo -n \"Interrupt $iis allowed on CPUs \"; cat /proc/irq/$i/smp_affinity_list; done Interrupt 62 is allowed on CPUs 4 Interrupt 63 is allowed on CPUs 8 Interrupt 64 is allowed on CPUs 13 Interrupt 65 is allowed on CPUs 21 Interrupt 66 is allowed on CPUs 6 Interrupt 67 is allowed on CPUs 31 Interrupt 68 is allowed on CPUs 5 Interrupt 69 is allowed on CPUs 2 Interrupt 70 is allowed on CPUs 31 62-70 的网卡队列中断都打到了不同的 CPU 上； 通过写入 echo \"\u003cbitmark\u003e\" \u003e /proc/irq/\u003cid\u003e/smp_affinity 可以中断绑定的 CPU，bitmark 的每个位对应一个 CPU，例如 “0x1111” 表示 CPU 0-3 都可以处理这个中断。 当然，你也可以通过 irqbalance 来均衡各个 CPU 的中断，动态的改变中断与绑定的 CPU。不过，irqbalance 不仅仅针对网卡队列中断，还会调整其他的。 如果你的网卡不支持多队列，可以尝试配置 RPS。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:2:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.2 接收数据 先来看第一个阶段，网卡接收到数据是如何处理的。 packet 进入物理网卡，物理网卡会根据目的 mac 判断是否丢弃（除非混杂模式）； 网卡通过 DMA 方式将 packet 写入到 ringbuffer ringbuffer 由网卡驱动程序分配并初始化。 网卡通过硬中断通知 CPU，有数据来了。 CPU 根据中断执行中断处理函数，该函数会调用网卡驱动函数。 驱动程序先禁用网卡中断（NAPI），表示网卡下次直接写到 ringbuffer 即可，不需要中断通知了。 这样避免 CPU 不断被中断。 驱动程序启动软中断，让 CPU 执行软中断处理函数不断从 ringbuffer 读取并处理 packet。 网卡多队列就是在这里生效，网卡会将 packet 放置到不同的 ringbuffer，不同的 ringbuffer 会中断不同的 CPU（如果设置了中断亲和性），使得各个 CPU 的队列硬中断是均衡的。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:2:2","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"2.3 发送数据 网络设备通过驱动函数发送数据后，就归网卡驱动管了，不同的驱动有着不同的处理方式。 大致的流程如下： 将 sk_buff 放入 TX ringbuff。 通知网卡发送数据。 网卡发送完数据后，通过中断通知 CPU。 收到中断后，进行 sk_buff 的清理工作。 当然，网卡驱动还有一些与 net_device 打交道的地方，比如网卡的队列满了，需要告诉上层不要再发了，等队列有空闲的时候，再通知上层接着发数据。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:2:3","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3 网络访问层 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.1 net_device 每个网络设备都表示为 net_device 一个实例。不同类型的网络设备都会有 net_device 表示，而其相关操作函数的实现不同。 net_device 是针对于 namespace 的，通过 sysfs 你可以看到当前命名空间下所有的 net_device。 $ ls /sys/class/net docker0 enp0s3 enp0s8 lo lxcbr0 vethea62395 net_device 包含了设备相关的所有信息，定义很长， 下面经过简化： struct net_device { char name[IFNAMSIZ]; // IO 相关字段 unsigned long mem_end; unsigned long mem_start; unsigned long base_addr; int irq; unsigned long state; struct list_head dev_list; int ifindex; const struct net_device_ops *netdev_ops; const struct ethtool_ops *ethtool_ops; unsigned int flags; unsigned int mtu; unsigned short type; unsigned short hard_header_len; unsigned char perm_addr[MAX_ADDR_LEN]; unsigned char addr_len; #if IS_ENABLED(CONFIG_VLAN_8021Q) struct vlan_info __rcu *vlan_info; #endif // … 协议相关的特殊指针 /* Interface address info used in eth_type_trans() */ unsigned char *dev_addr; struct netdev_rx_queue *_rx; unsigned int num_rx_queues; unsigned int real_num_rx_queues; struct bpf_prog __rcu *xdp_prog; unsigned long gro_flush_timeout; int napi_defer_hard_irqs; rx_handler_func_t __rcu *rx_handler; void __rcu *rx_handler_data; struct netdev_queue __rcu *ingress_queue; truct netdev_queue *_tx； unsigned int num_tx_queues; unsigned int real_num_tx_queues; struct Qdisc *qdisc; unsigned int tx_queue_len; }; name[IFNAMSIZ] ：设备命名； irq ：irq 编号； ifindex ：设备编号； mtu ：设备的最大传输单元； netdev_ops ：设备的操作接口，不同类型的接口有着不同的操作实现； dev_addr ：网卡硬件地址； _rx ：数据包接收队列（ringbuffer）； _tx ：数据包发送队列； qdisc : 设备进入的 qdisc； ingress_queue ：ingress 队列； 其他包含一些 XDP 相关，统计相关等字段； 大多数的统计信息都可以在对应设备的 sysfs 目录中找到。 网卡命名 linux 内核启动过程中，会默认给网卡以 ethx 方式命名，后面 systemd 回去 rename 网卡名称。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.2 GRO GROGeneric Receive Offloading 用于将 jumbo frame（超过 1500B）的多个分片合并，然后将给上层处理，以减少上层处理数据包的数量。 通过 ethtool -k/K \u003cnic\u003e 查和设置 GRO。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:2","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.3 RPS XPS RFS 3.3.1 RPS 网卡多队列需要硬件的支持，而 RPSReceive Packet Steering 这些则是软件实现多队列，将包让指定的 CPU 去处理。通常情况，当网卡队列数小于 CPU 个数时，可以让 RPS 进一步利用多余的 CPU，让其去处理中断。 Note RPS 的设置是针对单个 ringbuffer 的，与网卡多队列处理不是同一个阶段，也不是冲突的。 开启 RPS 后，数据会由经由被中断 CPU 转发，由其他 CPU 处理。流程如下： 当网卡收到数据存入 ring buffer 后，还是通知指定 CPUx 从 ringbuffer 取出数据； 不过接下来，CPUx 会为每个 packet 哈希放入其他 CPU 的 input_pkt_queue 中。 CPUx 通过 Inter-processor Interrupt (IPI) 中断告知其他 CPU，处理自己的 input_pkt_queue ； 其他 CPU 从各个的 input_pkt_queue 中取出数据包，并处理之后的流程； 可以看到，RPS 不是用于减少 CPU 软中断的次数，而是用于将数据包处理时间均摊到各个 CPU 上，也就是减少单个 CPU 的软中断执行时间（%soft）。 # 配置该 ringbuffer 使用 CPU0 CPU1 的队列 $ echo \"0x11\" \u003e /sys/class/net/eth0/queues/rx-0/rps_cpus # 配置每个 CPU 的 input_pkt_queue 的大小 $ sysctl -w net.core.netdev_max_backlog=1000 3.3.2 RFS RFSReceive Flow Steering 一般和 RPS 配合工作。 RPS 将受到的 packet 经过哈希发配到不同的 CPU input_pkt_queue。而 RFS 会根据 packet 的数据流，发送到对应被处理的 CPU input_pkt_queue 上，即同一个数据流的 packet 会被路由到处理当前数据流的 CPU 上，从而提高 CPU cache 的命中率。 RFS 默认是关闭的，需要通过配置生效，一般推荐的配置如下： # 配置系统期望的活跃连接数 $ sysctl -w net.core.rps_sock_flow_entries=32768 # 配置每个 rps 队列负责的 flow 最大数量，为 rps_sock_flow_entries / N $ echo 2048 \u003e /sys/class/net/eth0/queues/rx-0/rps_flow_cnt 3.3.3 XPS XPSTransmit Packet Steering 则是发送时的多队列处理。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:3","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.4 接收数据 在网卡层中，最后由异步的软中断处理函数来异步的从里 ringbuffer 的数据。而这由内核线程 ksoftirqd 来调用对应的网络软中断函数处理。 所以，在数据包到达 socket buffer 前，数据处理都是由 ksoftirqd 线程执行的，也就是算在软中断处理时间里的。 ksoftirqd 调用驱动程序的 poll 函数来一个个处理 packet。 如果没有 packet 的话，就会重新启动网卡硬中断，等待下一次重新的流程。 poll 函数将读取的每个 packet，转换为 sk_buff 数据格式，并分析其传输层协议。 （可选）如果开启了 GRO，那么进行 GRO 的处理。 如果开启了 RPS，那么进行 RPS 的处理，否则放入当前 CPU 的 input_pkt_queue。 RPS 处理的流程如下： 当前 CPU 将 sk_buff 放到其他 CPU 的 input_pkt_queue 中。如果 input_pkt_queue 满的话，packet 会被丢弃。 CPU 通过 IPI 硬中断通知其他 CPU 处理自己的 input_pkt_queue，也就是走 11 步流程。 到这里，而无论是否开启 RPS，接下来就是 CPU 从各自 input_pkt_queue 取出 sk_buff 并处理。 CPU 查看 socket 是否是 AF_PACKET 类型的。如果是的话复制一份数据处理（例如 tcpdump 抓这里的包）。 调用对应协议栈的函数，将数据包解析出网络层协议，并交给对应的协议栈处理。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:4","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"3.5 发送数据 接受到网络层的数据后，来到网络访问层会经过一个非常重要的系统：Traffic Controller。这是接收数据时不会经过。 根据 sk_buff 中的设备信息，获取对应 net_device.qdisc。 如果 qdisc 存在的话，走流量控制系统，可能会丢弃包： TODO 拷贝一份 sk_buff 给 “packet taps”。 调用具体驱动的发送数据的函数发送。 Note 注意，发送数据时并没有 CPU 的 “outputqueue”，因为流量控制系统已经进行了流量控制。 入口流量控制 如果想对入口流量进行流量控制，可以使用 tc ingressqdisc + 虚拟设备 IFB 进行 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:3:5","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4 网络层 网络访问层在接受到数据后，调用各个网络层协议的处理函数，进行 sk_buff 的处理。当然，我们下面说的是 IP 协议。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.1 sk_buff 网卡接受到的数据包，整个在内核中传递使用的都是 sk_buff 数据结构。网络的各个层都是使用的同一个 sk_buff 对象，而无需进行数据的复制，使得性能更高。 sk_buff 包含各个指针执行对应数据的内存区域，并且表示其协议的 head data 等区域。 // \u003csk_buff.h\u003e struct sk_buff { // ... union { __be16 inner_protocol; __u8 inner_ipproto; }; __u16 inner_transport_header; __u16 inner_network_header; __u16 inner_mac_header; __be16 protocol; __u16 transport_header; __u16 network_header; __u16 mac_header; /* These elements must be at the end, see alloc_skb() for details. */ sk_buff_data_t tail; sk_buff_data_t end; unsigned char *head, *data; }; head，end 为整个数据包的头尾内存地址； data，tail 为当前层对应的数据的头尾内存地址； transport_header，network_header，mac_header 传输层、网络层、链路层 header 的内存地址； 看图可能更好理解，sk_buff 通过指针将数据包各个区域表示出来了，而在各个协议层之间移动则是移动 data 与 tail 指针。 sk_buff_head 表示 sk_buff 组成的链表，这个结构就是 RPS 各个 CPU 的队列，以及 socket buffer 的实现。 // \u003csk_buff.h\u003e struct sk_buff_head { /* These two members must be first. */ struct sk_buff *next; struct sk_buff *prev; __u32 qlen; // 链表长度 spinlock_t lock; }; ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.2 netfilter netfilter 是一个在内核框架，位于网络层，可以根据动态的条件过滤或操作分组。 主要包含如下功能： filter：根据分组元信息，对不同数据流进行分组过滤； NAT：根据规则来转换 source ip 或者 destination ip； mangle: 根据特定分组拆分与修改； iptables iptables 是用于提供给用户配置防火墙、分组过滤等功能，使用的就是 netfilter 框架。 针对不同的阶段，内核代码中会存在不同的 hook 点，如下图： ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:2","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.3 接收数据 接受数据到这里，数据包的网络层协议已经解析过了，看一下网络层处理数据报的步骤： 如果其数据包的 MAC 地址不是当前网卡，那么丢弃（可能由于网卡混杂模式进来的，还是无法经过协议栈处理）。 经过 netfilter.PREROUTING 阶段回调。 进行路由判断：如果目的 IP 是本机 IP，那么接受该包。如果不是本机 IP，判断是否要路由。 经过 netfilter.LOCALIN 阶段回调。 进入传输层。 可以看到，如果仅仅是简单的接收数据包很简单，只要经过 netfilter 的回调即可。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:3","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.4 路由数据 当接受数据时发现数据包不是发往本机时，就会判断是否需要进行路由。 路由的前提是：机器开启了 ip forward 功能，否则会直接丢包。 # 开启 ip forward sysctl -w net.ipv4.ip_forward=1 看一下路由对数据包的处理： 检查是否开启 ip forward 功能，没有开启的话数据包会执行丢弃。 经过 netfilter.FORWARD 阶段回调。 走正常的发包流程发送数据包（会经过 netfilter.POSTROUTING 阶段回调） ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:4","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"4.5 发送数据 在 sk_buff 指向的数据区设置好 IP 报文头。 调用 netfilter.LOCALOUT 阶段回调。 调用相关协议的发送函数（IP 协议或其他），将出口网卡设备信息写入 sk_buff。 调用 netfilter.POSTOUTPUT 阶段回调。 POSTOUTPUT 可能设置了 SNAT，从而导致路由信息变化，如果发生变化重新重新回到 3。 根据目的 IP，从路由表中获取下一跳的地址，然后在 ARP 缓存表中找到下一跳的 neigh 信息。 如果没有 neigh 信息，那么会进行一个 ARP 请求，尝试得到下一跳的 mac 地址。 到这里，得到了下一跳设备的 mac 地址，将其填入 sk_buff 并调用下一层的接口。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:4:5","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"5 传输层 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:5:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"5.1 sock sock 是 socket 在内核的表示结构，每个 sock 对应于一个用户态使用的 socket。TCP UDP 都是基于该结构来实现的。 其中，sock 最主要的属性就是常说的 socket buffer 了。 // \u003csock.h\u003e struct sock { // … #define sk_state __sk_common.skc_state // … struct sk_buff_head sk_error_queue; struct sk_buff_head sk_receive_queue; struct sk_buff_head sk_write_queue; int sk_rcvbuf; int sk_sndbuf; u32 sk_ack_backlog; u32 sk_max_ack_backlog; // 各个回调函数 void (*sk_state_change)(struct sock *sk); void (*sk_data_ready)(struct sock *sk); void (*sk_write_space)(struct sock *sk); void (*sk_error_report)(struct sock *sk); int (*sk_backlog_rcv)(struct sock *sk, struct sk_buff *skb); // … } sk_state ：TCP 的状态； sk_receive_queue sk_write_queue ：接受/发送队列（buffer）； rcvbuf，sndbuf ：接受/发送队列的大小，单位 B； sk_ack_backlog ：经过三次握手后，等待 accept() 的全连接队列； 5.1.1 配置 socket buffer 大小 通过 sysctl 可以配置 TCP、UDP 的接受与发送缓冲区大小： sysctl -w net.ipv4.tcp_rmem=\"4096 503827 6291456\" sysctl -w net.ipv4.tcp_wmem=\"4096 503827 6291456\" sysctl -w net.ipv4.udp_mem=\"377868 503827 755736\" 每个设置包含三个值，min、default、max。内核会根据当前的可用内存动态调节队列的大小； Tip 使用 SO_SNDBUF/SO_RCVBUF 可以针对 socket 单独设置 r/w buffer。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:5:1","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"5.2 UDP 层 5.2.1 接收数据 先从简单的 UDP 处理开始看： 对数据包进行一致性检查。 根据目的 IP 与目的 port，在 udptable（包含机器所有的 udp sock）查找对应的 sock。没有找到则会丢弃数据包，否则继续。 检查 receive buffer 是否满，如果满了则会直接丢弃数据包。 检查数据包是否满足 BPF socket filter，如果不满足则直接丢弃数据包。 将数据包放入 receive buffer。 调用回调函数，以通知数据包已经准备好。这会将阻塞等待数据包到来的用户态程序唤醒。 UDP 的处理很简单，找到对应的 sock 结构，然后将其放入到队列中，唤醒用户态程序。 5.2.2 发送数据 发送操作与接收不是对称的，因为在发送时就要确定出口的设备，来确认包是否会被直接丢弃。 根据机器的路由表和目的 IP，决定数据包应该从哪个设备发送出去。如果根据路由表无法到达目的地址，直接丢弃包。 如果 socket 没有绑定源 IP，那么就使用设备的源 IP。 根据获取到路由信息，将 msg 构建为 sk_buff 结构体。 向 sk_buff 的数据区填充 UDP 包头，然后调用 IP 层相关函数。 Note UDP 不存在 send buffer，数据直接会发送出去，因为 UDP 没有拥塞控制。 设置 socket SO_SNDBUF 选项时，对于 UDP 这是每次发送数据的最大值，超过的话发送会直接返回 ENOBUFS。 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:5:2","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"总结 整个内核网络收发是个很复杂的模块，可能好多地方的细节都没有涉及，也有可能有理解错误的地方。 所幸内核实现层次分明的比较清楚，因此可以一层层观察其对应负责的行为。 在一个普通程序员的角度下，首先需要理解网络包收发涉及到的各个层的作用：网卡层、网络访问层、网络层、传输层。 对于各个层，需要知道一些包处理的关键点的所处于的位置，包括：网卡多队列、流量控制、netfilter、r/w buffer 等； 目前整理的主要的点如下： 网卡层 网卡多队列的概念与位置； CPU 如何接受网卡数据； CPU 如何发送网卡数据； 网络访问层 网络访问层接受数据； 网络访问层发送数据； ksoftiqrd 线程的任务； RPS、XPS、RFS 的概念； TC 流量控制处理的位置； XDP 处理的位置； 网络层 sk_buff 结构的概念； 网络层接受数据； 网络层发送数据； netfilter 各个阶段回调的位置； 如何进行路由判断； 传输层 sock 结构的概念； UDP 的 recv socket buffer； TCP 的半连接队列，全连接队列，r/w socket buffer； ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:6:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["linux"],"content":"参考 Blog: Linux 网络 - 数据包的接收过程 Blog: Linux 网络 - 数据包的发送过程 《深入 Linux 内核架构》 ","date":"2021-02-04","objectID":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/:7:0","tags":["linux","网络"],"title":"Linux 网络收发包过程总结","uri":"/posts/linux/net/linux-%E7%BD%91%E7%BB%9C%E6%94%B6%E5%8F%91%E5%8C%85%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"Go 协程实现，GMP 模型实现，调度算法实现","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Golang 并发调度实现的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 下面代码都是基于 go 1.15.6。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:0:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1 背景知识 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:1:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.1 进程、线程与协程 看协程的实现之前，绕不开的需要知道进程与线程，以及它们之间的比较。 进程progress 就是程序执行的实例。在内核角度上看，进程是分配系统资源的实体。 而这也可以推出，所谓的进程切换的消耗，也就是切换分配的系统资源，包括虚拟内存（页表等）、寄存器的值等。 Tip 据说每次上下文切换需要几十纳秒到微秒的 CPU 时间。 线程thread 是内核调度的基本单元，在 Linux 上，线程就是 “轻量级进程”，因为它与进程在内核的看来都一样，仅仅是共享了一些资源，包括： 内存地址空间； 进程的基础信息； 打开的文件描述符； 信号处理； 等等 所以相对于进程间切换，同一个进程下的线程切换，可以省略这些共享的资源的切换。因此，线程间切换速度快于进程间切换。 Note 所谓进程切换就是不同进程间的线程切换，所以下面所说的线程间的切换、线程上下文都是指同进程下的线程。 上面所说的调度、切换都是站在内核的角度看线程。而站在线程的角度，它可以认为自己是 “完全” 占用 CPU 的。如果线程阻塞等待，就等于 CPU 在 空闲 浪费。 所以，写代码往往会使用异步 API，通过回调/通知来使得线程阻塞的更加少（典型的 epoll），这时候 CPU 原来阻塞的时间可以执行其他的代码。 但是，回调的代码不是同一个函数里线性执行的，会有一些缺点，具体例子，A 函数最初的执行流为： 执行 -\u003e 读取数据 -\u003e 执行 如果使用异步 API，其执行流程变为： A 函数：执行 -\u003e 异步 -\u003e 返回 回调函数：读取数据 -\u003e 执行。 首先，单个函数的执行流变为了两个不同函数，代码写法上串行的思维要变为分割的思维。并且，如果这个操作还是有状态的，那么还涉及到了 “A 函数将状态传递到回调函数” 等问题。 因此，协程routine 出现，上面的例子的执行流还是： 执行 -\u003e 读取数据 -\u003e 执行 但是，在读取数据这一步，协程会主动让出 CPU，等待数据到来时再次切换到该协程，继续执行。因此，写法不变，功能相同。 并且，在一些用户空间的生产消费模型实现上（channel），协程的阻塞不需要线程阻塞，而在用户空间就完成了协程的切换。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:1:1","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.2 调度器 调度器scheduler 是为了在决定在有限的 CPU 上，选择哪些任务（进程/线程/协程）执行使得系统运行更高效，所以主要有两个工作： 决定为各个任务决定其运行多长时间，以及何时切换到下个任务执行； 在切换任务 A 至任务 B 时，必须保存任务 A 的运行环境，并且任务 B 的运行环境与上次其被切换时完全相同； 第 1 个工作涉及到的就是 调度算法，如何调度使得系统运行的最高效。 第 2 个工作涉及到的就是 上下文切换，这里再说一下进程、线程、协程的上下文： 进程：虚拟内存、寄存器的值、内核对应进程信息等；（内核完成上下文切换） 线程：寄存器的值、内核对应的进程信息等；（内核完成上下文切换） 协程：寄存器的值、用户空间对应的协程信息等；（用户空闲 runtime 完成协程上下文切换） Note 上面的切换没有说栈，因为栈的切换就是寄存器的切换。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:1:2","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.3 PC 与 SP 这一块在 内存管理总结 说过，因为对协程切换很重要，这个再次复制一下。 1.3.1 PC 程序计数器 PCProgram Counter 是 CPU 中的一个寄存器，保存着下一个 CPU 执行的指令的位置。顺序执行指令时，PC = PC + 1（一个指令）。而调用函数或者条件跳转时，会将跳到的指令地址设置到 PC 中。 所以，可以想到，当需要切换执行的 goroutine，调用 JMP 指令跳转到 G 对应的代码。 1.3.2 SP 栈顶指针 SPstack pointer 是保存栈顶地址的寄存器，我们平时所说的临时变量在栈上，就是将临时变量的值写入 SP 保存的内存地址，然后 SP 保存的地址减小（栈是从高地址向低地址变化），然后临时变量销毁时，SP 地址又变为高地址。 不过，因为 goroutine 切换时，必须要保存当前 goroutine 的上下文，也就是栈里的变量。因此，goroutine 栈肯定是不能使用 Linux 进程栈了（因为进程栈有上限，也无法实现“保存”这种功能）。所以所说的协程栈，都是基于 mmap 申请内存空间（基于 Go 内存管理，内存管理基于 mmap），然后切换时修改 SP 寄存器地址实现的。 这也是为什么 goroutine 栈可以“无限大”的原因了。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:1:3","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2 GMP 模型 先从整体模型入手，整个协程实现有着三个最主要的对象： G ：表示 Goroutine，保存并发任务的状态（上下文）； M ：表示系统线程，必须在绑定 P 后，执行 G 任务； P ：处理器，作用类似于 CPU 核，控制并发执行任务数量； schedt ：全局的链表，包括全局的 G 可运行链表、空闲 M 链表、空闲 P 链表； 而大致的运行的模型如下： 每个 P 绑定一个 M，与一个正在运行的 G； 每个 P 包含自己本地的可运行的 G 的链表； 全局的 G 的可运行链表，用以提供给无 G 可以执行的 P； 一些 M 与 G 的绑定，因为系统调用阻塞而解绑了 P，M 阻塞结束后还是会进入调度循环； 一些 G 主动进入 waiting 状态，等待唤醒后重新加入某个可运行队列（典型的，读取 channel 阻塞，这是一个用户空间的阻塞，由发送 channel 时将其唤醒）； 空闲 M 链表，空闲 P 链表，空闲 G 链表等； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.1 G g 结构（runtime/runtime2.go）代表一个 G，包含了某个任务的上下文，M 切换 G 执行时，当前 G 的上下文就保存在了 g 结构中。 type g struct { // Stack parameters.  // stack describes the actual stack memory: [stack.lo, stack.hi).  // stackguard0 is the stack pointer compared in the Go stack growth prologue.  // It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.  // stackguard1 is the stack pointer compared in the C stack growth prologue.  // It is stack.lo+StackGuard on g0 and gsignal stacks.  // It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).  stack stack // offset known to runtime/cgo  stackguard0 uintptr // offset known to liblink  stackguard1 uintptr // offset known to liblink _panic *_panic // innermost panic - offset known to liblink  _defer *_defer // innermost defer  m *m // current m; offset known to arm liblink  sched gobuf atomicstatus uint32 goid int64 preempt bool // preemption signal, duplicates stackguard0 = stackpreempt  preemptStop bool // transition to _Gpreempted on preemption; otherwise, just deschedule  preemptShrink bool // shrink stack at synchronous safe point lockedm muintptr … } stack：G 对应栈空间的地址，见（内存管理模型-Goroutine 的栈）； stackguar0：扩容栈的地址，也可以用于判断 G 是否应该被抢占；当 stackguard0 == stackpreempt 就表明当前 G 被抢占了； _panic：G 下的 panic 链表； _defer：G 下的所有 defer 组成的链表； m：当前绑定的 M，为 nil 就表示当前 G 没有在执行； sched：G 的部分上下文，会提供给汇编代码； atomicstatus：G 的状态； goid：G 的唯一 ID，但是用户代码无法读取； preempt：抢占标志； lockedm：记录 G 被锁定的 M，实现 runtime.LockOSThread() 其中 g.sched 是一个非常重要的结构，需要看一下其 gobuf 的实现（runtime/runtime2.go）： type gobuf struct { // The offsets of sp, pc, and g are known to (hard-coded in) libmach.  //  // ctxt is unusual with respect to GC: it may be a  // heap-allocated funcval, so GC needs to track it, but it  // needs to be set and cleared from assembly, where it's  // difficult to have write barriers. However, ctxt is really a  // saved, live register, and we only ever exchange it between  // the real register and the gobuf. Hence, we treat it as a  // root during stack scanning, which means assembly that saves  // and restores it doesn't need write barriers. It's still  // typed as a pointer so that any other writes from Go get  // write barriers.  sp uintptr pc uintptr g guintptr ctxt unsafe.Pointer ret sys.Uintreg … } sp：当前 G 的 SP 指针地址，在创建 G 时默认设置为 goexit 函数地址； pc：当前 G 的 PC 指针地址； g：gobuf 所属的 g 结构地址； ret：系统调用的返回值； 2.1.1 G 的状态 目前 G 可能处于以下 9 种状态： 状态 值 含义 _Gidle 0 G 刚分配，并且还没有被初始化 _Grunnable 1 G 处于可运行队列中，但是并没有被执行 _Grunning 2 G 绑定了 M、P，不在可运行队列，并且可能正在执行 _Gsyscall 3 G 正在执行系统调用而阻塞，不在可运行队列上，绑定了 M，没有绑定 P _Gwaiting 4 G 因为用户空间而阻塞，不在可运行队列，等待其他 G 唤醒，没有绑定 M、P _Gdead 5 G 当前没有被使用，其 g 结构可以被复用 _Gcopystack 6 G 的栈正在被拷贝，没有执行，不再可运行队列 _Gpreempted 7 由于抢占而被阻塞，没有执行，等待唤醒 _Gscan 8 GC 正在扫描 G 的栈，可与其他状态同时存在 其状态轮转如下图所示： 2.1.2 G 的创建 在代码中调用 go 语句时，编译器会将其翻译为 newproc() 调用，这也就是创建 G 的开端： func newproc(siz int32, fn *funcval) { argp := add(unsafe.Pointer(\u0026fn), sys.PtrSize) gp := getg() pc := getcallerpc() systemstack(func() { // 创建 g 结构  newg := newproc1(fn, argp, siz, gp, pc) // 放入当前 P 或者全局可运行队列  _p_ := getg().m.p.ptr() runqput(_p_, newg, true) if mainStarted { wakep() } }) } // Create a new g in state _Grunnable, starting at fn, with narg bytes // of arguments starting at argp. callerpc is the address of the go // statement that created this. The caller is responsible for adding // the new g to the scheduler. // // This must run on the system stack because it's the continuation of // newproc, which cannot split the stack. // //go:systemstack func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g { // 调用 go 语句的 G  _g_ := getg() siz := narg siz = (siz + 7) \u0026^ 7 // 从 P.gFree 获取一个可复用的 g 对象  _p_ := _g_.m.p.ptr() newg := gfget(_p_) // 没有可复用的，重新分配一个 g 对象  if newg == nil { newg = malg(_StackMin) casgstatus(newg, _Gidle, _Gdead) allgadd(newg) // publishes with a g-\u003estatus of Gdead so GC scanner doesn't look at uninitialized stack.  } totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:1","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.2 M M 代表的就是一个操作系统线程，由 m 结构 表示（runtime/runtime2.go）： type m struct { g0 *g // goroutine with scheduling stack  gsignal *g // signal-handling g  goSigStack gsignalStack // Go-allocated signal handling stack  curg *g // current running goroutine  p puintptr // attached p for executing go code (nil if not executing go code)  nextp puintptr oldp puintptr // the p that was attached before executing a syscall  id int64 lockedg guintptr lockedExt uint32 // tracking for external LockOSThread  lockedInt uint32 // tracking for internal lockOSThread } g0 : M 私有的 g0； gsignal ：用于处理操作系统信号的 G； curg ：当前 M 正在执行的 G； p : 当前 M 绑定的 P； nextp ：暂存的 nextp； oldp ：M 陷入系统调用前绑定的 P，用于系统调用结束后尝试重新绑定； id ：m 的 ID； lockedg ：保存 M 锁定的 G，实现 runtime.LockOSThread()； 2.2.1 M 的创建 在创建 G 或者其他地方，当 G 变为 runnable 后，就会调用 wakep() 触发一次 P 执行 G 的过程。其会调用 startm() 选择/创建 一个 M 绑定 P，并执行一个 G。 // Tries to add one more P to execute G's. // Called when a G is made runnable (newproc, ready). func wakep() { if atomic.Load(\u0026sched.npidle) == 0 { return } startm(nil, true) } // Schedules some M to run the p (creates an M if necessary). // If p==nil, tries to get an idle P, if no idle P's does nothing. // May run with m.p==nil, so write barriers are not allowed. // If spinning is set, the caller has incremented nmspinning and startm will // either decrement nmspinning or set m.spinning in the newly started M. //go:nowritebarrierrec func startm(_p_ *p, spinning bool) { // 选择一个空闲的 P  lock(\u0026sched.lock) if _p_ == nil { _p_ = pidleget() if _p_ == nil { unlock(\u0026sched.lock) return } } // 选择一个空闲的 M  mp := mget() if mp == nil { // No M is available, we must drop sched.lock and call newm.  // However, we already own a P to assign to the M.  //  // Once sched.lock is released, another G (e.g., in a syscall),  // could find no idle P while checkdead finds a runnable G but  // no running M's because this new M hasn't started yet, thus  // throwing in an apparent deadlock.  //  // Avoid this situation by pre-allocating the ID for the new M,  // thus marking it as 'running' before we drop sched.lock. This  // new M will eventually run the scheduler to execute any  // queued G's.  id := mReserveID() unlock(\u0026sched.lock) var fn func() if spinning { // The caller incremented nmspinning, so set m.spinning in the new M.  fn = mspinning } // 创建一个 M  newm(fn, _p_, id) return } unlock(\u0026sched.lock) // The caller incremented nmspinning, so set m.spinning in the new M.  mp.spinning = spinning mp.nextp.set(_p_) notewakeup(\u0026mp.park) } 当存在空闲的 P，但是没有空闲的 M 时，就会调用 newm() 创建一个 M； newm() 就是创建 m 结构，以及启动系统线程的地方（runtime/proc.go）： // Create a new m. It will start off with a call to fn, or else the scheduler. // fn needs to be static and not a heap allocated closure. // May run with m.p==nil, so write barriers are not allowed. // // id is optional pre-allocated m ID. Omit by passing -1. //go:nowritebarrierrec func newm(fn func(), _p_ *p, id int64) { // 分配 m 对象（复用 sched.freem 或者新创建）  mp := allocm(_p_, fn, id) mp.nextp.set(_p_) mp.sigmask = initSigmask … newm1(mp) } func newm1(mp *m) { … execLock.rlock() // Prevent process clone.  newosproc(mp) execLock.runlock() } // May run with m.p==nil, so write barriers are not allowed. //go:nowritebarrier func newosproc(mp *m) { stk := unsafe.Pointer(mp.g0.stack.hi) // 给 thread 初始的栈来自于 g0  // Disable signals during clone, so that the new thread starts  // with signals disabled. It will enable them in minit.  var oset sigset sigprocmask(_SIG_SETMASK, \u0026sigset_all, \u0026oset) ret := clone(cloneFlags, stk, unsafe.Pointer(mp), unsafe.Pointer(mp.g0), unsafe.Pointer(funcPC(mstart))) sigprocmask(_SIG_SETMASK, \u0026oset, nil) if ret \u003c 0 { throw(\"newosproc\") } } 获取 m 对象，来自于 sched.freem 或者新创建一个； 通过 clone() 系统调用创建一个系统线程，执行的函数为 mstart()； 看一下 clone 的 flags： var ( cloneFlags = _CLONE_VM | /* share memory */ _CLONE_FS | /* share cwd, etc */ _CLONE_FILES | /* share fd table */ _CLONE_SIGHAND | /* share sig handler table */ _CLONE_SYSVSEM | /* share SysV semaphore undo lists (see issue #20763) */ _CLONE_THREAD /* revis","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:2","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.3 P P 是 M 与 G 的中间层，没有了 P，M 与 G 实际上就是一个线程池。通过 P 来分片所有可运行的 G，使得运行效率更加的高。 p 是 P 的结构体表示（runtime/runtime2.go）： type p struct { id int32 status uint32 // one of pidle/prunning/...  m muintptr // back-link to associated m (nil if idle)  mcache *mcache pcache pageCache // Queue of runnable goroutines. Accessed without lock.  runqhead uint32 runqtail uint32 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready'd by  // the current G and should be run next instead of what's in  // runq if there's time remaining in the running G's time  // slice. It will inherit the time left in the current time  // slice. If a set of goroutines is locked in a  // communicate-and-wait pattern, this schedules that set as a  // unit and eliminates the (potentially large) scheduling  // latency that otherwise arises from adding the ready'd  // goroutines to the end of the run queue.  runnext guintptr // Available G's (status == Gdead)  gFree struct { gList n int32 } sudogcache []*sudog sudogbuf [128]*sudog } status ：P 的状态； m ：当前绑定的 M； mcache ：P 唯一的 mcache，将【内存管理】； runqhead ：P 的可运行队列的 head index； runqtail ：P 的可以行队列的 tail index； runq ：P 的可运行队列，可以看到大小为 256； runnext ：下一次优先执行的 G，优先级高于 runq； gFree ：可复用的 g 结构链表； P 还包含大量与 GC 内存管理相关的字段，这里暂时省略。 2.3.1 P 的状态 状态 值 含义 _Pidle 0 P 没有任何 G 可以执行，被空闲 P 链表持有着 _Prunning 1 P 绑定了一个 M，并且正在执行 G _Psyscall 2 P 绑定了 M，但是 M 陷入系统调用阻塞，P 可以被其他 M 获取 _Pgcstop 3 P 绑定着 M，但是因为 STW 挂起 _Pdead 4 P 不在被使用，由于 GOMAXPROCS 缩小 可以看到，_Pidle 与 _Psyscall 都属于 P 可以被其他 M 绑定的状态。 2.3.2 P 的创建/销毁 前面可以看到，G 是由 go 命令创建的，而 M 是按需创建的。P 的创建不同，因为其代表的是并发个数，所以其创建是在程序启动时创建。 在执行用户 main 函数之间的 scheinit() 中进行 runtime 的初始化，其中一项就是初始化 P: // The bootstrap sequence is: // // call osinit // call schedinit // make \u0026 queue new G // call runtime·mstart // // The new G calls runtime·main. func schedinit() { // … procs := ncpu if n, ok := atoi32(gogetenv(\"GOMAXPROCS\")); ok \u0026\u0026 n \u003e 0 { procs = n } if procresize(procs) != nil { throw(\"unknown runnable goroutine during bootstrap\") } // … } procresize() 用于改变 P 的数量（runtime/proc.go）： // Change number of processors. The world is stopped, sched is locked. // gcworkbufs are not being modified by either the GC or // the write barrier code. // Returns list of Ps with local work, they need to be scheduled by the caller. func procresize(nprocs int32) *p { old := gomaxprocs // 扩大 P 的数量  // Grow allp if necessary.  if nprocs \u003e int32(len(allp)) { // Synchronize with retake, which could be running  // concurrently since it doesn't run on a P.  lock(\u0026allpLock) if nprocs \u003c= int32(cap(allp)) { allp = allp[:nprocs] } else { nallp := make([]*p, nprocs) // Copy everything up to allp's cap so we  // never lose old allocated Ps.  copy(nallp, allp[:cap(allp)]) allp = nallp } unlock(\u0026allpLock) } // 初始化新的 P  // initialize new P's  for i := old; i \u003c nprocs; i++ { pp := allp[i] if pp == nil { pp = new(p) } pp.init(i) atomicstorep(unsafe.Pointer(\u0026allp[i]), unsafe.Pointer(pp)) } // 如果当前 M 绑定的 P 是要被释放的，那么 M 新选取一个可用的 P  _g_ := getg() if _g_.m.p != 0 \u0026\u0026 _g_.m.p.ptr().id \u003c nprocs { // continue to use the current P  _g_.m.p.ptr().status = _Prunning _g_.m.p.ptr().mcache.prepareForSweep() } else { // release the current P and acquire allp[0].  //  // We must do this before destroying our current P  // because p.destroy itself has write barriers, so we  // need to do that from a valid P.  if _g_.m.p != 0 { _g_.m.p.ptr().m = 0 } _g_.m.p = 0 p := allp[0] p.m = 0 p.status = _Pidle acquirep(p) if trace.enabled { traceGoStart() } } // g.m.p is now set, so we no longer need mcache0 for bootstrapping.  mcache0 = nil // 清理不使用的 P  // release resources from unused P's  for i := nprocs; i \u003c old; i++ { p := allp[i] p.destroy() // can't free P itself because it can be referenced by an M in syscall  } // Trim allp.  if int32(len(allp)) != nprocs { lock(\u0026allpLock) allp = allp[:nprocs] unlock(\u0026allpLock) } var runnablePs *p for i := nprocs - 1; i \u003e= 0; i-- { p := allp[i] if _g_.m.p.ptr() == p { continue } p.status = _Pidle if runqempty(p) { pidleput(p) } else { p.m.set(mget())","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:3","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.4 schedt schedt 是一个单例全局变量，包含一些全局的链表（runtime/runtime2.go）： var sched schedt type schedt struct { // When increasing nmidle, nmidlelocked, nmsys, or nmfreed, be  // sure to call checkdead(). midle muintptr // idle m's waiting for work  nmidle int32 // number of idle m's waiting for work pidle puintptr // idle p's  npidle uint32 // Global runnable queue.  runq gQueue runqsize int32 // disable controls selective disabling of the scheduler.  //  // Use schedEnableUser to control this.  //  // disable is protected by sched.lock.  disable struct { // user disables scheduling of user goroutines.  user bool runnable gQueue // pending runnable Gs  n int32 // length of runnable  } // Global cache of dead G's.  gFree struct { lock mutex stack gList // Gs with stacks  noStack gList // Gs without stacks  n int32 } // Central cache of sudog structs.  sudoglock mutex sudogcache *sudog // Central pool of available defer structs of different sizes.  deferlock mutex deferpool [5]*_defer // freem is the list of m's waiting to be freed when their  // m.exited is set. Linked through m.freelink.  freem *m } midle ：空闲的 M 的链表； pidle ：空闲的 P 的链表； runq ：全局的可运行的 G 队列； gFree ：全局的可复用的 g 结构队列； freem ：等待释放的 M 的链表，当 M 新建时会将其释放，并复用 m 对象； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:2:4","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3 调度循环 前面 M 的启动看到，每个 M 创建后会进入 schedule() 的调度循环，并且不会返回，每个 M 执行大致流程如下： 执行 schedule()， 使得 M 找到一个可用的 G，并绑定； 执行 execute()，完成执行 G.fn 的准备工作； 调用 G 的函数； G 函数调用退出后，调用 goexit() 函数清理相关资源，并重新进入 schedule()； 当然，上面是正常 G 执行并退出的逻辑，多数情况下 G 执行的过程中都会经历抢占与调度，也就是说会 M 可能会切换 P、切换 G 执行。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:3:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.1 schedule() schedule() 是调度循环的第一步，M 会在这里尽力寻找一个 runnable G，然后进入 execute() 执行 G 的代码： // One round of scheduler: find a runnable goroutine and execute it. // Never returns. func schedule() { _g_ := getg() // 锁定了 G, 那么还是执行锁定的 G  if _g_.m.lockedg != 0 { stoplockedm() execute(_g_.m.lockedg.ptr(), false) // Never returns.  } top: pp := _g_.m.p.ptr() pp.preempt = false // STW 中，等待结束  if sched.gcwaiting != 0 { gcstopm() goto top } // 运行 timer  checkTimers(pp, 0) // gp 为新调度到的 G  var gp *g var inheritTime bool // Normal goroutines will check for need to wakeP in ready,  // but GCworkers and tracereaders will not, so the check must  // be done here instead.  tryWakeP := false if trace.enabled || trace.shutdown { gp = traceReader() if gp != nil { casgstatus(gp, _Gwaiting, _Grunnable) traceGoUnpark(gp, 0) tryWakeP = true } } // GC 扫描开始工作，尝试 GCWorker 的 G  if gp == nil \u0026\u0026 gcBlackenEnabled != 0 { gp = gcController.findRunnableGCWorker(_g_.m.p.ptr()) tryWakeP = tryWakeP || gp != nil } // 定期直接从 全局可运行队列 获取 G，防止饥饿  if gp == nil { // Check the global runnable queue once in a while to ensure fairness.  // Otherwise two goroutines can completely occupy the local runqueue  // by constantly respawning each other.  if _g_.m.p.ptr().schedtick%61 == 0 \u0026\u0026 sched.runqsize \u003e 0 { lock(\u0026sched.lock) gp = globrunqget(_g_.m.p.ptr(), 1) unlock(\u0026sched.lock) } } // 从当前 P 的 可运行队列 获取一个 G  if gp == nil { gp, inheritTime = runqget(_g_.m.p.ptr()) // We can see gp != nil here even if the M is spinning,  // if checkTimers added a local goroutine via goready.  } // 上面都尝试了，尽可能去找到一个 G，这里会阻塞  if gp == nil { gp, inheritTime = findrunnable() // blocks until work is available  } // This thread is going to run a goroutine and is not spinning anymore,  // so if it was marked as spinning we need to reset it now and potentially  // start a new spinning M.  if _g_.m.spinning { resetspinning() } // 帮忙唤醒别的 P  // If about to schedule a not-normal goroutine (a GCworker or tracereader),  // wake a P if there is one.  if tryWakeP { wakep() } // 如果选出的 G 是被别的 M 锁定的，那么只能重新走流程  if gp.lockedm != 0 { // Hands off own p to the locked m,  // then blocks waiting for a new p.  startlockedm(gp) goto top } // 执行新的 G  execute(gp, inheritTime) } M 有锁定的 G，执行锁定 G 代码； 从各个地方得到一个 runnable G，包括： GCWork 的 G，见： 定期直接从 全局可运行的队列 获取 G，防止全局队列的 G 长时间饥饿； 从绑定的 P 的 可运行队列 获取 G； 通过 fundrunnable() 尽可能获取一个 G； 最终，获取到一个 G 后，execute() 执行 G 的代码； 3.1.1 fundrunnable() 为了找到可以运行的 G，findrunnable() 会尝试各个手段。但是这个代码比较复杂，这里捡最关键的点看（runtime/proc.go）： // Finds a runnable goroutine to execute. // Tries to steal from other P's, get g from local or global queue, poll network. func findrunnable() (gp *g, inheritTime bool) { _g_ := getg() top: // 正在垃圾回收 STW，休眠轮询  _p_ := _g_.m.p.ptr() if sched.gcwaiting != 0 { gcstopm() goto top } now, pollUntil, _ := checkTimers(_p_, 0) if fingwait \u0026\u0026 fingwake { if gp := wakefing(); gp != nil { ready(gp, 0, true) } } // 从绑定的 P 队列获取  if gp, inheritTime := runqget(_p_); gp != nil { return gp, inheritTime } // 从全局队列获取  if sched.runqsize != 0 { lock(\u0026sched.lock) gp := globrunqget(_p_, 0) unlock(\u0026sched.lock) if gp != nil { return gp, false } } // 检查全局的 netpoll 的 G  if netpollinited() \u0026\u0026 atomic.Load(\u0026netpollWaiters) \u003e 0 \u0026\u0026 atomic.Load64(\u0026sched.lastpoll) != 0 { if list := netpoll(0); !list.empty() { // non-blocking  gp := list.pop() injectglist(\u0026list) casgstatus(gp, _Gwaiting, _Grunnable) if trace.enabled { traceGoUnpark(gp, 0) } return gp, false } } // 从别的 P 偷取一半的任务  // Steal work from other P's.  procs := uint32(gomaxprocs) ranTimer := false for i := 0; i \u003c 4; i++ { for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() { if sched.gcwaiting != 0 { goto top } stealRunNextG := i \u003e 2 // first look for ready queues with more than 1 g  p2 := allp[enum.position()] if _p_ == p2 { continue } if gp := runqsteal(_p_, p2, stealRunNextG); gp != nil { return gp, false } // Consider stealing timers from p2. // This call to checkTimers is the only place where // we hold a lock on a different P's timers. // Lock content","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:3:1","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.2 execute() execute() 让当前线程执行 G 的代码，并且不会返回： // Schedules gp to run on the current M. // If inheritTime is true, gp inherits the remaining time in the // current time slice. Otherwise, it starts a new time slice. // Never returns. func execute(gp *g, inheritTime bool) { _g_ := getg() // Assign gp.m before entering _Grunning so running Gs have an  // M.  _g_.m.curg = gp gp.m = _g_.m casgstatus(gp, _Grunnable, _Grunning) gp.waitsince = 0 gp.preempt = false gp.stackguard0 = gp.stack.lo + _StackGuard if !inheritTime { _g_.m.p.ptr().schedtick++ } // … gogo(\u0026gp.sched) } 不过 execute() 执行的是一些初始化的操作，切换 PC、SP 等操作只能通过汇编的 gogo 实现，注意关键的 g.sched 结构（gobuf 结构）的传参。 // func gogo(buf *gobuf) // restore state from Gobuf; longjmp TEXT runtime·gogo(SB), NOSPLIT, $16-8 MOVQ buf+0(FP), BX // gobuf 内容放到 BX 寄存器  MOVQ gobuf_g(BX), DX MOVQ 0(DX), CX // make sure g != nil  get_tls(CX) MOVQ DX, g(CX) MOVQ gobuf_sp(BX), SP // 将 SP 地址设置为 gobuf.sp。第一次执行 G 时，这里是 goexit 函数地址  MOVQ gobuf_ret(BX), AX MOVQ gobuf_ctxt(BX), DX MOVQ gobuf_bp(BX), BP MOVQ $0, gobuf_sp(BX) // clear to help garbage collector  MOVQ $0, gobuf_ret(BX) MOVQ $0, gobuf_ctxt(BX) MOVQ $0, gobuf_bp(BX) MOVQ gobuf_pc(BX), BX // 将 BX 寄存器设置为 gobuf.pc。第一次执行 G 时，这里是 G 的函数地址  JMP BX // JMP gobuf.pc，开始执行 这里最关键的就是切换为 G 的执行上下文： 将 SP 设置为 gobuf.sp。如果 G 没有执行过，那么值就是创建 g 结构时填入的 goexit() 函数的地址。 代码流通过 JMP 指令跳转到 gobuf.pc。如果 G 没有执行过，那么就是 G 对应代码的地址。 这里我们也可以知道了，因为函数调用就是将 返回函数、参数 压栈的过程。而这里将栈顶设置为 goexit() 函数，所以当 G 对应用户代码执行完后，就会继续执行 goexit() 函数。这就是 M 不断执行调度循环的关键。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:3:2","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.3 goexit() goexit() 在 G 用户代码执行后，执行的汇编代码，其最后通过切换到 g0 栈调用 goexit0() 函数： // goexit continuation on g0. func goexit0(gp *g) { _g_ := getg() // G running 变为 dead  casgstatus(gp, _Grunning, _Gdead) // 重置 g 的属性  if isSystemGoroutine(gp, false) { atomic.Xadd(\u0026sched.ngsys, -1) } gp.m = nil locked := gp.lockedm != 0 gp.lockedm = 0 _g_.m.lockedg = 0 gp.preemptStop = false gp.paniconfault = false gp._defer = nil // should be true already but just in case.  gp._panic = nil // non-nil for Goexit during panic. points at stack-allocated data.  gp.writebuf = nil gp.waitreason = 0 gp.param = nil gp.labels = nil gp.timer = nil if gcBlackenEnabled != 0 \u0026\u0026 gp.gcAssistBytes \u003e 0 { // Flush assist credit to the global pool. This gives  // better information to pacing if the application is  // rapidly creating an exiting goroutines.  scanCredit := int64(gcController.assistWorkPerByte * float64(gp.gcAssistBytes)) atomic.Xaddint64(\u0026gcController.bgScanCredit, scanCredit) gp.gcAssistBytes = 0 } // 解除 M 与 G 的绑定  dropg() // 将 dead G 放到 p.gFree 或者 sched.gFree  gfput(_g_.m.p.ptr(), gp) if locked { // 如果 M 与 G 是锁定的，那么 M 线程退出  // The goroutine may have locked this thread because  // it put it in an unusual kernel state. Kill it  // rather than returning it to the thread pool. // Return to mstart, which will release the P and exit // the thread. if GOOS != \"plan9\" { // See golang.org/issue/22227. gogo(\u0026_g_.m.g0.sched) } else { // Clear lockedExt on plan9 since we may end up re-using // this thread. _g_.m.lockedExt = 0 } } // 重新进入调度循环  schedule() } G 状态由 _Grunning 变为 _Gdead； 重置 G 的属性； 通过 dropg() 解除 M 与 G 的绑定； 将 g 对象放到 p.gFree 或者 sched.gFree，以便后续创建 G 时可以复用对象； 如果 M 与 G 是锁定着的，而 G 执行完毕，让 m 回到 mstart() 函数继续执行，这样 M 线程会被销毁并退出； 重新进入 schedule() 调度循环； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:3:3","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4 调度切换 当前，调度循环中描述的情况是一个 M 执行 G 不被抢占与调度的情况。大多数情况下，当 M 执行 G.fn 的过程中就会被切换，执行其他的 G 的情况。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.1 切换时机 先大体总结一下可能的切换时机： 主动挂起 遇到 runtime 级别阻塞（例如 channel 读写阻塞） 主动调度 调用 runtime.Gosched() 主动进行调度 系统调用 系统调用结束后，M 可能进行 G 的切换； 抢占 sysmon 判断 G 运行时间大于 10ms，进行抢占； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:1","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.2 主动挂起 在 G 遇到非系统调用的阻塞前，就会调用 gopark()，将 G 由 _Grunning -\u003e _Gwaiting，而 M 解绑 G，重新进入调度循环： // Puts the current goroutine into a waiting state and calls unlockf. // If unlockf returns false, the goroutine is resumed. func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) { if reason != waitReasonSleep { checkTimeouts() // timeouts may expire while two goroutines keep the scheduler busy  } mp := acquirem() gp := mp.curg status := readgstatus(gp) mp.waitlock = lock mp.waitunlockf = unlockf gp.waitreason = reason mp.waittraceev = traceEv mp.waittraceskip = traceskip releasem(mp) // can't do anything that might move the G between Ms here.  mcall(park_m) } // park continuation on g0. func park_m(gp *g) { _g_ := getg() // 改变 G 状态  casgstatus(gp, _Grunning, _Gwaiting) // M 解绑 G  dropg() if fn := _g_.m.waitunlockf; fn != nil { ok := fn(gp, _g_.m.waitlock) _g_.m.waitunlockf = nil _g_.m.waitlock = nil if !ok { if trace.enabled { traceGoUnpark(gp, 2) } casgstatus(gp, _Gwaiting, _Grunnable) execute(gp, true) // Schedule it back, never returns.  } } // 重新进入调度循环  schedule() } Note 上面没有任何地方记录 _Gwaiting 状态的 G，Why？ 因为这时 gopark() 调用者的责任，例如 channel 读写阻塞时，会将 g 记录到 channel 时，在唤醒时将 G 重新加入到可运行队列。 当进入 _Gwaiting 状态的 G 需要恢复时，调用 goready() / goparkunlock() 函数进行恢复： func goready(gp *g, traceskip int) { systemstack(func() { ready(gp, traceskip, true) }) } // Mark gp ready to run. func ready(gp *g, traceskip int, next bool) { status := readgstatus(gp) // 获取一个 M  // Mark runnable.  _g_ := getg() mp := acquirem() // disable preemption because it can be holding p in a local var // waiting -\u003e runnable  // status is Gwaiting or Gscanwaiting, make Grunnable and put on runq  casgstatus(gp, _Gwaiting, _Grunnable) // 放置到 M 对应 P 的 runq，等待调度执行  runqput(_g_.m.p.ptr(), gp, next) wakep() releasem(mp) } 调用 gopark() 的地方有许多，列出主要的几个地方： channel 的发送/接受阻塞； select 所有 case 不满足，陷入阻塞； time.Sleep 使得 goroutine 进入阻塞； GC 工作的 gcwork 挂起等待唤醒； main goroutine 挂起并且不会被唤醒； ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:2","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.3 主动调度 标准库接口 runtime.Gosched() 可以在用户代码中使 G 主动让出 M： func Gosched() { checkTimeouts() mcall(gosched_m) } // Gosched continuation on g0. func gosched_m(gp *g) { goschedImpl(gp) } func goschedImpl(gp *g) { // running 状态变为 runnable  casgstatus(gp, _Grunning, _Grunnable) // M 与 G 解绑  dropg() // 放到 global runq  lock(\u0026sched.lock) globrunqput(gp) unlock(\u0026sched.lock) schedule() } 最后会调用 goschedImpl()，将 G 由 _Grunning -\u003e _Grunnable 状态，M 与 G 解绑，并将其放到全局运行队列。 Note 因为调用 goschedImpl() 是要切换正在运行的 G，所以放到全局队列。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:3","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.4 系统调用 Go 通过 syscall.Syscall 和 syscall.RawSyscall 来封装系统的所有系统调用。 syscall.Syscall 针对可能长时间阻塞的系统调用，例如 IO 操作。 使得在陷入系统调用之间，系统调用结束后，可以触发一些准备和情况工作。 syscall.RawSyscall 针对不太会长时间阻塞的系统调用，例如 读取时间等。 直接进行系统调用，不做其他处理。 系统调用分类 大神将各个系统调用分类了，见 这里 TEXT ·Syscall(SB),NOSPLIT,$0-56 CALL runtime·entersyscall(SB) // 调用 entersyscall()  MOVQ a1+8(FP), DI MOVQ a2+16(FP), SI MOVQ a3+24(FP), DX MOVQ trap+0(FP), AX // syscall entry  SYSCALL CMPQ AX, $0xfffffffffffff001 JLS ok MOVQ $-1, r1+32(FP) MOVQ $0, r2+40(FP) NEGQ AX MOVQ AX, err+48(FP) CALL runtime·exitsyscall(SB) // 结束调用 exitsyscall()  RET ok: MOVQ AX, r1+32(FP) MOVQ DX, r2+40(FP) MOVQ $0, err+48(FP) CALL runtime·exitsyscall(SB) // 结束调用 exitsyscall()  RET 系统调用前，执行 entersyscall()； 执行系统调用； 系统调用后，执行 exitsyscall()； 4.4.1 系统调用前的准备 entersyscall() 会调用 reentersyscall() 函数，执行进入系统调用前的准备工作： func reentersyscall(pc, sp uintptr) { _g_ := getg() _g_.m.locks++ _g_.stackguard0 = stackPreempt _g_.throwsplit = true save(pc, sp) _g_.syscallsp = sp _g_.syscallpc = pc casgstatus(_g_, _Grunning, _Gsyscall) _g_.m.syscalltick = _g_.m.p.ptr().syscalltick _g_.m.mcache = nil pp := _g_.m.p.ptr() pp.m = 0 _g_.m.oldp.set(pp) _g_.m.p = 0 atomic.Store(\u0026pp.status, _Psyscall) if sched.gcwaiting != 0 { systemstack(entersyscall_gcwait) save(pc, sp) } _g_.m.locks-- } 禁止线程上发生的抢占，防止出现内存不一致的问题； 保证当前函数不会触发栈分裂或者增长； 通过 save() 保存 PC、SP 值至 g.sched； G 状态 _Grunning -\u003e _Gsyscall； m.oldp 设置为当前 P，m.p 设置为 0，这意味着记录但是解绑当前的 P，而 P 状态为 _Psyscall； 这里比较重要的就是让 M 与 P 解绑，使得其他 M 可以获取到该 P 并执行 G。 因此，P 代表的是并发数，而不是线程数。 4.4.2 系统调用后的恢复 系统调用结束后，执行 exitsyscall() 进行恢复操作。 func exitsyscall() { _g_ := getg() // M 尝试绑定阻塞前使用的 P，或者一个新的 P  oldp := _g_.m.oldp.ptr() _g_.m.oldp = 0 if exitsyscallfast(oldp) { _g_.m.p.ptr().syscalltick++ casgstatus(_g_, _Gsyscall, _Grunning) ... return } // M 解绑 G，重新进入调用循环  mcall(exitsyscall0) _g_.m.p.ptr().syscalltick++ _g_.throwsplit = false } M 尝试获取一个空闲的 P，从两个地方： 如果 m.oldp 还是为 _Psyscall 状态，说明没被人使用，那么记录使用阻塞前的 P； 从全局空闲 P sched.pidle 链表中获取一个 P； 如果没有 P，那么确实 M 无法执行当前 G，就 M 解绑 G，将 G 放入全局队列； 无论如何，最后 M 都会调用 schedule() 重启进入调度循环，切换一个 G 执行。 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:4","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.5 抢占 每个运行中的 G 会有一个运行的时间片，而 sysmon 会周期性检查部分 G，如果其执行时间大于 10ms，就会触发抢占。 抢占包括两种方式： 抢占标志：通过设置 g.stackguard0=stackPreempt。这必然会导致 G 执行下次函数调用时触发栈扩容逻辑，从而走到切换调度的逻辑； 信号抢占（v1.14）：信号抢占会使得 M 线程触发信号中断，执行信号处理函数，从而重新进入调度循环。 4.5.1 触发抢占时机 sysmon 会重启执行 retake() 函数，判断哪些正在运行的 G 是需要抢占的。 func retake(now int64) uint32 { n := 0 // Prevent allp slice changes. This lock will be completely  // uncontended unless we're already stopping the world.  lock(\u0026allpLock) // 遍历所有 P  for i := 0; i \u003c len(allp); i++ { _p_ := allp[i] pd := \u0026_p_.sysmontick s := _p_.status sysretake := false // 允许抢占 _Prunning 与 _Psyscall 状态的 P  if s == _Prunning || s == _Psyscall { t := int64(_p_.schedtick) if int64(pd.schedtick) != t { pd.schedtick = uint32(t) pd.schedwhen = now } else if pd.schedwhen+forcePreemptNS \u003c= now { // 到达抢占时间  preemptone(_p_) // In case of syscall, preemptone() doesn't  // work, because there is no M wired to P.  sysretake = true } } // 对于 _Psyscall，可以让其与 M 解绑，等其他的 M 绑定  if s == _Psyscall { t := int64(_p_.syscalltick) if !sysretake \u0026\u0026 int64(pd.syscalltick) != t { pd.syscalltick = uint32(t) pd.syscallwhen = now continue } if runqempty(_p_) \u0026\u0026 atomic.Load(\u0026sched.nmspinning)+atomic.Load(\u0026sched.npidle) \u003e 0 \u0026\u0026 pd.syscallwhen+10*1000*1000 \u003e now { continue } unlock(\u0026allpLock) incidlelocked(-1) if atomic.Cas(\u0026_p_.status, s, _Pidle) { n++ _p_.syscalltick++ handoffp(_p_) } incidlelocked(1) lock(\u0026allpLock) } } unlock(\u0026allpLock) return uint32(n) } 运行中的 G 运行超过 10ms，调用 preemptone() 进行抢占； 对于 _Psyscall 中的 P，将其解绑 M，使得其他 M 可以绑定该 P； 4.5.2 触发抢占 preemptone() 触发抢占，通过上面所述的两种方式。 func preemptone(_p_ *p) bool { mp := _p_.m.ptr() gp := mp.curg gp.preempt = true // 设置抢占标志  // Every call in a go routine checks for stack overflow by  // comparing the current stack pointer to gp-\u003estackguard0.  // Setting gp-\u003estackguard0 to StackPreempt folds  // preemption into the normal stack overflow check.  gp.stackguard0 = stackPreempt // 信号抢占  // Request an async preemption of this P.  if preemptMSupported \u0026\u0026 debug.asyncpreemptoff == 0 { _p_.preempt = true preemptM(mp) } return true } 设置 G 的抢占标志，gp.stackguard0 = stackPreempt； 执行 preemptM() 进行信号抢占； 4.5.3 通过抢占标志抢占 前面看到，设置 gp.stackguard0 = stackPreempt，而这在每次 G 函数调用前的检查是否扩容栈时，必然会触发 G 栈扩容逻辑 newstack()。 而在 内存管理 时，介绍了 newstack() 如果扩容 G 的栈，但是省略了一个重要的逻辑分支：newstack() 函数中还会进行 G 的调度： func newstack() { thisg := getg() gp := thisg.m.curg // …  preempt := atomic.Loaduintptr(\u0026gp.stackguard0) == stackPreempt // 特殊情况下不能抢占时，继续走 G 代码逻辑  if preempt { if !canPreemptM(thisg.m) { // Let the goroutine keep running for now.  // gp-\u003epreempt is set, so it will be preempted next time.  gp.stackguard0 = gp.stack.lo + _StackGuard gogo(\u0026gp.sched) // never return  } // …  if preempt { if gp == thisg.m.g0 { throw(\"runtime: preempt g0\") } if thisg.m.p == 0 \u0026\u0026 thisg.m.locks == 0 { throw(\"runtime: g is running but p is not\") } if gp.preemptShrink { // We're at a synchronous safe point now, so  // do the pending stack shrink.  gp.preemptShrink = false shrinkstack(gp) } if gp.preemptStop { preemptPark(gp) // never returns  } // Act like goroutine called runtime.Gosched.  gopreempt_m(gp) // never return  } // … } func gopreempt_m(gp *g) { // 老朋友了  goschedImpl(gp) } 判断到 gp.stackguard0 = stackPreempt 后，无论如何都会走重新调度的逻辑，M 重新进入调度循环。 4.5.4 信号抢占 执行 preemptM() 会对 M 进行信号抢占，通过发送 SIGURG 信号触发 M 线程的信号处理函数。 const sigPreempt = _SIGURG func preemptM(mp *m) { if GOOS == \"darwin\" \u0026\u0026 GOARCH == \"arm64\" \u0026\u0026 !iscgo { return } if atomic.Cas(\u0026mp.signalPending, 0, 1) { if GOOS == \"darwin\" { atomic.Xadd(\u0026pendingPreemptSignals, 1) } // If multiple threads are preempting the same M, it may send many  // signals to the same M such that it hardly make progress, causing  // live-lock problem. Apparently this could happen on darwin. See  // issue #37741.  // Only send a signal if there isn't already one pending.  signalM(mp, sigPreempt) } } M 会执行信号处理函数 asyncPreempt()，最后调用到 asyncPreempt2()，使得 M 进入重新调度： // asyncPreempt saves all user registers and calls asyncPreempt2. // // When stack scanning encounters an asyncPreempt frame","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:4:5","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"总结 相对于 内存管理 与 垃圾收集，竟然感觉并发调度的结构还算简单。 需要弄清楚的有以下几点： 协程出现的意义 调度器的工作 GMP 模型整体框架 G、M、P 代表的意义 调度循环的流程 进入调度循环 M 的行为 找寻 G 的各个途径 M 执行 G 时，如何切换上下文 如何退出回到调度循环 调度切换 切换的各个时机 系统调用进行调度切换的行为 抢占的两种方式 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:5:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"参考 《Golang 学习笔记》 《Golang 设计与实现》：调度器 ","date":"2021-01-23","objectID":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/:6:0","tags":["Golang","Golang 原理"],"title":"Go 并发调度总结","uri":"/posts/language/golang/go-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"Golang 三色标记收集算法，算法实现原理","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Golang 垃圾收集的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 下面代码都是基于 go 1.15.6。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:0:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1 背景知识 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:1:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.1 术语 垃圾回收算法有一些基本的术语，首先需要知道对应的含义： Mutator：具有“改变对象”的意思，GC 中就是改动对象间引用关系的意思，也就是程序； 堆：对象使用的内存空间，GC 就是将垃圾对象空间放回到堆中； 根对象：对象的指针的“起点”部分，一般就是全局对象和栈对象； ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:1:1","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.2 三色标记算法 三色标记算法是 GC 标记清除算法Mark-Sweep 的一种，也是 Golang 中使用的算法。 推荐阅读 推荐阅读文章，写的非常详细：垃圾收集器 首先，三色标记算法的最基本逻辑为： 标记的最开始，所有对象默认为白色； 将 根对象标记为灰色，放入灰色集合； 从 灰色集合 中取出灰色对象，将其子对象标记为灰色，加入灰色集合，该灰色对象标记为黑色； 重复第 3 步，直到灰色集合为空； 清理所有的白色对象； 整个逻辑很简单，就是一个树的层次遍历，将所有可达的结点标记，然后清理未标记的不可达结点。 但是，仅仅是普通的三色标记算法要求执行时，Mutator 不能同时运行。因为如果 Mutator 并行时，某个扫描过的结点的引用关系变化，就可能导致 悬挂指针dangling pointer 问题。 例如，上图中第 3 步将 A 指向 D，那么 D 还是无法被标记，被错误回收。 而想要让 Mutator 同时运行时，标记的结果还保持正确，那么每个时刻标记的结果要满足 三色不变性Tri-color invariant 强三色不变性：黑色对象不会指向白色对象，只会指向灰色对象或者黑色对象。 因为黑色对象不会再被扫描，如果黑色对象指向白色对象，那么肯定该白色对象会被错误回收。 当然，除非这种情况能够满足弱三色不变性。 弱三色不变性：黑色对象执行白色对象，那么必须包含一条灰色对象经由多个白色对象的可达路径。 因为有了后面这个可达路径，也就是说白色对象还是可以被标记的，那么黑色对象可以指向该白色对象。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:1:2","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1.3 屏障技术 为了满足两个不变性，所以要在对象引用变更时，做出一些操作改变对象标记。这就是 GC 里 屏障技术barrier 的作用。 Go 中使用了写屏障，即在用户程序更新对象指针时，执行一些代码，使得继续满足不变性。 Note 这里的屏障技术似乎和我知道的 CPU 的屏障技术含义不太类似，更像是回调函数，也挺困惑 1.3.1 插入写屏障 Dijkstra 提出的 插入写屏障，在更新对象指针时，将其被指向的对象重新加入扫描集合（三色标记中也就是变为灰色），这样接下来还是能够被扫描。 可以看到，这样黑色对象始终指向的是灰色对象，永远都会满足强三色不变性。 但是，Dijkstra 也有一些缺点： 对象指针变动时，没有考虑旧的指针引用。例如 *field 原来的对象 oldobj 已经扫描成黑色了，那么 *field = newobj 变动后，可能 oldobj 变为垃圾对象，只有等到下一轮标记时才会被回收。 TODO 1.3.2 删除写屏障 Yuasa 提出的 删除写屏障，让老对象的引用被删除时，将白色的老对象涂成灰色，这样删除写屏障就可以保证弱三色不变性。 1.3.3 Go 中的屏障 Go 中使用 混合写屏障，即插入写屏障与删除写屏障都开启，并且在标记阶段开始后，将创建的所有新对象都标记为黑色，防止新分配的对象被错误的回收。 具体操作为： GC 开始将栈上的对象全部扫描并标记为黑色（之后不再进行第二次重复扫描，无需 STW)， GC 期间，任何在栈上创建的新对象，均为黑色。 被删除的对象标记为灰色。 被添加的对象标记为灰色。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:1:3","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2 三色标记算法实现 我们先不看整个的流程实现，而是从核心的标记算法入手。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:2:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.1 标记 2.1.1 并发标记框架 runtime 使用并发的标记方式，多个 groutine 同时执行一部分内存的标记操作。其整个就是基于一个工作池框架。 首先，有着一个全局的工作队列，其放在 work 全局变量中： var work struct { full lfstack // lock-free list of full blocks workbuf empty lfstack // lock-free list of empty blocks workbuf } full：全局的非空的队列组成的链表，单个 groutine 没有任务时就从这里取一个队列使用； empty：全局的空的队列组成的链表，单个 groutine 的工作队列满时就会这里去一个队列切换； 每个 goroutine 有着独立对应的 gcWork 对象，其类似一个队列，从队列中取出需要扫描的内存的地址。 type workbuf struct { workbufhdr obj [(_WorkbufSize - unsafe.Sizeof(workbufhdr{})) / sys.PtrSize]uintptr } type gcWork struct { // wbuf1 and wbuf2 are the primary and secondary work buffers.  //  // This can be thought of as a stack of both work buffers'  // pointers concatenated. When we pop the last pointer, we  // shift the stack up by one work buffer by bringing in a new  // full buffer and discarding an empty one. When we fill both  // buffers, we shift the stack down by one work buffer by  // bringing in a new empty buffer and discarding a full one.  // This way we have one buffer's worth of hysteresis, which  // amortizes the cost of getting or putting a work buffer over  // at least one buffer of work and reduces contention on the  // global work lists.  //  // wbuf1 is always the buffer we're currently pushing to and  // popping from and wbuf2 is the buffer that will be discarded  // next.  //  // Invariant: Both wbuf1 and wbuf2 are nil or neither are.  wbuf1, wbuf2 *workbuf … } wbuf1 与 wbuf2: 就是主队列与备队列。所有操作都会先操作 wbuf1，如果 wbuf1 空间不足或者没有对象，就会触发 wbuf1 与 wbuf2 切换。当两个缓冲区都空间不足或者满时，就会从 work.free 或者 work.list 得到一个空闲的，并赋值 wbuf1。 gcWork 有几个重要的方法： gcWork.tryGetFast() ：从 wbuf1 快速得到一个 obj 的地址； gcWork.tryGet() ：从 wbuf1 -\u003e wbuf2 -\u003e work.full 获取一个 obj； gcWork.putFast() ：将一个待扫描的 obj 放入 wbuf1； gcWork.put() ：将一个待扫描的 obj 放入 wbuf1 -\u003e wbuf2 -\u003e work.empty； gcWork.balance() ：将 wbuf2/wbuf1 放入 work.full 中； gcWork.empty() ：判断是否 gcWork 为空，为空表明没有会灰色 obj； 可以看到，整个过程就是围绕了各个工作队列的生产-消费过程。先从根对象开始，goroutine 从 gcWork 取出待扫描的 obj，将其标记，然后将 obj 指向的子 obj 再次放入 gcWork。不断循环，直到 gcWork 为空。 2.1.2 groutine 标记流程 下面看下核心的标记流程，这里我们仅仅关注一个 groutine 的工作。大致的标记步骤如下： 将根对象放入 gcWork； groutine 从 gcWork 取出一个 object 地址，将其标记； 将 object 包含的指针指向的 object 再次放入 gcWork； 重复 2-3 步，直到 gcWork 为空； 而对应于三色标记，我们可以确认不同颜色的对象在 runtime 中的对应： 灰色对象 -\u003e gcWork 中的 object； 黑色对象 -\u003e 不在 gcWork 中，但是被 mark 的 object； 白色对象 -\u003e 不在 gcWork 中，没有被 mark 的 object； 每个 P 会对应一个标记使用的 groutine，执行 gcDrain() 函数（runtime/mgcmark.go）： // gcDrain scans roots and objects in work buffers, blackening grey // objects until it is unable to get more work. It may return before // GC is done; it's the caller's responsibility to balance work from // other Ps. // // If flags\u0026gcDrainUntilPreempt != 0, gcDrain returns when g.preempt // is set. // // If flags\u0026gcDrainIdle != 0, gcDrain returns when there is other work // to do. // // If flags\u0026gcDrainFractional != 0, gcDrain self-preempts when // pollFractionalWorkerExit() returns true. This implies // gcDrainNoBlock. // // If flags\u0026gcDrainFlushBgCredit != 0, gcDrain flushes scan work // credit to gcController.bgScanCredit every gcCreditSlack units of // scan work. // // gcDrain will always return if there is a pending STW. // //go:nowritebarrier func gcDrain(gcw *gcWork, flags gcDrainFlags) { gp := getg().m.curg preemptible := flags\u0026gcDrainUntilPreempt != 0 flushBgCredit := flags\u0026gcDrainFlushBgCredit != 0 idle := flags\u0026gcDrainIdle != 0 initScanWork := gcw.scanWork // 配置退出标记的 check 函数，根据不同策略退出标记  checkWork := int64(1\u003c\u003c63 - 1) var check func() bool if flags\u0026(gcDrainIdle|gcDrainFractional) != 0 { checkWork = initScanWork + drainCheckThreshold if idle { check = pollWork } else if flags\u0026gcDrainFractional != 0 { check = pollFractionalWorkerExit } } // 扫描根对象放入 gcWork  if work.markrootNext \u003c work.markrootJobs { // Stop if we're preemptible or if someone wants to STW.  for !(gp.preempt \u0026\u0026 (preemptible || atomic.Load(\u0026sched.gcwaiting) != 0)) { job := atomic.Xadd(\u0026work.markrootNext, +1) - 1 if job \u003e= work.markrootJobs { break } markroot(gcw, job) if check != nil \u0026\u0026 check() { goto done } } } // 标记循环  for !(gp.preem","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:2:1","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.2 写屏障 在各个代码的注释中，可以看到 “//go:nowritebarrier”。显然，这样让编译器编译时不加入写屏障的意思，因此可以想到，默认的函数执行都会加入写屏障的逻辑。 在 SSA 中间代码生成阶段，编译器会在 Store、Move、Zero 操作中加入写屏障，写屏障函数为 writebarrier() 函数（cmd/compile/internal/ssa/writebarrier.go）。 writebarrier() 函数很复杂，这里不展开。再次看一下混合写屏障操作： GC 开始将栈上的对象全部扫描并标记为黑色（之后不再进行第二次重复扫描，无需 STW)。 GC 期间，任何在栈上创建的新对象，均为黑色。 被删除的对象标记为灰色。 被添加的对象标记为灰色。 当开始 GC 时，全局变量 runtime.writeBarrier.enabled 变为 true，所有的写操作都会经过 writebarrier() 的操作。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:2:2","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3 内存清理 内存清理与标记就是完全分隔的逻辑了，通过判断对象是否被标记就可决定是否将其内存回收。 gcSweep() 函数用于在 GC 标记结束后执行清理（src/runtime/mgc.go）： // gcSweep must be called on the system stack because it acquires the heap // lock. See mheap for details. // // The world must be stopped. // //go:systemstack func gcSweep(mode gcMode) { lock(\u0026mheap_.lock) // 关键的 sweepgen 变量  mheap_.sweepgen += 2 mheap_.sweepdone = 0 if !go115NewMCentralImpl \u0026\u0026 mheap_.sweepSpans[mheap_.sweepgen/2%2].index != 0 { // We should have drained this list during the last  // sweep phase. We certainly need to start this phase  // with an empty swept list.  throw(\"non-empty swept list\") } mheap_.pagesSwept = 0 mheap_.sweepArenas = mheap_.allArenas mheap_.reclaimIndex = 0 mheap_.reclaimCredit = 0 unlock(\u0026mheap_.lock) if go115NewMCentralImpl { sweep.centralIndex.clear() } // 阻塞清理  if !_ConcurrentSweep || mode == gcForceBlockMode { // Special case synchronous sweep.  // Record that no proportional sweeping has to happen.  lock(\u0026mheap_.lock) mheap_.sweepPagesPerByte = 0 unlock(\u0026mheap_.lock) // 清理 span !  // Sweep all spans eagerly.  for sweepone() != ^uintptr(0) { sweep.npausesweep++ } // Free workbufs eagerly.  prepareFreeWorkbufs() for freeSomeWbufs(false) { } // All \"free\" events for this mark/sweep cycle have  // now happened, so we can make this profile cycle  // available immediately.  mProf_NextCycle() mProf_Flush() return } // 后台清理（并发清理）  // Background sweep.  lock(\u0026sweep.lock) if sweep.parked { sweep.parked = false ready(sweep.g, 0, true) } unlock(\u0026sweep.lock) } 不断执行 sweepone() 来清理 mspan； 启动后台并发清理； ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:3:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.1 阻塞清理 通过不断执行 sweepone() 来进行 mspan 的清理，sweepone() 从 heap 得到一个 mspan 并清理（src/runtime/mgcsweep.go）： // sweepone sweeps some unswept heap span and returns the number of pages returned // to the heap, or ^uintptr(0) if there was nothing to sweep. func sweepone() uintptr { … // 得到一个被清理的 mspan var s *mspan sg := mheap_.sweepgen for { if go115NewMCentralImpl { s = mheap_.nextSpanForSweep() } else { s = mheap_.sweepSpans[1-sg/2%2].pop() } if s == nil { atomic.Store(\u0026mheap_.sweepdone, 1) break } // 设置标记 if s.sweepgen == sg-2 \u0026\u0026 atomic.Cas(\u0026s.sweepgen, sg-2, sg-1) { break } } // 清理 mspan npages := ^uintptr(0) if s != nil { npages = s.npages if s.sweep(false) { // Whole span was freed. Count it toward the // page reclaimer credit since these pages can // now be used for span allocation. atomic.Xadduintptr(\u0026mheap_.reclaimCredit, npages) } else { // Span is still in-use, so this returned no // pages to the heap and the span needs to // move to the swept in-use list. npages = 0 } } … return npages } 最终的回收工作靠 mspan.sweep() 完成，这给在 Go 内存管理总结 中可以看到具体实现，大致就是将 GC 后没有被 mark 的 object 记录为可用，以后续申请使用覆盖。如果整个 mspan 变回空，就由 mheap 回收其对应的 page。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:3:1","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.2 并发清理 并发清理就是一个死循环，被唤醒后开始执行清理任务。 func bgsweep(c chan int) { sweep.g = getg() lockInit(\u0026sweep.lock, lockRankSweep) lock(\u0026sweep.lock) sweep.parked = true c \u003c- 1 goparkunlock(\u0026sweep.lock, waitReasonGCSweepWait, traceEvGoBlock, 1) for { // 依旧通过 sweepone() 清理 for sweepone() != ^uintptr(0) { sweep.nbgsweep++ Gosched() } for freeSomeWbufs(true) { Gosched() } lock(\u0026sweep.lock) if !isSweepDone() { // This can happen if a GC runs between // gosweepone returning ^0 above // and the lock being acquired. unlock(\u0026sweep.lock) continue } // 等待唤醒 sweep.parked = true goparkunlock(\u0026sweep.lock, waitReasonGCSweepWait, traceEvGoBlock, 1) } } 依旧是通过不断执行 sweepone() 进行清理。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:3:2","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4 标记流程 下面来看如何触发的 GC 以及 GC 的大致流程。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:4:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.1 GC 触发 GC 有三个点会被触发： runtime 启动后会启动一个后台 gourtine，被唤醒后就会执行 GC，而唤醒操作由 sysmon 负责执行。 sysmon 会根据系统情况决定是否触发。 分配新 object 时（mallocgc() 函数），如果 mcache 需要重新刷新，或者是分配的是 large object，那么也会触发一次 GC。 通过接口 runtime.GC() 主动触发。 所有的触发都会使用 gcTrigger.test() 进行条件检测（runtime/mgc.go）： // test reports whether the trigger condition is satisfied, meaning // that the exit condition for the _GCoff phase has been met. The exit // condition should be tested when allocating. func (t gcTrigger) test() bool { if !memstats.enablegc || panicking != 0 || gcphase != _GCoff { return false } switch t.kind { case gcTriggerHeap: // 由 heap 触发，也就是分配 object 时触发  // Non-atomic access to heap_live for performance. If  // we are going to trigger on this, this thread just  // atomically wrote heap_live anyway and we'll see our  // own write.  return memstats.heap_live \u003e= memstats.gc_trigger case gcTriggerTime: // 由 sysmon 周期性触发  if gcpercent \u003c 0 { return false } lastgc := int64(atomic.Load64(\u0026memstats.last_gc_nanotime)) return lastgc != 0 \u0026\u0026 t.now-lastgc \u003e forcegcperiod case gcTriggerCycle: // 通过 runtime.GC() 主动触发  // t.n \u003e work.cycles, but accounting for wraparound.  return int32(t.n-work.cycles) \u003e 0 } return true } 对应的条件为： sysmon 周期性触发：触发间隔大于 2min； 分配 object 触发：堆内存的分配达到控制计算的触发堆大小； runtime.GC() 主动触发：当前没有正在 GC，则触发； ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:4:1","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.2 GC 开始 所有触发 GC 后调用的都是 gcStart() 函数（runtime/mgc.go）： // gcStart starts the GC. It transitions from _GCoff to _GCmark (if // debug.gcstoptheworld == 0) or performs all of GC (if // debug.gcstoptheworld != 0). // // This may return without performing this transition in some cases, // such as when called on a system stack or with locks held. func gcStart(trigger gcTrigger) { // Since this is called from malloc and malloc is called in // the guts of a number of libraries that might be holding // locks, don't attempt to start GC in non-preemptible or // potentially unstable situations. mp := acquirem() if gp := getg(); gp == mp.g0 || mp.locks \u003e 1 || mp.preemptoff != \"\" { releasem(mp) return } releasem(mp) mp = nil // 再次验证是否条件，并不断调用 sweepone() 来完成上一次垃圾收集的收尾工作 // Pick up the remaining unswept/not being swept spans concurrently // // This shouldn't happen if we're being invoked in background // mode since proportional sweep should have just finished // sweeping everything, but rounding errors, etc, may leave a // few spans unswept. In forced mode, this is necessary since // GC can be forced at any point in the sweeping cycle. // // We check the transition condition continuously here in case // this G gets delayed in to the next GC cycle. for trigger.test() \u0026\u0026 sweepone() != ^uintptr(0) { sweep.nbgsweep++ } // Perform GC initialization and the sweep termination // transition. semacquire(\u0026work.startSema) // Re-check transition condition under transition lock. if !trigger.test() { semrelease(\u0026work.startSema) return } // 获取 STW 的锁 // Ok, we're doing it! Stop everybody else semacquire(\u0026gcsema) semacquire(\u0026worldsema) // 每个 P 分配一个 G，准备开始执行后台的标记工作 gcBgMarkStartWorkers() // 执行 STW ! systemstack(stopTheWorldWithSema) // Finish sweep before we start concurrent scan. systemstack(func() { finishsweep_m() }) // 修改 GC 状态，进入标记 setGCPhase(_GCmark) // 初始化标记所需状态 gcBgMarkPrepare() // 计算 Data、BSS、Stack 等需要扫描的数量 gcMarkRootPrepare() // 直接标记 tiny object gcMarkTinyAllocs() // 可以开始运行标记 atomic.Store(\u0026gcBlackenEnabled, 1) // STW 结束，G 开始进行并行标记 // Concurrent mark. systemstack(func() { now = startTheWorldWithSema(trace.enabled) work.pauseNS += now - work.pauseStart work.tMark = now }) // 释放锁 semrelease(\u0026worldsema) releasem(mp) if mode != gcBackgroundMode { Gosched() } semrelease(\u0026work.startSema) } 该函数比较复杂，大致分为下面几个步骤： 主动进入休眠状态，并等待唤醒； 根据 P.gcMarkWorkerMode 决定标记的策略； 调用 gcDrain() 进行标记 所有标记任务完成后，调用 gcMarkDone() 完成标记阶段； 因为标记阶段是与用户进程并发的，所以会涉及到执行垃圾收集还是普通程序的问题。为此，每个垃圾收集的 G 有着不同的标记策略，其依赖于 P.gcMarkWorkerMode（由一个独立的 G 计算出不同模式的 P 的数量并设置）。 其包含三种标记策略： gcMarkWorkerDedicatedMode：P 专门用于标记对象，不会被抢占； gcMarkWorkerFractionalMode：当垃圾收集后台 CPU 使用率达不到 25%，会启动该类型工作协程帮助垃圾收集达到利用率目标，因为只占用一个 CPU 部分资源，可以被抢占； gcMarkWorkerIdleMode：当 P 没有可以执行的 G 时，会运行垃圾收集标记任务直到被抢占； ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:4:2","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.3 标记结束 当每个标记 Groutine 结束后，都会调用 gcMarkDone()，但是等待所有标记结束后，只有一个 groutine 会真正执行结束逻辑（runtime/mgc.go）。 func gcMarkDone() { // 第一个获取到锁的才会执行 gcMarkDone() // Ensure only one thread is running the ragged barrier at a // time. semacquire(\u0026work.markDoneSema) top: // 后续的 gouroute 会串行的在这里退出 if !(gcphase == _GCmark \u0026\u0026 work.nwait == work.nproc \u0026\u0026 !gcMarkWorkAvailable(nil)) { semrelease(\u0026work.markDoneSema) return } // 循环等待所有标记结束 gcMarkDoneFlushed = 0 systemstack(func() { gp := getg().m.curg casgstatus(gp, _Grunning, _Gwaiting) forEachP(func(_p_ *p) { wbBufFlush1(_p_) _p_.gcw.dispose() if _p_.gcw.flushedWork { atomic.Xadd(\u0026gcMarkDoneFlushed, 1) _p_.gcw.flushedWork = false } }) casgstatus(gp, _Gwaiting, _Grunning) }) if gcMarkDoneFlushed != 0 { goto top } … // Perform mark termination. This will restart the world. gcMarkTermination(nextTriggerRatio) } 在一大堆判断标记结束的逻辑后，调用 gcMarkTermination() 进入标记终止阶段。 在 gcMarkTermination() 会关闭混合写屏障，决定触发垃圾收集的 heap 阈值，并进行相关信息的统计，然后调用 gcSweep() 进行阻塞式清理。 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:4:3","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"总结 垃圾回收真的很复杂，上面省略了大量的细节，也有可能理解错误的情况。但是忽略掉繁琐的细节，需要完全明白的有几个点： 三色标记算法的步骤 强三色不变性概念，以及为什么需要 写屏障的作用，以及 Go 使用的混合写屏障 GC 触发的时机与条件 GC 并发标记的实现 GC 标记的实现，与内存管理的协同 内存清理的时机 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:5:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"参考 《Golang 学习笔记》 《Golang 设计与实现》：内存分配器 ","date":"2021-01-14","objectID":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/:6:0","tags":["Golang","Golang 原理"],"title":"Go 垃圾收集总结","uri":"/posts/language/golang/go-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"Golang 协程栈实现，堆内存管理实现","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对 Golang 内存模型与内存管理的一个总结，基本内容来源于网络的学习，以及自己观摩了下源码。 所以学习的书籍与文章见 参考。 下面代码都是基于 go 1.15.6。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:0:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"1 Linux 内存模型 所有语言的内存管理，在 Linux 上都是在以基本的进程内存模型基础上实现的，首先需要知道 Linux 进程内存布局。 在进程角度，看到的所有内存就是 虚拟地址空间virtual address space ，整个是一个线性的存储地址。其中一部分高地址区域用户态无法访问，是内核地址空间。而另一部分就是由栈、mmap、堆等内存区域组成的用户地址空间。 上面进程可以自己分配与管理的进程，就是 mmap 与 堆，对应的系统调用为 mmap() 与 brk()，因此所有语言的内存管理都是基于这两个内存区域在进一步实现的（包括 glibc 的 malloc() 与 free()）。 mmap 最基本有两个用途： 文件映射 ：申请一块内存区域，映射到文件系统上一个文件（这也是 page_cache 的基本原理，所以他们在内核中都使用 address_space 实现） 匿名映射 ：申请一块内存区域，但是没有映射到具体文件，相当于分配了一块内存区域（可以用于父子进程共享、或者自己管理内存的分配等功能） 而所有在内存上所说的地址，包括代码指令地址、变量地址都是上面地址空间的一个地址。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:1:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2 PC 与 SP Goroutine 将进程的切换变为了协程间的切换，那么就需要在用户空间负责执行代码与协程上下文的保留与切换。因此，有两个关键的寄存器：PC 与 SP。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:2:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.1 PC 程序计数器 PCProgram Counter 是 CPU 中的一个寄存器，保存着下一个 CPU 执行的指令的位置。顺序执行指令时，PC = PC + 1（一个指令）。而调用函数或者条件跳转时，会将跳到的指令地址设置到 PC 中。 所以，可以想到，当需要切换执行的 goroutine，调用 JMP 指令跳转到 G 对应的代码。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:2:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"2.2 SP 栈顶指针 SPstack pointer 是保存栈顶地址的寄存器，我们平时所说的临时变量在栈上，就是将临时变量的值写入 SP 保存的内存地址，然后 SP 保存的地址减小（栈是从高地址向低地址变化），然后临时变量销毁时，SP 地址又变为高地址。 不过，因为 goroutine 切换时，必须要保存当前 goroutine 的上下文，也就是栈里的变量。因此，goroutine 栈肯定是不能使用 Linux 进程栈了（因为进程栈有上限，也无法实现“保存”这种功能）。所以所说的协程栈，都是基于 mmap 申请内存空间（基于 Go 内存管理，内存管理基于 mmap），然后切换时修改 SP 寄存器地址实现的。 这也是为什么 goroutine 栈可以“无限大”的原因了。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:2:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3 Goroutine 栈 整体的一个 G 的栈如下图所示： stack.lo： G 栈的最大低地址（也就是上限）； stack.hi：G 栈的初始地址； stackguard0：阈值地址，用于判断 G 栈是否需要扩容； StackGuard：常量，栈的保护区，也就是预留的地址； StackSmall：常量，用于小函数调用的优化； 先看一下 g 的实现中包含的 stack 属性（runtime/runtime2.go），其实注释写的就很明白了： type g struct { // Stack parameters. // stack describes the actual stack memory: [stack.lo, stack.hi). // stackguard0 is the stack pointer compared in the Go stack growth prologue. // It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption. // stackguard1 is the stack pointer compared in the C stack growth prologue. // It is stack.lo+StackGuard on g0 and gsignal stacks. // It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash). stack stack // offset known to runtime/cgo stackguard0 uintptr // offset known to liblink stackguard1 uintptr // offset known to liblink // ... } stack 属性就是 G 对应的栈了（这也表明了不是使用的进程栈）； Note stack 与 stackguard0 属性一定要在 g 结构的开头，因为汇编中会使用指定的偏移 (0x10) 来获取对应的值； 具体看一下 stack 结构（runtime/runtime2.go）： // Stack describes a Go execution stack. // The bounds of the stack are exactly [lo, hi), // with no implicit data structures on either side. type stack struct { lo uintptr hi uintptr } ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.1 新 G 的栈 在 malg 函数中，可以看到对于新 G 的栈的分配（一开始为 2KB）： // Allocate a new g, with a stack big enough for stacksize bytes. func malg(stacksize int32) *g { newg := new(g) if stacksize \u003e= 0 { stacksize = round2(_StackSystem + stacksize) // 在公共的 goroutine(g0) 上调用函数 systemstack(func() { // 分配一个 stack newg.stack = stackalloc(uint32(stacksize)) }) // 设置 stackguard0 地址 newg.stackguard0 = newg.stack.lo + _StackGuard newg.stackguard1 = ^uintptr(0) // Clear the bottom word of the stack. We record g // there on gsignal stack during VDSO on ARM and ARM64. *(*uintptr)(unsafe.Pointer(newg.stack.lo)) = 0 } return newg } Note 注意 systemstack()，用于将当前栈切换到 M 的 g0 协程栈上执行命令。 Why? 因为 G 用于执行用户逻辑，而某些管理操作不方便在 G 栈上执行（例如 G 可能中途停止，垃圾回收时 G 栈空间也有可能被回收），所以需要执行管理命令时，都会通过 systemstack 方法将线程栈切换为 g0 的栈执行，与用户逻辑隔离。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.2 栈的分配 stackalloc() 函数用于分配一个栈，无论是给新 G 还是扩容栈时都会用到，因此栈空间的分配与回收是一个比较频繁的操作，所以栈空间采取了缓存复用的方式。 主要逻辑如下： 如果分配的栈空间不大，就走缓存复用这种方式分配。没有可以复用的就创建； 如果分配的栈空间很大（大于 32KB），就直接从 heap 分配； 这里主要关注第 1 中方式，会调用 stackpoolalloc() 函数。 // Allocates a stack from the free pool. Must be called with // stackpool[order].item.mu held. func stackpoolalloc(order uint8) gclinkptr { list := \u0026stackpool[order].item.span s := list.first // ... if s == nil { // 没有可以复用的栈，走内存管理创建 s = mheap_.allocManual(_StackCacheSize\u003e\u003e_PageShift, \u0026memstats.stacks_inuse) // ... } x := s.manualFreeList if x.ptr() == nil { throw(\"span has no free stacks\") } s.manualFreeList = x.ptr().next // ... return x } 可以看到，首先尝试从 stackpool 缓存的空闲的 stack 获取，如果没有则走 Go 内存管理申请一个。 再接下来就是 Go 内存管理模块负责的事了，不深入下去（后面再说）。底层创建都是使用 mmap 系统调用实现的，这里可以看下使用的参数： // Don't split the stack as this method may be invoked without a valid G, which // prevents us from allocating more stack. //go:nosplit func sysAlloc(n uintptr, sysStat *uint64) unsafe.Pointer { p, err := mmap(nil, n, _PROT_READ|_PROT_WRITE, _MAP_ANON|_MAP_PRIVATE, -1, 0) if err != 0 { if err == _EACCES { print(\"runtime: mmap: access denied\\n\") exit(2) } if err == _EAGAIN { print(\"runtime: mmap: too much locked memory (check 'ulimit -l').\\n\") exit(2) } return nil } mSysStatInc(sysStat, n) return p } 通过 mmap 调用的参数可以看到，申请了一个系统分配的匿名内存映射。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.3 栈的扩容 3.3.1 扩容判断 Go 编译器会在执行函数前，插入一些汇编指令，其中一个功能就是检查 G 栈是否需要扩容。看一个函数调用的实现： // main() 调用 test() $ go build -gcflags \"-l\" -o test main.go $ go tool objump -s \"main\\.test\" test TEXT main.test(SB) /root/yusihao/onething/BizImages/main.go main.go:3 0x45dc80 MOVQ FS:0xfffffff8, CX // CX 为当前 G 地址 main.go:3 0x45dc89 CMPQ 0x10(CX), SP // CX+0x10 执行 g.stackguard0 属性，与 SP 指针地址比较 main.go:3 0x45dc8d JBE 0x45dccf // 如果 SP \u003c=stackguard0 跳转到 0x45dccf，也就是调用 runtime.morestack_noctxt(SB) 函数 main.go:3 0x45dc8f SUBQ $0x18, SP // ... main.go:5 0x45dcce RET // 函数执行结束，RET 返回，不会执行后面两个指令 main.go:3 0x45dccf CALL runtime.morestack_noctxt(SB) // 执行栈扩容 main.go:3 0x45dcd4 MP main.test(SB) // 执行结束后，重新执行当前函数 逻辑很简单，如果 SP \u003c= stackguard0，那么就执行栈的扩容，扩容结束重新执行当前函数。 Note 上面比较 SP 时候，没有考虑当前函数调用使用的空间大小。Why? 因为测试程序这个函数中使用的空间比较小，而 stackguard0 与 stack.lo 有一段保护区，所以编译器允许这里 “溢出” 一些，所以这里就没有让 SP 考虑函数使用空间。 如果函数中使用的空间大过保护区时，比较时就会让 SP 减去当前函数使用空间再比较了。 3.3.2 扩容 扩容逻辑大致分为三步： 分配一个 2x 新栈； 拷贝当前栈数据至新栈； “释放\"掉旧栈； 从上面扩容判断可以看到，会调用 morestack 的汇编代码： // Called during function prolog when more stack is needed. // R3 prolog's LR // using NOFRAME means do not save LR on stack. // // The traceback routines see morestack on a g0 as being // the top of a stack (for example, morestack calling newstack // calling the scheduler calling newm calling gc), so we must // record an argument size. For that purpose, it has no arguments. TEXT runtime·morestack(SB),NOSPLIT|NOFRAME,$0-0 // Cannot grow scheduler stack (m-\u003eg0). MOVW g_m(g), R8 MOVW m_g0(R8), R4 CMP g, R4 BNE 3(PC) BL runtime·badmorestackg0(SB) B runtime·abort(SB) // Cannot grow signal stack (m-\u003egsignal). MOVW m_gsignal(R8), R4 CMP g, R4 BNE 3(PC) BL runtime·badmorestackgsignal(SB) B runtime·abort(SB) // Called from f. // Set g-\u003esched to context in f. MOVW R13, (g_sched+gobuf_sp)(g) MOVW LR, (g_sched+gobuf_pc)(g) MOVW R3, (g_sched+gobuf_lr)(g) MOVW R7, (g_sched+gobuf_ctxt)(g) // Called from f. // Set m-\u003emorebuf to f's caller. MOVW R3, (m_morebuf+gobuf_pc)(R8) // f's caller's PC MOVW R13, (m_morebuf+gobuf_sp)(R8) // f's caller's SP MOVW g, (m_morebuf+gobuf_g)(R8) // Call newstack on m-\u003eg0's stack. MOVW m_g0(R8), R0 BL setg\u003c\u003e(SB) MOVW (g_sched+gobuf_sp)(g), R13 MOVW $0, R0 MOVW.W R0, -4(R13) // create a call frame on g0 (saved LR) BL runtime·newstack(SB) // Not reached, but make sure the return PC from the call to newstack // is still in this function, and not the beginning of the next. RET TEXT runtime·morestack_noctxt(SB),NOSPLIT|NOFRAME,$0-0 MOVW $0, R7 B runtime·morestack(SB) 可以看到 g0，gsignal 的栈都不会扩容 在 g0 栈上会调用 newstack() 函数 调用的 newstack() 函数（runtime/stack.go），过程很复杂，只看一下关键点： // Called from runtime·morestack when more stack is needed. // Allocate larger stack and relocate to new stack. // Stack growth is multiplicative, for constant amortized cost. // // g-\u003eatomicstatus will be Grunning or Gscanrunning upon entry. // If the scheduler is trying to stop this g, then it will set preemptStop. // // This must be nowritebarrierrec because it can be called as part of // stack growth from other nowritebarrierrec functions, but the // compiler doesn't check this. // //go:nowritebarrierrec func newstack() { thisg := getg() gp := thisg.m.curg // ... // 新栈大小为当前两倍 oldsize := gp.stack.hi - gp.stack.lo newsize := oldsize * 2 // ... // 改变 G 状态为 copy stack，gc 会跳过该状态的 G casgstatus(gp, _Grunning, _Gcopystack) // 分配新栈，拷贝数据，释放旧站 // The concurrent GC will not scan the stack while we are doing the copy since // the gp is in a Gcopystack status. copystack(gp, newsize) if stackDebug \u003e= 1 { print(\"stack grow done\\n\") } casgstatus(gp, _Gcopystack, _Grunning) // 执行 G 代码 gogo(\u0026gp.sched) } // Copies gp's stack to a new stack of a different size. // Caller must have changed gp status to Gcopystack. func copystack(gp *g, newsize uintptr) { // 创建新 stack new := stackalloc(uint32(newsize)) // ... // 拷贝数据 memmove(unsafe.Pointer(new.hi-ncopy), unsafe.Pointer(old.hi-ncopy), ncopy) // ... gp.stack = new gp.stackguard0 = new.lo + _StackGuard // 释放旧 stack if stackPoisonCopy != 0 { fillstac","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:3","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.4 栈的释放 stackfree 栈的释放与申请相反，放入 stackpool，或者直接调用内存管理删除，重点还是内存管理的活，所以这里不展开。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:4","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.5 栈的切换 切换应该是属于 goroutine 调度的内容，不过这里可以关注一下栈时如何切换的。 当 M 执行的 G 需要切换，或者一个新创建 G 执行时，最后都会调用 execute() 函数，而 execute() 函数会调用 gogo 汇编实现的函数。 // func gogo(buf *gobuf) // restore state from Gobuf; longjmp TEXT runtime·gogo(SB), NOSPLIT, $16-8 MOVQ buf+0(FP), BX // gobuf MOVQ gobuf_g(BX), DX MOVQ 0(DX), CX // make sure g != nil get_tls(CX) MOVQ DX, g(CX) MOVQ gobuf_sp(BX), SP // restore SP （关键！) MOVQ gobuf_ret(BX), AX MOVQ gobuf_ctxt(BX), DX MOVQ gobuf_bp(BX), BP MOVQ $0, gobuf_sp(BX) // clear to help garbage collector MOVQ $0, gobuf_ret(BX) MOVQ $0, gobuf_ctxt(BX) MOVQ $0, gobuf_bp(BX) MOVQ gobuf_pc(BX), BX JMP BX gobuf 中保存着要执行的 G 的 sp、pc 指针，可以看到通过将对应 gobuf.sp 写入到 SP 寄存器中，也就是将使用的栈切换为了 G 的栈。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:5","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"3.6 g0 的栈 在阅读网上的文章时，许多文章都说 g0 使用的是系统栈，我理解为使用的是进程的栈内存区域。但是思考一下，每个 M 对应一个 g0，也就是说有多个线程要同时共享系统栈，这是不可能的。例如在 pthread 实现中，对应新建的线程也是使用 mmap 分配一个内存区域，然后调用 clone() 系统调用时传入栈地址参数。 看一下代码，确认一下到底 g0 的栈到底是啥，找到一个新建 m 的地方： mp := new(m) mp.mstartfn = fn mcommoninit(mp, id) // In case of cgo or Solaris or illumos or Darwin, pthread_create will make us a stack. // Windows and Plan 9 will layout sched stack on OS stack. if iscgo || GOOS == \"solaris\" || GOOS == \"illumos\" || GOOS == \"windows\" || GOOS == \"plan9\" || GOOS == \"darwin\" { mp.g0 = malg(-1) } else { mp.g0 = malg(8192 * sys.StackGuardMultiplier) } mp.g0.m = mp 可以看到，m 的 g0 属性还是使用的 [malg() 函数](#31-新-g-的栈） 去创建的，与普通的 g 创建一样，只不过初始大小为 8KB。malg() 流程上面有说到，就是走内存管理分配 mspan 作为栈的方式。 不过，g0 的栈还是有些不同的，不会进行栈的扩容（因为仅仅内部管理时用到，不需要进行自动扩容），在栈扩容的 morestack 汇编代码 里可以看到。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:3:6","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4 内存模型 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.1 概览 Golang 内存管理包含四个组件： object ：object 代表用户代码申请的一个对象，没有实际的数据结构，而是在 mspan 中以逻辑切分的方式分配； page：切分内存的单元，mheap 将内存以 8KB page 切分，然后组合成为 mspan； runtime.mspan ：内存管理的最小单元，由多个 8KB 大小的 page 构成，按照固定大小来切分为多个 object； runtime.mcache ：单个 P 对应的 mspan 的缓存，无锁分配； runtime.mcentral ：按照不同大小的 mspan 分组的管理链表，为 mcache 提供空闲 mspan runtime.mheap ：保存闲置的 mspan 与 largerspan 链表，与操作系统申请与释放内存； 上面的组件也可以看做分层，普通对象（object）的申请与释放就是按照上下层顺序申请与释放的。 Note 下面不会说（后面再说）具体的 object 分配流程，而是说明各个层次时的申请与释放操作。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.2 mspan 每个 mspan 由多个 8KB 的 page 组成，所有的 mspan 会以 list 的方式构建，而不同的模块（mcache、mcentral）通过引用指针，来不同方式来组织不同的 mspan。 每个 mspan 管理多个固定大小的 object，通过编号 (index) 方式来寻找 object 的地址。 结构如下图所示： 其数据结构如下（省略部分）： type mspan struct { next *mspan // next span in list, or nil if none  prev *mspan // previous span in list, or nil if none startAddr uintptr // address of first byte of span aka s.base()  npages uintptr // number of pages in span manualFreeList gclinkptr // list of free objects in mSpanManual spans  freeindex uintptr nelems uintptr // number of object in the span.  allocCache uint64 allocBits *gcBits gcmarkBits *gcBits // sweep generation: // if sweepgen == h-\u003esweepgen - 2, the span needs sweeping // if sweepgen == h-\u003esweepgen - 1, the span is currently being swept // if sweepgen == h-\u003esweepgen, the span is swept and ready to use // if sweepgen == h-\u003esweepgen + 1, the span was cached before sweep began and is still cached, and needs sweeping // if sweepgen == h-\u003esweepgen + 3, the span was swept and then cached and is still cached // h-\u003esweepgen is incremented by 2 after every GC  sweepgen uint32 spanclass spanClass // size class and noscan (uint8)  allocCount uint16 // number of allocated objects  elemsize uintptr // computed from sizeclass or from npages } next、prev ：链表前后 span； startAddr ：span 在 arena 区域的起始地址； npages ：占用 page(8KB) 数量； manualFreeList ：空闲 object 链表； freeindex ：下一个空闲的 object 的编号，如果 freeindex == nelem，表明没有空闲 object 可以分配 nelems ：当前 span 中分配的 object 的上限； allocCache ：freeindex 的 cache，通过 bitmap 的方式记录对应编号的 object 内存是否是空闲的； allocBits : 通过 bitmap 标识哪些编号的 object 是分配出去的； gcmarkBits : 经过 GC 后，gcmarkBits 标识出的 object 就是被 mark 的，没有 mark 的变为垃圾对象清除； sweepgen ：mspan 的状态，见注释； spanclass ：mspan 大小类别； allocCount ：已经分配的 object 数量； elemsize ：管理的 object 的固定大小； 可以看到，每个 mspan 管理着固定大小的 object，并通过一个 freeindex+allocCache 来记录空闲的 object 的编号。由此可以得出： mspan 的地址区域: [startAddr, startAddr + npages*8*1024) 某个 object 的起始地址: \u003cindex\u003e*elemsize + startAddr 4.2.1 object 分配 在创建新的 object 时，对于普通大小的 object 分配（16\u003csize\u003c32KB)，会在从 mcache 中选出具有空闲空间的 mspan，然后记录到 mspan.allocCache 中。 具体代码如下，nextFreeIndex() 函数就是用于得到下一个空闲 object，并移动 freeindex（runtime/mbitmap.go)： // nextFreeIndex returns the index of the next free object in s at // or after s.freeindex. // There are hardware instructions that can be used to make this // faster if profiling warrants it. func (s *mspan) nextFreeIndex() uintptr { sfreeindex := s.freeindex snelems := s.nelems if sfreeindex == snelems { return sfreeindex } aCache := s.allocCache bitIndex := sys.Ctz64(aCache) for bitIndex == 64 { // Move index to start of next cached bits.  sfreeindex = (sfreeindex + 64) \u0026^ (64 - 1) if sfreeindex \u003e= snelems { s.freeindex = snelems return snelems } whichByte := sfreeindex / 8 // Refill s.allocCache with the next 64 alloc bits.  s.refillAllocCache(whichByte) aCache = s.allocCache bitIndex = sys.Ctz64(aCache) // nothing available in cached bits  // grab the next 8 bytes and try again.  } result := sfreeindex + uintptr(bitIndex) if result \u003e= snelems { s.freeindex = snelems return snelems } s.allocCache \u003e\u003e= uint(bitIndex + 1) sfreeindex = result + 1 if sfreeindex%64 == 0 \u0026\u0026 sfreeindex != snelems { // We just incremented s.freeindex so it isn't 0.  // As each 1 in s.allocCache was encountered and used for allocation  // it was shifted away. At this point s.allocCache contains all 0s.  // Refill s.allocCache so that it corresponds  // to the bits at s.allocBits starting at s.freeindex.  whichByte := sfreeindex / 8 s.refillAllocCache(whichByte) } s.freeindex = sfreeindex return result } 注意：目前跳过了 “nextFreeFast” 实现，该获取 span 比 “nextFree” 更快，使用了 mspan.allocCache。 4.2.2 mspan 的清理 mspan.sweep() 用于进行一个 mspan 的清理，我们先看下旧版本的实现 mspan.oldSweep()： // Sweep frees or collects finalizers for blocks not marked in the mark phase. // It clears the mark bits in preparation for the next GC round. // Returns true if the span was returned to heap. // If preserve=true, don't return it to heap nor relink in mcentral lists; // caller takes care of it. /","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.3 mcache 每个 P 拥有一个 mcache，mcache 中保存着具有空闲空间的 mspan，用于分配 object 时，不需要加锁即可从 mspan 分配对象。 有两种 object 走 mcache 分配： tiny object：mcache 还单独使用一个 mspan 进行非指针微小对象的分配。与普通 object 对象分配不同的是，tiny object 不是固定大小分配的，而是通过 mcache 记录其 offset 偏移量，让 tiny object “挤在” 同一个 mspan 中。 normal object：普通大小的 object，会使用 mcache.alloc 进行分配。mcache.alloc 包含 134 个数组项（67 sizeclass * 2），对于每个大小规格的 mspan 有着两个类型： scan：包含指针的对象 noscan：不包含指针的对象，GC 时无需进一步扫描是否引用着其他活跃对象 mcache “永远” 有空闲的 mspan 用于 object 的分配，当 mcache 缓存的 mspan 没有空闲空间时，就会找 mcentral 去申请新的 mspan 用于使用。 数据结构如下（runtime/mcache.go）： // Per-thread (in Go, per-P) cache for small objects. // No locking needed because it is per-thread (per-P). // // mcaches are allocated from non-GC'd memory, so any heap pointers // must be specially handled. type mcache struct { // Allocator cache for tiny objects w/o pointers.  // See \"Tiny allocator\" comment in malloc.go. // tiny points to the beginning of the current tiny block, or  // nil if there is no current tiny block.  //  // tiny is a heap pointer. Since mcache is in non-GC'd memory,  // we handle it by clearing it in releaseAll during mark  // termination.  tiny uintptr tinyoffset uintptr local_tinyallocs uintptr // number of tiny allocs not counted in other stats // The rest is not accessed on every malloc. alloc [numSpanClasses]*mspan // spans to allocate from, indexed by spanClass stackcache [_NumStackOrders]stackfreelist } tiny tinyoffset ：用于小对象（\u003c16）的分配。tiny 指向当前为 tiny object 准备的 span 的起始地址，tinyoffset 指向对象使用的偏移地址； alloc ：最重要的属性，保存着不同大小的 mspan 各一个。目前，包含固定 64 类 sizeclass：0、8 … 32768； 4.3.1 mspan 的分配 先看下 tiny object 分配，在分配一个 object 时，如果大小小于 16 字节时，就会走 tiny object 逻辑。 func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { if size \u003c= maxSmallSize { if noscan \u0026\u0026 size \u003c maxTinySize { off := c.tinyoffset // Align tiny pointer for required (conservative) alignment. if size\u00267 == 0 { off = alignUp(off, 8) } else if size\u00263 == 0 { off = alignUp(off, 4) } else if size\u00261 == 0 { off = alignUp(off, 2) } if off+size \u003c= maxTinySize \u0026\u0026 c.tiny != 0 { // The object fits into existing tiny block. x = unsafe.Pointer(c.tiny + off) c.tinyoffset = off + size c.local_tinyallocs++ mp.mallocing = 0 releasem(mp) return x } // Allocate a new maxTinySize block. span = c.alloc[tinySpanClass] v := nextFreeFast(span) if v == 0 { v, span, shouldhelpgc = c.nextFree(tinySpanClass) } x = unsafe.Pointer(v) (*[2]uint64)(x)[0] = 0 (*[2]uint64)(x)[1] = 0 // See if we need to replace the existing tiny block with the new one // based on amount of remaining free space. if size \u003c c.tinyoffset || c.tiny == 0 { c.tiny = uintptr(x) c.tinyoffset = size } size = maxTinySize } else { ... } } else { ... } } 如果当前 tinyoffset+size \u003c 16B（因为每个 tiny span 的大小为 32B，而每个 tiny object 最大为 16，所以比较 16 即可），那么表明当前的 tiny span 肯定可以放下，那么移动 c.tinyoffset 偏移即可； 如果没有，那么就需要重新申请 tinySpanClass=5 的 span（32B），并替换当前 c.tiny 与 c.tinyoffset（当前 object 可以放在老的 span 或者新的 span）。 接着看下普通大小的 object 的分配，在外层函数计算好 spanClass 后，就会调用 nextFree() 函数（runtime/malloc.go）： // nextFree returns the next free object from the cached span if one is available. // Otherwise it refills the cache with a span with an available object and // returns that object along with a flag indicating that this was a heavy // weight allocation. If it is a heavy weight allocation the caller must // determine whether a new GC cycle needs to be started or if the GC is active // whether this goroutine needs to assist the GC. // // Must run in a non-preemptible context since otherwise the owner of // c could change. func (c *mcache) nextFree(spc spanClass) (v gclinkptr, s *mspan, shouldhelpgc bool) { s = c.alloc[spc] shouldhelpgc = false freeIndex := s.nextFreeIndex() if freeIndex == s.nelems { // The span is full.  if uintptr(s.allocCount) != s.nelems { println(\"runtime: s.allocCount=\", s.allocCount, \"s.nelems=\", s.nelems) throw(\"s.allocCount != s.nelems \u0026\u0026 freeIndex == s.nelems\") } c.refill(spc) shouldhelpgc = true s = c.alloc[spc] freeIndex = s.nextFreeIndex() } if freeIndex \u003e= s.nele","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:3","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.4 mcentral mcentral 是内存分配器的中心缓存，用于给 mcache 提供空闲的 mspan。因为不是 P 对应的，所以访问也需要锁。 mheap 会创建 64 个 sizeClass 的 mcentral，每个 mcentral 管理相同大小的所有 mspan，以两个链表结构管理： nonempty ：包含空闲空间的 mspan 组成的链表； empty ：不包含空闲空间，或者被 mcache 申请的 mspan 组成的链表（判断是否被 mcache 使用是通过 mspan.sweepgen 属性来判断）； 当 mcache 要申请某大小的 mspan 时，会回去指定大小的 mcentral 实例上申请。 数据结构如下： // Central list of free objects of a given size. // //go:notinheap type mcentral struct { lock mutex spanclass spanClass // For !go115NewMCentralImpl. nonempty mSpanList // list of spans with a free object, ie a nonempty free list empty mSpanList // list of spans with no free objects (or cached in an mcache) … } lock ：访问需要加的锁； spanclass ：当前 mcentral 管理的 mspan 大小； nonempty ：包含空闲空间的 mspan 链表； empty ：不包含空闲空间，或者被 mcache 申请了的 mspan 链表； Note 源码中存在 go115NewMCentralImpl 的注释，对 mcentral 结构做了很大的改动，但是在 go1.15 release 页面上并没有看到对应的说明。 其 commit 见：runtime: add new mcentral implementation 4.4.1 从 mcentral 申请 mspan 在 [mcache 的获取](#432-mspan-的获取） 中，可以看到 mcache 通过调用 mcentral.cacheSpan() 申请新的空闲 mspan。在 go1.15 中，因为有新版 mcentral 的实现，因此双链表方式移动到了 mcentral.oldCacheSpan() 方法中。 // Allocate a span to use in an mcache. func (c *mcentral) cacheSpan() *mspan { if !go115NewMCentralImpl { return c.oldCacheSpan() } … } // Allocate a span to use in an mcache. // // For !go115NewMCentralImpl. func (c *mcentral) oldCacheSpan() *mspan { lock(\u0026c.lock) sg := mheap_.sweepgen retry: var s *mspan // 走 nonempty 链表找 for s = c.nonempty.first; s != nil; s = s.next { if s.sweepgen == sg-2 \u0026\u0026 atomic.Cas(\u0026s.sweepgen, sg-2, sg-1) { c.nonempty.remove(s) c.empty.insertBack(s) unlock(\u0026c.lock) s.sweep(true) goto havespan } if s.sweepgen == sg-1 { // the span is being swept by background sweeper, skip continue } // we have a nonempty span that does not require sweeping, allocate from it c.nonempty.remove(s) c.empty.insertBack(s) unlock(\u0026c.lock) goto havespan } // 走 empty 链表找 for s = c.empty.first; s != nil; s = s.next { if s.sweepgen == sg-2 \u0026\u0026 atomic.Cas(\u0026s.sweepgen, sg-2, sg-1) { // we have an empty span that requires sweeping, // sweep it and see if we can free some space in it c.empty.remove(s) // swept spans are at the end of the list c.empty.insertBack(s) unlock(\u0026c.lock) s.sweep(true) freeIndex := s.nextFreeIndex() if freeIndex != s.nelems { s.freeindex = freeIndex goto havespan } lock(\u0026c.lock) // the span is still empty after sweep // it is already in the empty list, so just retry goto retry } if s.sweepgen == sg-1 { // the span is being swept by background sweeper, skip continue } // already swept empty span, // all subsequent ones must also be either swept or in process of sweeping break } unlock(\u0026c.lock) // 向 heap 申请新的 mspan // Replenish central list if empty. s = c.grow() if s == nil { return nil } lock(\u0026c.lock) c.empty.insertBack(s) unlock(\u0026c.lock) // At this point s is a non-empty span, queued at the end of the empty list, // c is unlocked. havespan: … freeByteBase := s.freeindex \u0026^ (64 - 1) whichByte := freeByteBase / 8 // Init alloc bits cache. s.refillAllocCache(whichByte) // Adjust the allocCache so that s.freeindex corresponds to the low bit in // s.allocCache. s.allocCache \u003e\u003e= s.freeindex % 64 return s } 上面逻辑可以大致分为几个步骤： 遍历 nonempty 链表，找到可用的 mspan（对于需要 sweep 的 mspan 先进行 sweep）； 没找到，遍历 empty 链表，仅仅遍历需要 sweep 的 mspan，执行 sweep 并判断是否可用； 还是没有，通过 mcentral.grow() 向 mheap 申请新的 mspan，mheap 中都没有，return nil； 找到空闲 mspan 后，会放置到 empty 链表尾部，并返回； 4.4.2 mcentral 扩容 在 mcentral 没有任何空闲 mspan 给 mcache 时，就会调用 mcentral.grow() 申请新的 mspan。 // grow allocates a new empty span from the heap and initializes it for c's size class. func (c *mcentral) grow() *mspan { npages := uintptr(class_to_allocnpages[c.spanclass.sizeclass()]) size := uintptr(class_to_size[c.spanclass.sizeclass()]) s := mheap_.alloc(npages, c.spanclass, true) if s == nil { return nil } // Use division by multiplication and shifts to quickly compute: // n := (npages \u003c\u003c _PageShift) / size n := (npages \u003c\u003c _PageShift) \u003e\u003e s.divShift * uintptr(s.divMul) \u003e\u003e s.divShift2 s.limit = s.base() + size*n heapBitsForAddr(s.b","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:4","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.5 mheap mheap 是最核心的组件了，runtime 只存在一个 mheap 对象，分配、初始化 mspan 都从 mheap 开始。 mheap 直接与虚拟内存打交道，并将在虚拟内存上创建 mspan 提供给上层使用。 mheap 的功能可以看做两个方面： 与下层（虚拟内存）：内存管理（类似文件系统）。申请虚拟内存得到多个 heaparena，每个 heaparena 将可用内存区域切分为 page 单元，以倍数组成 mspan 分配给上层； 与上层：提供创建 mspan 的接口。通过 mcentral 分类不同大小的 mspan，或者大内存需要直接走 mspan 分配； 其数据结构很大，省略了部分不会提到的属性（runtime/mheap.go），mheap_ 就是 heap 的单实例对象： var mheap_ mheap // Main malloc heap. // The heap itself is the \"free\" and \"scav\" treaps, // but all the other global data is here too. // // mheap must not be heap-allocated because it contains mSpanLists, // which must not be heap-allocated. // //go:notinheap type mheap struct { // arenas is the heap arena map. It points to the metadata for // the heap for every arena frame of the entire usable virtual // address space. // // Use arenaIndex to compute indexes into this array. // // For regions of the address space that are not backed by the // Go heap, the arena map contains nil. // // Modifications are protected by mheap_.lock. Reads can be // performed without locking; however, a given entry can // transition from nil to non-nil at any time when the lock // isn't held. (Entries never transitions back to nil.) // // In general, this is a two-level mapping consisting of an L1 // map and possibly many L2 maps. This saves space when there // are a huge number of arena frames. However, on many // platforms (even 64-bit), arenaL1Bits is 0, making this // effectively a single-level map. In this case, arenas[0] // will never be nil. arenas [1 \u003c\u003c arenaL1Bits]*[1 \u003c\u003c arenaL2Bits]*heapArena // central free lists for small size classes. // the padding makes sure that the mcentrals are // spaced CacheLinePadSize bytes apart, so that each mcentral.lock // gets its own cache line. // central is indexed by spanClass. central [numSpanClasses]struct { mcentral mcentral pad [cpu.CacheLinePadSize - unsafe.Sizeof(mcentral{})%cpu.CacheLinePadSize]byte } pages pageAlloc // page allocation data structure spanalloc fixalloc // allocator for span* cachealloc fixalloc // allocator for mcache* specialfinalizeralloc fixalloc // allocator for specialfinalizer* specialprofilealloc fixalloc // allocator for specialprofile* speciallock mutex // lock for special record allocators. arenaHintAlloc fixalloc // allocator for arenaHints } arenas ：内存管理的元信息数组，对于虚拟内存的逻辑切割与管理就靠这个数组了； central ：按照大小分类的各个 mcentral 对象； pages ：在 arena 区域上用于分配空闲的 pages，依旧使用空闲链表； spanalloc、cachealloc 等 ：各个数据结构的空闲链表分配器，通过连接空闲的 mspan、mcache 等对象，调用 fixalloc.alloc() 函数就获取下一个空闲的内存空间； 4.5.1 虚拟内存布局 网上大部分文章还是说 mheap 管理的虚拟内存以 spans+bitmap+arena 管理，如下图： 但是从 go 1.11 开始，Go 开始使稀疏内存方式管理，即管理相互之间不连续的连续的内存区域，如下图（图片来自 《Golang 设计与实现》）： 使用的就是 mheap.arenas，一个二维的 heapArena 数组。 不同平台的 heapArena 管理的 arena 大小不同，在 Linux 64bit 平台下，每个 heapArena 管理着 64MB 的 arena 内存区域。 // Currently, we balance these as follows: // // Platform Addr bits Arena size L1 entries L2 entries // -------------- --------- ---------- ---------- ----------- // */64-bit 48 64MB 1 4M (32MB) // windows/64-bit 48 4MB 64 1M (8MB) // */32-bit 32 4MB 1 1024 (4KB) // */mips(le) 31 4MB 1 512 (2KB) Note 这里不太好理解，但是我觉可以简单理解就是，将原来的 spans+bitmap+arena 管理方式，变为了多个 spans+bitmap+arena 实现。而不同 arena 之间的地址不是连续的。 但是为什么要用二维数组？目前不知道，但是 Linux x86-64 架构上一维数组大小为 1，就是相当于 1 维数组。 heapArena 数据结构如下（runtime/mheap.go）： // A heapArena stores metadata for a heap arena. heapArenas are stored // outside of the Go heap and accessed via the mheap_.arenas index. // //go:notinheap type heapArena struct { // bitmap stores the pointer/scalar bitmap for the words in // this arena. See mbitmap.go for a description. Use the // heapBits type to access this. bitmap [heapArenaBitmapBytes]byte // spans maps from virtual address page ID within this arena to *mspan. // For allocated spans, their pages map to the span itself. // For free spans, only the lowest and highest pages map to the span itself. // Internal pages map to an arbitrary span. // For pages that have never been allocated, spans entries are nil. // // Modifications are protected by m","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:5","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"4.6 总结 粗略地看完整个内存模型后，大概内存的结构如下： 其中比较核心的就是：内存被切分为 page，多个 page 组成不同大小的 mspan，而在 mspan 上又分割为固定大小的 object。 而上层的 mcache、mcentral 只是以不同的方式组织 mspan，通过多级缓存的思想，使得并发的获取一个可用的 mspan 更快。 mcache 将一部分 mspan 独立于 P 所有，使得不需要加锁既可以获取 mspan； mcentral 以大小来分类 mspan，将各个大小的 mspan 请求独立，缩小了锁的粒度； mheap 作为最底层，就好像文件系统一样，管理着整个内存分配的骨架。而与上层的交互就是靠 mspan 作为单位。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:4:6","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"5 对象分配流程 前面一直提到的，对象的分配分为三类： tiny object (0, 16B): 使用 tiny allocator 分配，使用 mcahe 一个独立的 mspan，挤压式的； object [16B, 32KB]: 使用 mcache 分配； large object (32KB, +∞): 直接通过 mheap 分配； 所有的分配逻辑在 mallocgc() 开始分叉，下面分别看下具体的分配代码。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:5:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"5.1 tiny object 分配 tiny object 分配的代码在 mspan 分配中已经说明了，这里再理一下大致步骤： 不包含指针 (noscan) 并且小于 16B 的对象才走微小对象分配； tiny object 分配仅仅是增大 mcache.tinyoffset 的值，所以是不同大小 tiny object 挤压在一个 mspan 中； 如果当前的 mspan 没有空间了，通过 mcache.nextFree() 来获取新的指定大小的 mspan，而获取的流程就是前面所说的（走 mcentral-\u003emheap); ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:5:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"5.2 object 分配 普通大小 object 分配流程就很简单了： func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { if size \u003c= maxSmallSize { if noscan \u0026\u0026 size \u003c maxTinySize { ... } else { var sizeclass uint8 if size \u003c= smallSizeMax-8 { sizeclass = size_to_class8[divRoundUp(size, smallSizeDiv)] } else { sizeclass = size_to_class128[divRoundUp(size-smallSizeMax, largeSizeDiv)] } size = uintptr(class_to_size[sizeclass]) spc := makeSpanClass(sizeclass, noscan) span = c.alloc[spc] v := nextFreeFast(span) if v == 0 { v, span, shouldhelpgc = c.nextFree(spc) } x = unsafe.Pointer(v) if needzero \u0026\u0026 span.needzero != 0 { memclrNoHeapPointers(unsafe.Pointer(v), size) } } } else { ... } } 计算出对应的 sizeclass； 从 mcache.alloc[] 得到对应的 mspan。如果没有，通过 nextFree() 申请； 调用 memclrNoHeapPointers() 清理空闲内存中所有数据； ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:5:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"5.3 large object 分配 func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { if size \u003c= maxSmallSize { if noscan \u0026\u0026 size \u003c maxTinySize { ... } else { ... } } else { shouldhelpgc = true systemstack(func() { span = largeAlloc(size, needzero, noscan) }) span.freeindex = 1 span.allocCount = 1 x = unsafe.Pointer(span.base()) size = span.elemsize } } func largeAlloc(size uintptr, needzero bool, noscan bool) *mspan { // print(\"largeAlloc size=\", size, \"\\n\") if size+_PageSize \u003c size { throw(\"out of memory\") } npages := size \u003e\u003e _PageShift if size\u0026_PageMask != 0 { npages++ } // Deduct credit for this span allocation and sweep if // necessary. mHeap_Alloc will also sweep npages, so this only // pays the debt down to npage pages. deductSweepCredit(npages*_PageSize, npages) spc := makeSpanClass(0, noscan) s := mheap_.alloc(npages, spc, needzero) if s == nil { throw(\"out of memory\") } if go115NewMCentralImpl { // Put the large span in the mcentral swept list so that it's // visible to the background sweeper. mheap_.central[spc].mcentral.fullSwept(mheap_.sweepgen).push(s) } s.limit = s.base() + size heapBitsForAddr(s.base()).initSpan(s) return s } large object 会切换到系统栈，然后走 mheap 申请； 计算对象需要的 page 数量，然后调用 mheap.alloc() 申请空闲的 mspan； 而 mheap.alloc() 就是 mcentral 申请 mspan 的方法。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:5:3","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"6 内存的释放 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:6:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"6.1 释放操作 前面 4.5.4 mheap 回收 mspan 中看到，mheap 不会真正的释放内存，而是等待其被复用。但是不可能一直扩展内存，而不释放。 释放内存由 mheap.page 的 pageAlloc.scavenge() 函数负责（runtime/mgcscavenge.go）: // scavenge scavenges nbytes worth of free pages, starting with the // highest address first. Successive calls continue from where it left // off until the heap is exhausted. Call scavengeStartGen to bring it // back to the top of the heap. // // Returns the amount of memory scavenged in bytes. // // s.mheapLock must be held, but may be temporarily released if // mayUnlock == true. // // Must run on the system stack because s.mheapLock must be held. // //go:systemstack func (s *pageAlloc) scavenge(nbytes uintptr, mayUnlock bool) uintptr { var ( addrs addrRange gen uint32 ) released := uintptr(0) for released \u003c nbytes { if addrs.size() == 0 { // 通过标记选出一部分需要释放的内存区域 if addrs, gen = s.scavengeReserve(); addrs.size() == 0 { break } } // 释放内存 r, a := s.scavengeOne(addrs, nbytes-released, mayUnlock) released += r addrs = a } // Only unreserve the space which hasn't been scavenged or searched // to ensure we always make progress. s.scavengeUnreserve(addrs, gen) return released } 释放的流程比较复杂，没有研究过看不懂，目前知道下最后会调用 sysUnused() 函数释放（runtime/mem_linux.go）： func sysUnused(v unsafe.Pointer, n uintptr) { // huge page 处理 ... var advise uint32 if debug.madvdontneed != 0 { advise = _MADV_DONTNEED } else { advise = atomic.Load(\u0026adviseUnused) } if errno := madvise(v, n, int32(advise)); advise == _MADV_FREE \u0026\u0026 errno != 0 { // MADV_FREE was added in Linux 4.5. Fall back to MADV_DONTNEED if it is // not supported. atomic.Store(\u0026adviseUnused, _MADV_DONTNEED) madvise(v, n, _MADV_DONTNEED) } } 通过系统调用 madvise() 告知操作系统某段内存不适用，建议内核回收对应物理内存。 当然，内核在物理内存充足情况下可能不会实际回收内存，以减少无谓的回收消耗。 而当再次使用此内存块时，会引发缺页异常，内核会自动重新关联物理内存页。 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:6:1","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"6.2 释放时机 scavenge() 有两个地方会被调用： 周期性的触发（每 5 min?）； mheap 扩容时 或者 调用 runtime/debug.FreeOSMemory() 主动触发； ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:6:2","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Golang"],"content":"参考 《Golang 学习笔记》 Blog：Go 内存管理可视化 《Golang 设计与实现》：内存分配器 知乎：图解 Go 语言内存分配 ","date":"2021-01-05","objectID":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/:7:0","tags":["Golang","Golang 原理"],"title":"Go 内存管理总结","uri":"/posts/language/golang/go-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["VM"],"content":"总结 KVM 虚拟机使用存储与网络的方式","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文主要总结一些遇到的虚拟机使用存储与网络的方式，挖了许多坑，不断补充中。 下面所有的虚拟机启动都使用 libvirt，通过修改其配置文件来设置网络或者存储，省略了虚拟机的启动、停止等操作命令。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:0:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1 virtio 概述 KVM 在 IO 虚拟化方面，传统的方式是使用 QEMU 纯软件方式模拟网卡、磁盘。 KVM 中，可以在客户机使用 半虚拟化驱动Paravirtualized Drivers 来提高客户机性能。 目前，采用的是 virtio 这个设备驱动标准框架。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1.1 全模拟 I/O 设备基本原理 纯软件模拟 I/O 如下图所示： 客户机中设备驱动程序（Device Driver）发起 I/O 操作请求时，KVM 内核模块会拦截这次请求，然后将请求信息放到 I/O 共享页（sharing page）； KVM 模块通知用户空间 QEMU 程序（客户机对应 qemu 进程的一个线程）有 I/O 请求； QEMU 模拟程序根据 I/O 请求信息，交由硬件模拟代码（Emulation Code）模拟 I/O 请求，阻塞等待 I/O 结果； 如果是普通的 I/O 请求完成后，将结果放回到 I/O 共享页； 如果客户机是通过 DMA 访问大块 I/O 时，QEMU 模拟程序会直接通过内存映射的将结果直接写到客户机内存中，并通知 KVM 模块客户机 DMA 操作已经完成； KVM 内核模块读取 I/O 共享页中结果，将结果返回给客户机； 或者，接受到 QEMU 通知，通知客户机 DMA 操作已经完成； QEMU 模拟 I/O 设备的优点：可以通过软件模拟出各种设备，兼容性好。但是也有明显缺点：每次 I/O 操作比较长，有较多的 VMEntry、VMExit 发生，需要多次上下文切换与数据复制，性能很差。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:1","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1.2 virtio 基本原理 virtio 是一个在 Hypervisor 上抽象 API 接口，让客户机知道运行与虚拟化环境中，进而根据 virtio 标准与 Hypervisor 协作，从而在客户机达到更好性能。 其 virtio 基本结构如下： 前端驱动fronded 是在客户机中的驱动模块，如 virtio-blk、virtio-net 等； 后端处理程序backend 在 QEMU 中实现，执行具体的 I/O 操作； virtio 是虚拟队列接口，在逻辑上将前端驱动附加到后端处理程序，虚拟队列实际上被实现为跨越客户机操作系统和 Hypervisor 的衔接点，该衔接点可以用任意方式实现，前提是前后端都遵循标准实现。 一个前端驱动可以有 0 或多个队列，例如 virtio-net 使用两个虚拟队列（接受+发送）。 virtio-ring 实现了环形缓冲区（ring buffer），用于保存前端驱动和后端处理程序执行的信息。 环形缓冲区可以一次性保存前端驱动多个 I/O 请求，并交由后端驱动批量处理，最后实际调用宿主机设备驱动实现的设备 I/O 操作，以提升效率。 virtio 的性能很好，一般都推荐使用 virtio。但是，virtio 要求客户机必须安装特定的 virtio 驱动，并且按照 virtio 规定格式传输数据，一些老的 Linux 系统可能不支持。 在客户机中查看是否存在 virtio 相关内核模块，以确定系统是否支持 virtio： [root@kvm-guest ~]# lsmod | grep virtio virtio_net 28024 0 virtio_pci 22913 0 virtio_ring 21524 2 virtio_net,virtio_pci virtio 15008 2 virtio_net,virtio_pci 其中，virtio、virtio_ring、virtio_pci 等驱动提供对 virtio API 支持，任何 virtio 前端驱动都必须使用。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:2","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1.3 vhost 正如 前面所述，virtio 分为前后端，而后端默认为用户空间的 QEMU 进程。由 QEMU 进程模拟虚拟机的命令，在将结果返回给虚拟机。可以看到，这里存在着一次 用户空间至内核空间的转换。 而 vhost 出现就是用于优化这一次的转换。vhost 是宿主机中的一个内核模块，用于和虚拟机直接进行通信，也是通过 virtio 提供的数据队列进行通信。 目前，网络支持有 vhost-net，块设备支持有 vhost-blk，它们都依赖于基础的内核模块 vhost。 $ lsmod | grep vhost vhost_net 28672 1 vhost 53248 1 vhost_net vhost_iotlb 16384 1 vhost ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:3","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"1.4 总结 首先，需要明白 virtio 为什么比纯模拟的方式快？ 最主要因为纯模拟的方式存在多次内核 KVM 与用户空间 QEMU 进程的数据交互，简单点说，客户机需要 KVM 进行数据的中转。 而 virtio 消除了 KVM 进行数据的中转，通过一套标准框架实现虚拟机与 QEMU 进程的直接信息交互。也是因此，虚拟机需要使用特殊的 virtio 相关的驱动，也就知道了自己处于虚拟化环境，所以是半虚拟化。 其次，需要明确，virtio 由前后端组成，其中后端是由 QEMU 实现的，目前主流的 QEMU 版本都实现了，不需要 care。而需要注意的是，virtio 的前端是要在虚拟机中满足，也就是相关的 virtio 驱动，这在使用时需要进行确认。 而 virtio 的通信方式中，还存在 QEMU 模拟命令执行这一个优化点，因此，vhost 出现优化了这一次的用户空间至内核空间的切换。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:1:4","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"2 设备直接分配 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:2:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"2.1 PCI 设备直接分配 PCI 设备直接分配 允许将宿主机的物理 PCI（或 PCI-E）设备直接分配给客户机完全使用。 设备直接分配相当于虚拟机直接使用硬件设备，也就没有了 QEMU 进程的模拟，因此速度最快。 但是，设备直接分配有一些条件： 硬件平台需要支持 Intel 硬件支持设备直接分配规范为 “Intel(R)Virtualization Technology for Directed I/O”（VT-d）或者 AMD 的规范为 “AMD-Vi”（也叫作 IOMMU）。 目前市面上的 x86 硬件平台基本都支持 VT-d，在 BIOS 中设置打开 VT-d 特性。 内核配置支持相关 VT-d 的特性，并且加载内核模块 vfio-pci（内核 \u003e= 3.10）。 内核启动参数需要打开 intel_iommu=on（intel 平台） $ modprobe vfio_pci $ lsmod | grep vfio vfio_pci 53248 0 vfio_virqfd 16384 1 vfio_pci vfio_iommu_type1 32768 0 vfio 36864 2 vfio_iommu_type1,vfio_pci irqbypass 16384 8 vfio_pci,kvm ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:2:1","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"2.2 SR-IOV VT-d 技术只能讲一个物理设备分配给一个客户机使用，为了让多个虚拟机共享同一个物理设备，PCI_SIG 组织发布了 SR-IOVSingle Root I/O Virtualizaiton and Sharing 规范，该规范定义了标准化机制，用以原生的支持多个共享的设备（不一定网卡）。 目前，SR-IOV 最广泛应用在以太网设备的虚拟化方面。 SR-IOV 引入了两个新的 function： PF 物理功能Physical Function ：PF 是一个普通的 PCI-e 设备（带有 SR-IOV 功能），在宿主机中配置和管理其他 VF，本身也可以作为独立 function 使用； VF 虚拟功能Virtual Function ：由 PF 衍生的 “轻量级” PCI-e 功能，可以分配到客户机中作为独立 function 使用； SR-IOV 为客户机中使用的 VF 提供了独立的内存空间、中断、DMA 流，从而不需要 Hypervisor 介入数据传输。 一个具有 SR-IOV 功能的设备能够被配置为 PCI 配置空间中呈现出多个 Function（1 个 PF + 多个 VF），每个 VF 有独立的配置空间和完整的 BAR（Base Address Register，基址寄存器）。 Hypervisor 通过将 VF 实际的配置空间映射到客户机看到的配置空间的方式，将一个或多个 VF 分配给一个客户机。并且，通过 Intel VT-x 和 VT-d 等硬件辅助虚拟化技术提供的内存转换技术，允许直接的 DMA 传输去往或来自一个客户机，因此几乎不需要 Hypervisor 的参与。 在客户机中看到的 VF，表现给客户机操作系统的就是一个完整的普通的设备。 SR-IOV 需要硬件平台支持 Intel VT-x 和 VT-d（或 AMD 的 SVM 和 IOMMU）虚拟化特性，并且需要具体设备支持 SR-IOV 规范。 在 Linux 中，可以查看 PCI 信息的“Capabilities”项目，以确定设备是否具备 SR-IOV 功能： $ lspci -s 82:00.0 -v 82:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01) Flags: bus master, fast devsel, latency 0, IRQ 38, NUMA node 1 Memory at d0100000 (32-bit, non-prefetchable) [size=1M] I/O ports at c020 [size=32] Memory at d0204000 (32-bit, non-prefetchable) [size=16K] Expansion ROM at d0400000 [disabled] [size=1M] Capabilities: [40] Power Management version 3 Capabilities: [50] MSI: Enable- Count=1/1 Maskable+ 64bit+ Capabilities: [70] MSI-X: Enable+ Count=10 Masked- Capabilities: [a0] Express Endpoint, MSI 00 Capabilities: [100] Advanced Error Reporting Capabilities: [140] Device Serial Number a4-dc-be-ff-ff-17-8d-52 Capabilities: [150] Alternative Routing-ID Interpretation (ARI) Capabilities: [160] Single Root I/O Virtualization (SR-IOV) Capabilities: [1a0] Transaction Processing Hints Capabilities: [1c0] Latency Tolerance Reporting Capabilities: [1d0] Access Control Services Kernel driver in use: igb Kernel modules: igb “Capabilities: [160] Single Root I/O Virtualization (SR-IOV)\" 表示网卡支持 SR-IOV 功能 具体使用方式见网络模式中的示例。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:2:2","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3 存储模式 下面的磁盘测试都是以 4k 随机读写测试，命令如下： fio -thread -name=${DISK} -filename=${DISK} \\ -ioengine=libaio -direct=1 -bs=4k -rw=randrw -iodepth=32 \\ -size=8G -rw=readrw 说明，测试仅仅适用于简单对比各个模式之间性能差异，而不是标准的基准测试，不能作为靠谱数据。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.1 纯模拟 纯模拟是最简单的方式，所有的 IO 请求发送给 QEMU，由 QEMU 在宿主机执行后将结果返回给虚拟机中（见 1.1）。 创建一个 qcow2 文件，并将其以 sata 驱动方式提供给虚拟机： … \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/root/disk1.qcow2'/\u003e \u003ctarget dev='sda' bus='sata'/\u003e \u003caddress type='drive' controller='0' bus='0' target='0' unit='0'/\u003e \u003c/disk\u003e … 在虚拟机中，可以看到对应的磁盘与对应的驱动： [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 500G 0 disk vda 253:0 0 500G 0 disk └─vda1 253:1 0 500G 0 part / [root@localhost ~]# lspci … 00:09.0 SATA controller: Intel Corporation 82801IR/IO/IH (ICH9R/DO/DH) 6 port SATA Controller [AHCI mode] (rev 02) 压测结果： read: IOPS=9183, BW=35.9MiB/s (37.6MB/s)(4098MiB/114246msec) write: IOPS=9172, BW=35.8MiB/s (37.6MB/s)(4094MiB/114246msec) ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:1","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.2 virtio-blk virtio-blk 实现了 virtio 标准，在虚拟机中使用 virtio 驱动，加速存储的访问。当然，需要客户机中系统支持 virtio-blk 内核模块。 [root@localhost ~]# lsmod | grep blk virtio_blk 18323 2 目前，virtio-blk 可用于宿主机文件、裸设备、LVM 设备，挂载到虚拟机后命名为 vdx。 将 \u003cdisk\u003e.\u003ctarget\u003e 中的 bus 改为 virtio，就是使用 virtio-blk 方式。 … \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/root/disk1.qcow2'/\u003e \u003ctarget dev='sda' bus='virtio'/\u003e \u003caddress type='pci' domain='0x0000' bus='0x00' slot='0x0a' function='0x0'/\u003e \u003c/disk\u003e … 不过使用 virtio 驱动，设置的 dev 名称会失效，自动使用 vdx 这种命名方式。在虚拟机中看到对应的磁盘以及驱动： [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 253:0 0 500G 0 disk └─vda1 253:1 0 500G 0 part / vdb 253:16 0 500G 0 disk [root@localhost ~]# lspci … 00:0a.0 SCSI storage controller: Red Hat, Inc. Virtio block device 压测结果，可以看到，比纯模拟要快很多： read: IOPS=41.7k, BW=163MiB/s (171MB/s)(4098MiB/25145msec) write: IOPS=41.7k, BW=163MiB/s (171MB/s)(4094MiB/25145msec) virtio-blk 虽然提供了很高的存储访问性能，但是其设计上也有着一些缺点： virtio blk 的范围有限，这使得新的命令实现变得复杂。每次开发一个新命令时，virtio blk 驱动程序都必须在每个客户机中更新 virtio blk 将 PCI 功能和存储设备映射为 1:1，一个映射就需要占用虚拟机一个 PCI 地址，限制了可扩展性。 virtio blk 不是真正的 SCSI 设备。这会导致一些应用程序在从物理机移动到虚拟机时中断。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:2","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.3 virtio-scsi virito-scsi 有着与 virtio-blk 相同的性能，但是改善了 virtio-blk 的相关缺点，其中最大的优势就是 virtio-scsi 可以允许虚拟机处理数百个设备，而 virtio-blk 只能处理大约 30 个设备就会耗尽 PCI 插槽。 因为 virtio-scsi 在虚拟机里使用的是 scsi 驱动，因此设备的命名也变为了 sdx。 将磁盘的驱动设置为 scsi，同时将 scsi 驱动设置为使用 virtio-scsi（默认使用的驱动是 lsi）： … \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/root/disk1.qcow2'/\u003e \u003ctarget dev='sdc' bus='scsi'/\u003e \u003caddress type='drive' controller='0' bus='0' target='0' unit='2'/\u003e \u003c/disk\u003e \u003ccontroller type='scsi' index='0' model='virtio-scsi'/\u003e … 可以看到，\u003caddress\u003e 中设置相关设备地址时，可以看到不是设置 pci 槽地址，而是设置设备对应 SCSI 映射地址，因此通过增加 “target” 属性就可以添加多个硬盘，不需要占用多个 PCI 槽； 虚拟机中看到对应的块设备 “sda”，以及驱动 “Virtio SCSI”： [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 500G 0 disk vda 253:0 0 500G 0 disk └─vda1 253:1 0 500G 0 part / [root@localhost ~]# lspci … 00:08.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI 压测结果： read: IOPS=45.5k, BW=178MiB/s (186MB/s)(4098MiB/23055msec) write: IOPS=46.2k, BW=180MiB/s (189MB/s)(4094MiB/22695msec) 目前，virtio-scsi 还存在一种设备直通的模式，IO 性能更高（具体实现原理还不了解）。但是，这种模式还能用于块设备，并且是使用 scsi 协议的块设备（测试文件、usb 都不支持）。 通过设置 device=“lun” 使用： ... \u003cdisk type='block' device='lun'\u003e \u003cdriver name='qemu' type='raw'/\u003e \u003csource dev='/dev/sda'/\u003e \u003ctarget dev='sdc' bus='scsi'/\u003e \u003caddress type='drive' controller='0' bus='0' target='0' unit='2'/\u003e \u003c/disk\u003e \u003ccontroller type='scsi' index='0' model='virtio-scsi' /\u003e ... 使用 lun 方式，并指定使用宿主机块设备 sda 虚拟机中看到对应块设备 “sda”，以及驱动 \"” ： [root@localhost ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 465.8G 0 disk vda 253:0 0 500G 0 disk └─vda1 253:1 0 500G 0 part / [root@localhost ~]# lspci … 00:08.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI 发现的问题 在使用 lun 分配磁盘时，发现虚拟机内部对磁盘的修改，在宿主机上是无法可见的（可能出现磁盘文件系统 UUID 都不一致的问题）。并且，在虚拟机内部给磁盘创建文件系统还会出现有报错的情况。 如果是要将 scsi 磁盘单独分给虚拟机，推荐使用设备直接分配中的 scsi 设备分配。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:3","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.4 设备直接分配 3.4.1 PCI 设备 在 2.1 PCI 设备直接分配 中所说，PCI 设备都支持进行设备直接分配，使得虚拟机中直接对 PCI 设备进行访问，速度最快。nvme 磁盘可以使用这种方式。 使用设备直接分配时，需要现在宿主机上取消对应设备的驱动绑定，然后将其分配给虚拟机，然后 libvirt 中配置好相关参数后，会自动帮我们执行这些步骤。 在设备 xml 配置文件中添加 \u003chostdev\u003e 项，指定对应的设备 PCI 地址，来直接分配设备（也可以使用 \u003cinterface type=‘hostdev’/\u003e，这是一种 libvirt 提供的较新的配置方式，但是不兼容所有设备）。 … \u003chostdev mode='subsystem' type='pci' managed='yes'\u003e \u003csource\u003e \u003caddress domain='0x0000' bus='0x08' slot='0x00' function='0x0'/\u003e \u003c/source\u003e \u003c/hostdev\u003e … 3.4.2 SCSI 设备 对于 SCSI 磁盘，通过 PCI 无法单独分配某个磁盘，而 libvirt 中提供了 type=‘scsi’ 类型的直接分配。但是与 PCI 不同的是，scsi 无法设置 ‘managed’ 参数，也就是说宿主机上还是能够看见该 scsi 设备，如果想不可见，需要自己 unbind。 添加 scsi 类型的 \u003chostdev\u003e 项，其中 \u003csource\u003e.\u003caddress\u003e 中指定宿主机对应磁盘的 scsi 地址。 ... \u003chostdev mode='subsystem' type='scsi' managed='no' sgio='filtered' rawio='yes'\u003e \u003csource\u003e \u003cadapter name='scsi_host0'/\u003e \u003caddress bus='2' target='1' unit='0'/\u003e \u003c/source\u003e \u003caddress type='drive' controller='0' bus='0' target='0' unit='2'/\u003e \u003c/hostdev\u003e \u003ccontroller type='scsi' index='0' model='virtio-scsi'/\u003e ... Note 这种模式还是会使用 virtio-scsi 驱动，我不确定这是否属于设备直接分配，但是在文档中其属于 passthrough 的一类 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:4","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"3.5 libvirt 提供的存储模型 TODO ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:3:5","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"4 网络模式 虚拟机网络的构建一般都是需要构建 宿主机网络环境 + 虚拟机虚拟网络设备，得益于 libvirt，一些常用的网络的构建只需要配置好相关配置就行，libvirt 会进行网络的构建。在 libvirt 中，一个可用的网络就称为 netowrk 首先需要明确的，下面所说的网络模式不同的在于如何让宿主机接收的数据包，到达 qemu 进程 或者 vhost 内核模块。而 qemu 进程与 vhost 模块如何将数据包传递到虚拟机中，这是虚拟化方式的问题，即全虚拟化或者半虚拟化。 更简单点说，虚拟机的网络收发有三个点：宿主机 \u003c-\u003e qemu/vhost \u003c-\u003e 虚拟机。而不同网络模式不同点在 宿主机至 qemu/vhost 阶段，而数据包到虚拟机，这就是不同虚拟化的工作了。 在下面的配置中，你会发现网络模式还需要配置 driver 选项，这就是配置具体的虚拟化方式了。但是这不是这里的重点，所以不会有特殊的说明。 libvirt 支持的网络模式有很多，下面仅仅提到我使用过的。 ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:4:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"4.1 虚拟机网络 4.1.1 NAT Mode NAT 网络是最简单的网络，不需要任何的依赖。通过虚拟的 bridge 网卡创建一个属于虚拟机的内网，然后在宿主机上通过 iptables 实现内网地址的 NAT。（没错，这和 docker bridge network 的原理一样） libvirt 会存在一个名为 default 的 network，其就是一个 NAT 网络。通过 virsh net-list 查看： $ virsh net-list Name State Autostart Persistent -------------------------------------------- default active yes yes 其默认会被 active，也就是说宿主机环境在 libvirtd 启动后就会构建好，你可以看到对应的 bridge 网卡。 $ ip a ... 3: virbr0: \u003cNO-CARRIER,BROADCAST,MULTICAST,UP\u003e mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:12:34:56 brd ff:ff:ff:ff:ff:ff inet 172.27.0.1/16 brd 172.27.255.255 scope global galaxybr0 valid_lft forever preferred_lft forever ... 默认下，default 网络的内网为 192.168.122.1/24，我通过修改其配置文件 /etc/libvirt/qemu/networks/default.xml 修改了内网范围： $ cat /etc/libvirt/qemu/networks/default.xml \u003cnetwork\u003e \u003cname\u003edefault\u003c/name\u003e \u003cuuid\u003e341adece-b07a-4eb0-92f2-d92f59ed266f\u003c/uuid\u003e \u003cforward mode='nat'/\u003e \u003cbridge name='virbr0' stp='on' delay='0'/\u003e \u003cmac address='52:54:00:12:34:56'/\u003e \u003cip address='172.27.0.1' netmask='255.255.0.0'\u003e \u003cdhcp\u003e \u003crange start='172.27.0.2' end='172.27.255.254'/\u003e \u003c/dhcp\u003e \u003c/ip\u003e \u003c/network\u003e libvirtd 会为启动的 nat 网络启动一个 dnsmasq 进程，用于提供 DNS 与 DHCP 功能，其对应配置就是对应网络的配置 $ ps x | grep dns 2185 ? S 0:00 /usr/sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper $ cat /var/lib/libvirt/dnsmasq/default.conf strict-order pid-file=/var/run/libvirt/network/default.pid except-interface=lo bind-dynamic interface=virbr0 dhcp-range=172.27.0.2,172.27.255.254,255.255.0.0 dhcp-no-override dhcp-authoritative dhcp-lease-max=65533 dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts Note 这里与 docker bridge network 不同点，因为 docker 中仅仅是 namespace 隔离，容器内网卡 IP 还是可以由 docker 进入 namespace 分配，所以是 docker 承担了 dhcp 服务器的职责。 但是虚拟机和宿主机是强隔离的，因此需要虚拟机启动后去作为 dhcpclient 申请地址，因此需要一个 dhcp 服务，这就是 dnsmasq 的作用。 在 xml 配置文件中设置 \u003cinterface type=‘network’\u003e 项，并设置 \u003csource network=‘default’/\u003e 使用 default network。 \u003cinferface type='network'\u003e \u003cmac address='50:54:00:87:bc:c3'/\u003e \u003csource network='default'/\u003e \u003cmodel type='virtio'/\u003e \u003cdriver name='vhost' txmode='iothread' ioeventfd='on' event_idx='off' queues='16'\u003e \u003chost csum='off' gso='off' tso4='off' tso6='off' ecn='off' ufo='off' mrg_rxbuf='off'/\u003e \u003cguest csum='off' tso4='off' tso6='off' ecn='off' ufo='off'/\u003e \u003c/driver\u003e \u003c/interface\u003e 启动虚拟机后，可以看到虚拟机内部存在对应的网卡，通过 dhclient -v eth0 命令申请一个 IP，广播的 dhcp request 会被宿主机 dnsmasp 进程响应，并返回 dhcp offer。 [root@localhost ~]# ip a ... 2: eth0: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 50:54:00:87:bc:c3 brd ff:ff:ff:ff:ff:ff [root@localhost ~]# dhclient -v eth0 ... Listening on LPF/eth0/50:54:00:87:bc:c3 Sending on LPF/eth0/50:54:00:87:bc:c3 Sending on Socket/fallback DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 4 (xid=0x57740476) DHCPREQUEST on eth0 to 255.255.255.255 port 67 (xid=0x57740476) DHCPOFFER from 172.27.0.1 DHCPACK from 172.27.0.1 (xid=0x57740476) bound to 172.27.163.138 -- renewal in 1653 seconds. [root@localhost ~]# ping www.baidu.com PING www.a.shifen.com (14.215.177.38) 56(84) bytes of data. 64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=1 ttl=48 time=7.93 ms ... 4.1.2 Routed Mode TODO ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:4:1","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"4.2 共享物理设备网络 4.2.1 Bridge TODO 4.2.2 Macvtap 在 docker 的网络中，存在着 macvlan 网络，其在宿主机的物理网卡上创建 macvlan 设备，使得在二层上构建多个接口，从而让多个容器处于与宿主机同一个局域网内。在虚拟机网络中，单单 macvlan 网卡无法连接虚拟机设备与宿主机网卡，还需要一个 tap 设备连接，将 macvlan + tap 组合就是 macvtap 设备。 与 macvlan 设备相同，macvtap 同样存在：private、vepa、bridge、passthru 四种模式，下面的示例中都以 bridge 模式为例。 在 libvirt 中，当 interface type 为 direct 时，表明设备是直接附加到物理网卡上，而就会使用 macvtap 构建网络。通过 \u003csource dev=xx mode=xx\u003e 指定附加的物理网卡，以及 macvtap 的模式。 ... \u003cinterface type='direct'\u003e \u003cmac address='50:54:00:87:bc:c3'/\u003e \u003csource dev='eth3' mode='bridge'/\u003e \u003cmodel type='virtio'/\u003e \u003cdriver name='vhost' txmode='iothread' ioeventfd='on' event_idx='off' queues='16'\u003e \u003chost csum='off' gso='off' tso4='off' tso6='off' ecn='off' ufo='off' mrg_rxbuf='off'/\u003e \u003cguest csum='off' tso4='off' tso6='off' ecn='off' ufo='off'/\u003e \u003c/driver\u003e \u003caddress type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/\u003e \u003c/interface\u003e ... 启动虚拟机后，在宿主机上可以看到对应的 macvtap 设备被创建，其 mac 地址也与配置中的一致。 $ ip a ... 5: eth3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether a4:dc:be:0a:7d:37 brd ff:ff:ff:ff:ff:ff inet 192.168.1.107/24 brd 192.168.1.255 scope global eth3 valid_lft forever preferred_lft forever 10: macvtap0@eth3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP group default qlen 500 link/ether 50:54:00:87:bc:c3 brd ff:ff:ff:ff:ff:ff 在虚拟机内部，可以看到对应的虚拟网卡，通过 dhcp 上层路由器获取 IP，可以发现，其网关就是宿主机的网关，其网段与宿主机一致。因此，使用 macvtap 相当于让虚拟机与宿主机处于同一个二层。 [root@localhost ~]# dhclient -v eth0 ... Listening on LPF/eth0/50:54:00:87:bc:c3 Sending on LPF/eth0/50:54:00:87:bc:c3 Sending on Socket/fallback DHCPREQUEST on eth0 to 255.255.255.255 port 67 (xid=0x4e586341) DHCPNAK from 192.168.1.1 (xid=0x4e586341) DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 8 (xid=0x1d605c78) DHCPREQUEST on eth0 to 255.255.255.255 port 67 (xid=0x1d605c78) DHCPOFFER from 192.168.1.1 DHCPACK from 192.168.1.1 (xid=0x1d605c78) bound to 192.168.1.105 -- renewal in 3423 seconds. [root@localhost ~]# ip a 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:00:87:bc:c1 brd ff:ff:ff:ff:ff:ff inet 192.168.1.105/24 brd 192.168.1.255 scope global dynamic eth0 valid_lft 7023sec preferred_lft 7023sec ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:4:2","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"4.3 设备直接分配 前面的各种网络模式，最后还是靠着全虚拟化或者半虚拟化将数据包传递给虚拟机，而如果你有着多个可用的网卡，那么可以考虑使用设备直接分配，这是性能最高的方式。 4.3.1 PCI 网卡 在设备 xml 配置文件中添加 \u003chostdev\u003e 项，指定对应的设备 PCI 地址，来直接分配设备（也可以使用 \u003cinterface type=‘hostdev’/\u003e，这是一种 libvirt 提供的较新的配置方式，但是不兼容所有设备）。 … \u003chostdev mode='subsystem' type='pci' managed='yes'\u003e \u003csource\u003e \u003caddress domain='0x0000' bus='0x01' slot='0x00' function='0x0'/\u003e \u003c/source\u003e \u003c/hostdev\u003e … 启动虚拟机后，在宿主机上就无法看到对应的网卡了，因为对应的驱动被 libvirt 解绑了。 关闭虚拟机后，libvirt 又会重新将网卡绑定驱动，在宿主机上有可见了。 4.3.1 SR-IOV TODO ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:4:3","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["VM"],"content":"参考 libvirt domain 配置官方文档 RedHat：Guest Virtual Machine Device Configuration virtio-scsi passthrough virtio-scsi 和 virtio-blk 的理解 libvirt: Networking libvirt: VirtualNetworking ","date":"2020-11-28","objectID":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/:5:0","tags":["虚拟机","KVM","云计算"],"title":"KVM 虚拟机的存储与网络总结","uri":"/posts/cloud_computing/vm/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AD%98%E5%82%A8%E4%B8%8E%E7%BD%91%E7%BB%9C/"},{"categories":["Docker 原理总结"],"content":"容器启动背后的执行过程","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文主要描述容器启停背后的步骤，但是不涉及源码。 示例的基于 ubuntu 20.04.1 LTS 虚拟机运行，docker 版本如下： $ docker version Client: Version: 19.03.8 API version: 1.40 Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 19:43:43 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.8 API version: 1.40 (minimum version 1.12) Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 16:41:21 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.3-0ubuntu2 GitCommit: runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit: ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:0:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1 启动 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:1:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1.1 Create 通过 docker create 命令可以创建一个容器，但是容器并不会真正的运行，其对应进程不会存在。 create 容器经过的具体流程为： 容器参数检查与调整； 容器对应的读写层（RWLayer）的创建； 容器元信息的记录（主要是配置信息）； (1) 容器参数检查与调整 检查容器运行参数是否合法，并调整一些参数的值，这一步不具体描述。 (2) 读写层的创建 容器的 “层” 可以分为： 读写层 ：保存容器对 rootfs 写、修改结果的层，并在容器退出后被 docker 删除； 例如，容器中对系统盘 root 目录中某个文件的修改，这个修改后的文件会被复制一份在系统盘上。 init 层 ：用于处理一些与镜像不绑定，但是与运行容器相关的文件修改，主要是 /etc/resolve.conf /etc/hosts 等； 为什么要有 init 层？ 我的理解是：镜像层（只读层）提供的是一个静态的环境，即所有容器看到的环境都是一样。 而有些东西并不是想让所有容器看到的相同，例如 hostname、nameserver，这些 docker 都提供了参数配置，所 docker 单独抽出了一个 “机器维度” 的只读层，init 层 置于为什么不是在读写层修改，个人觉得是因为读写层是可以被 “导出” 的（docker save），而这些 /etresolve.conf 的内容又是不应该被导出的，所以放在了 init 层。 只读层 ：镜像包含的所有层，仅仅只读。对只读层任何修改都会以 COW 形式放在读写层。 具体读写层的概念这里不展开，可以阅读官方文档：storagedriver。 这里读写层的创建仅仅指的是创建了 init 层与读写层的目录，并没有做 union mount，毕竟容器没有运行嘛，没必要。 找个例子看一下： $ docker create --rm -t ubuntu 361c520da78f848d639d65f042fcf5d448c13cbc4ce8c251dcba2250162b48fe inspect 可以看到对应的读写层的目录： $ docker inspect 361c520da78f … \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff:/var/lib/docker/overlay2/0336c489d40e65588748265a95f18328ddb1f5bcb9ebf10909fbf3f5f35b9496/diff:/var/lib/docker/overlay2/77d3ac91877751678bfec0576dab39ccd4b73666f8040aef387ef47ff30b4cf1/diff:/var/lib/docker/overlay2/ec8326178c990b52970a65371fd375737fdf256db597aa821a2b0f7d79bcc6f3/diff:/var/lib/docker/overlay2/385038374d3d369e98724926d0e1c240dcb74e31b1663ec1cb434c43ca2826f1/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/work\" }, \"Name\": \"overlay2\" }, … $ ls /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/ diff link lower work 所有的层（包含容器、镜像）都位于 /var/lib/docker/[driver] 目录下，不过不同的 driver 有着不同的目录结构； 每个层的目录结构也和对应的 driver 有关，overlay2 中就会包含 diff、work 等子目录，而真正容器运行后看到的就是 diff 目录被挂载后的内容； 在 /var/lib/docker/overlay2/ 目录下，我们还可以看到一个同读写层类似名字的 “xxx-init” 目录，这就是 init 层目录，对应的 diff 子目录也是用于挂载的目录： $ ls /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init committed diff link lower work $ tree /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff |-- dev | |-- console | |-- pts | `-- shm `-- etc |-- hostname |-- hosts |-- mtab -\u003e /proc/mounts `-- resolv.conf 但是这两个目录仅仅是被创建，如果执行 mount 命令可以看到，这些目录是没有被挂载的。 (3) 元信息的记录 执行 docker create 后，通过 docker ps -a 可以看到对应的容器，并且 inspect 可以看到对应的配置信息，因此，create 之后是有元信息的记录的。 而这个元信息就保存在 /var/lib/docker/containers 目录下，每个子目录的名字就是对应的容器 ID： $ cd /var/lib/docker/containers \u0026\u0026 ls 361c520da78f848d639d65f042fcf5d448c13cbc4ce8c251dcba2250162b48fe $ ls 361c520da78f848d639d65f042fcf5d448c13cbc4ce8c251dcba2250162b48fe checkpoints config.v2.json hostconfig.json config.v2.json 保存的就是对应的容器配置信息； 可以看到， /var/lib/docker/containers 目录就是所有容器信息的 “数据库”，这与层的概念是解耦的。 如果，你设置了 docker daemon 退出后不停止所有容器（默认情况 docker daemon 退出前会停止并删除所有容器，通过配置可以改变这个行为），那么 docker daemon 重新启动后，就会依靠这个目录进行 container 信息的恢复。 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:1:1","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1.2 Start 通过 docker start 命令运行一个容器，其主要的步骤如下： 状态检查，只有 Create 与 Stop 状态容器才可以被 Start； 进行容器 rootfs 的构建，这里会进行读写层、init 层、镜像层的 union mount； 容器网络构建； 调用 containerd 进行容器的启动； docker run 等同于 docker create + docker run，所以不需要特别说明。 (2) rootfs 的构建 rootfs 指定是容器最后看到的根文件系统，也就是 读写层 + init 层 + 镜像层经过 union mount 后的读写层。 我感觉，union mount 主要由两个特点： 统一视图 ：将各个层 “压扁”，最后得到一个层。而上下层之间相同的文件、目录，就会被上层的覆盖。 写时复制 ：对于整个视图的操作，只会影响最顶层（读写层），不会影响其他层，并且是写时复制的。 看一下具体示例： $ docker start 361c520da78f 361c520da78f 通过 mount 命令，可以看到对应容器的 union mount 已经出现： $ mount … overlay on /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ:/var/lib/docker/overlay2/l/UHTTQ5AJKPR23Y3V7J4ZLOIFDR:/var/lib/docker/overlay2/l/VWIFLRAQOPMH7LBAQQ5DDGIYVM:/var/lib/docker/overlay2/l/LQBRTVETGGWVU2OHWC42443K7X:/var/lib/docker/overlay2/l/5PDNI5HSOH6UMUDNWF4VMR46TS,upperdir=/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/diff,workdir=/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/work,xino=off) nsfs on /run/docker/netns/4500ea4f0025 type nsfs (rw) $ ls -lh /var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ lrwxrwxrwx 1 root root 77 Nov 14 15:51 /var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ -\u003e ../d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff 第一个挂载信息看出，将 init 层与镜像层的目录挂载到了读写的 merged 目录； 后面执行的命令看待，使用的 /var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ 这样的目录是软链，指向前面说的对应的各个层的目录，因为整个层的名字太长，所以使用软链别名让挂载属性短一些； (3) 容器网络构建 首先明确，先有网络环境，再有的容器运行。也就是说，是容器中的进程加入一个特定的 net namespace。所以，网络环境的构建的目标就是：得到一个配好网络的 net namespace。 所以网络构建可以分为三个大概的步骤： 创建一个新的 net namespace； 在这个 net namespace 里操作，构建好网络； 保留这个 net namesapce； 如何 “配好网络的 net namespace” 就和具体的网络模式有关了，具体见：容器网络总结。 默认下，当一个 namespace 中没有任何进程时，该 namespace 就自动被内核销毁了（垃圾回收），而要将一个 namespace 持久化，就需要将其挂载到一个具体文件，这样该 net namespace 就会保留。 因此，docker 会通过这种方式先保留 net namespace，并让容器运行后的进程可以加入。当容器停止是，docker 将其手动删除。 通过 mount 命令，你可以看到对应容器的 net namespace 的挂载会随着容器运行出现。 $ mount … nsfs on /run/docker/netns/4500ea4f0025 type nsfs (rw) … 当然，上面说的 “构建网络”，“挂载文件”，包括 “namespace 文件如何映射到对应的容器” 这些行为与信息，都是由 libnetwork 库中负责的。 Tip 看 libnetwork 源码时发现一点，当某个代码需要进入 namespace，一定需要将当前 goroutine 与 thread 绑定（runtime.LockOSThread()）接口。 Why ？举个例子，一段代码由 G1 groutine 绑定到了 T1 线程执行，创建了 N1 namespace，并且期望在 N1 namespace 下执行。但是代码运行中，可能由于 Go 的调度，变成了 G1 groutine 绑定到 T2 线程执行。这时，就切换了 namespace 了，代码也就不是在期望的 namespace 下执行，这可能带来很大的问题。 (4) 调用 containerd 启动容器 docker daemon 在设计到容器进程的运行时，都是交给 containerd，然后 containerd 调用 shim，shim 调用 runc 库执行。 真正容器内进程怎么运行还包括很多内容，特别是 runc 如何调整进程 namespace，如何启动进程这些内容，这里不再深入说。 如果有空，等后面出个文章详述（挖坑。 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:1:2","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2 停止 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:2:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.1 Stop stop 的步骤就是对 run 操作的回滚，包括： 停止容器的运行； 销毁容器网络； 解除容器读写层的挂载； 其中第 2、3 步就是反着来操作，没啥好说的。 (1) 停止容器运行 停止容器运行，其实就是停止容器内所有进程的运行。 有趣的是停止容器运行的过程里，并没有使用 runc 库，而是在 shim 这一层中，对 shim 进程发送信号。这一块也是后面细说。 大致的停止流程就是下面两步： 发送 配置/默认 (SIGTERM) 的停止信号； 上一步停止失败/超时，那么就发送 SIGKILL 信号； ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:2:1","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.2 Remove 老样子，操作的逆向，这也没啥好说的。 删除对应读写层目录与 init 层目录； 删除对应的容器元信息； ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:2:2","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3 暂停与恢复 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:3:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.1 Pause 与 Resume Pause 操作的原理很简单，通过 cgroup.freezer 冻结进程的运行，也就是不让进程被内核调度运行。 在容器运行状态，读取对应 cgroup 的 freezer.state 文件可以看到是 THAWED 状态。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f4b92a61851a ubuntu \"/bin/bash\" 2 seconds ago Up 1 second host_container $ cat /sys/fs/cgroup/freezer/docker/f4b92a61851a7f5f85569318c830722c275796b36d34a506eae05b547b31cb7c/freezer.state THAWED 执行 docker pause 后，看到对应的 freezer.state 变为 FROZEN，表明被冻结了。 $ docker pause host_container host_container $ cat /sys/fs/cgroup/freezer/docker/f4b92a61851a7f5f85569318c830722c275796b36d34a506eae05b547b31cb7c/freezer.state FROZEN 执行 docker unpause 恢复运行后，对应状态又变为了 THAWED： $ docker unpause host_container $ cat /sys/fs/cgroup/freezer/docker/f4b92a61851a7f5f85569318c830722c275796b36d34a506eae05b547b31cb7c/freezer.state THAWED 而这个状态的变化，其实就是通过 echo \"\u003cstate\u003e\" \u003e freezer.state 实现的。 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:3:1","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"描述 docker 下容器网络模型与实现","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":" 总结系列的文章是自己的学习或使用后，对相关知识的一个总结，用于后续可以快速复习与回顾。 本文是对自己使用过的 docker 使用的网络模式的原理的总结。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:0:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1 概览 docker 容器网络目前包含 5 中模式，包括： bridge：默认的网络模式，使用 bridge 虚拟网卡 + iptables 实现一个内地内网，所有容器都处于该内网内，并且可以相互访问； host：与宿主机处于同一个 net namespace，使用宿主机网络环境； overlay：在多个 docker daemon 之间建立 overlay 网络，使得不同 docker daemon 的容器之间可以相互通信； macvlan：使用 macvlan 虚拟网卡，将容器物理地址暴露在宿主机局域网中，你可以认为就是一台同局域网的物理机； none：不进行任何网络配置，通常与自定义网络 driver 配合使用； 除了上述模式之外，每个容器也可以加入其它容器的网络中（通过加入对应的 net namespace）。 docker 还支持使用自定义的网络插件，这块不了解，具体见官方文档。 下面所有示例都在虚拟机 ubuntu 20.04 与内核 5.4.0-52-generic 中完成，docker 版本如下： $docker version Client: Version: 19.03.8 API version: 1.40 Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 19:43:43 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.8 API version: 1.40 (minimum version 1.12) Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 16:41:21 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.3-0ubuntu2 GitCommit: runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit: ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:1:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2 背景知识 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.1 cgroup 与 namespace 这部分网上知识很多，这里就不复制别人的了。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:1","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.2 docker 如何使用 net namespace namespace 用于各个进程间的环境的隔离，而容器运行（非 host 与 container 模式）的就是处于一个独立的 net namespace。 当处于一个 net namespace 时，可以认为，内核协议栈、iptables(net_filter)、网络设备等与其他 net namespace 都是隔离的。（这里只是说可以这么认为，但是真正还是只有一个内核，内核为 namespace 做了逻辑上的隔离） 在容器运行之间，docker 就会创建容器对应的 net namespace，并构建好对应的网络，然后将其 ‘持久化’（因为默认 namespace 是随着进程消失而消失的，如果想进程消失而 namespace 存在，那么需要将其 mount 到一个文件上）。 例如，当我们创建了一个容器后，可以看到这么一个挂载： $ mount … nsfs on /run/docker/netns/9779108cb6b0 type nsfs (rw) 该文件就是对应 net namespace 的挂载，通过对比容器进程的 netns inode 与 文件 inode 可以证明： $ docker top br0_container UID PID PPID C STIME TTY TIME CMD root 92658 92640 0 Nov06 pts/0 00:00:00 /bin/bash $ ls -lhi /proc/92658/ns/net 474863 lrwxrwxrwx 1 root root 0 Nov 7 12:42 /proc/92658/ns/net -\u003e 'net:[4026532287]' # 文件 inode 与进程 net 指向 inode 相同 $ ls -lhi /run/docker/netns/9779108cb6b0 4026532287 -r--r--r-- 1 root root 0 Nov 6 19:47 /run/docker/netns/9779108cb6b0 在容器被删除后，对应 net namespace 就会被销毁。 而各个网络模式最大的不同，就是在于 namespace 创建后，对应的 “构建网络” 的操作了。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:2","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.3 bridge 虚拟网络设备 bridge 网络设备 相当于一个 “交换机”，让任何其他网络设备链接上 bridge 时，所有包的都会无条件经过 bridge 转发，而链接的网络设备就变成了一根 “网线”。 不过与真实的交换机不同，brdige 网卡可以被赋值 IP，当 bridge 拥有 IP 后，它就与内核协议栈连接了，因此接收到的包可以到达内核协议栈的 IP 层处理，也就会经过 net_filter 处理。 推荐阅读 bridge 网卡推荐阅读：Linux 虚拟网络设备之 bridge（桥） ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:3","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.4 veth-pair 虚拟网络设备 veth-pair 设备 总是成对的出现，当数据包进入一端 veth 设备时，会从另一端 veth 设备出。veth-pair 两个设备可以处于不同的 net namespace，也就可以实现不同 net namespace 间数据传输。 默认下，veth 设备链接的两端是内核协议栈。不过 veth 设备链接上 bridge，这样另一端发送的数据都会由 bridge 处理。 推荐阅读 veth-pair 设备了解推荐文章：Linux 虚拟网络设备之 veth ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:4","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.5 macvlan 虚拟网络设备 macvlan 网络设备 可以有 mac 地址与 ip 地址，用于将 net namespace 连接到宿主机的物理网络中，相当于，容器直接连接着物理网络。 macvlan 网络设备有着多种的模式，包括：bridge、private 等，这影响着各个 macvlan 网络设备之间的通信。 更多 macvlan 网络设备推荐文章：Linux interfaces for virtual networking ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:5","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3 Bridge 网络 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.1 创建/删除 Bridge 网络 (1) 创建网络 先试着创建一个自定义的 bridge 网络，观察 docker 会对应创建哪些东西。 $ docker network create --driver=bridge \\ --subnet=192.168.100.0/24 \\ --ip-range=192.168.100.0/26 \\ --gateway=192.168.100.1 \\ --opt com.docker.network.bridge.name=mybr0 \\ mybridge0 2e61a7dc333c1bc61d9cb86503ce4cd5a7435977ea2f9b7cc97fc71ae0e2bb93 --driver=bridge 指定创建的网络 driver； --subnet=192.168.100.0/24 指定对应 bridge 网络的网段； --ip-range=192.168.100.0/26 指定运行分配给容器的 ip 范围，当然，这个是要在指定的网段内的； --gateway=192.168.100.1 指定该内网的网关 IP； --opt com.docker.network.bridge.name=mybr0 指定创建虚拟 bridge 网卡的命名； mybridge0 为创建的 docker network 的命名； 通过 ifconfig 可以看到，bridge 网络创建会对应创建一个 bridge 网络设备，作为整个内网的 ‘交换机’。其 IP 就是指定的 gateway IP。 $ ifconfig … mybr0: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 192.168.100.1 netmask 255.255.255.0 broadcast 192.168.100.255 ether 02:42:46:8a:cf:34 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 … $ brctl show bridge name bridge id STP enabled interfaces mybr0 8000.0242efdb0984 no 但是与虚拟机网络中的 bridge 网卡不同，该 bridge 不会连接任何的物理网卡，仅仅是作为内网的 ‘交换机’ 使用。但是，毕竟内网是虚拟的，没有实际与物理网络连接，如何访问外网呢？ 答案是，通过内核 iptables 进行 NAT，然后将包从实际的物理网卡上发送与接受。因此还有一部分的改变在于 iptables，主要会建立的是 nat 与 filter 表的规则。 先看 nat 表的相关规则（下面输出中省略了不相关规则）： $ iptables -t nat -L -nv Chain PREROUTING (policy ACCEPT 2 packets, 88 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL Chain INPUT (policy ACCEPT 2 packets, 88 bytes) target prot opt in out source destination Chain OUTPUT (policy ACCEPT 124 packets, 8797 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT 124 packets, 8797 bytes) target prot opt in out source destination MASQUERADE all -- * !mybr0 192.168.100.0/24 0.0.0.0/0 Chain DOCKER (2 references) target prot opt in out source destination RETURN all -- mybr0 * 0.0.0.0/0 0.0.0.0/0 PREROUTING 与 OUTPUT 链中规则，使得所有入和出的包都会经过 DOCKER 链； POSTROUTING 链中，将 mybridge0 网络（192.168.100.0/24）的内网 ip 通过 MASQUERADE 行为进行伪装（可以简单认为内网 ip 会变为当前网卡的 ip）； 当然，如果包发往的是 mybr0 网卡，说明是 mybridge0 网络内部通信，就不需要进行 MASQUERADE 伪装（!mybr0）； 当容器发包时，会通过 mybr0 网卡转发进入内核栈，因此在 filter 表中，相关的规则都是针对于 “in=mybr0”。看一下 filter 表的规则： $ iptables -t filter -L -nv Chain INPUT (policy ACCEPT 61774 packets, 79M bytes) Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- * mybr0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED DOCKER all -- * mybr0 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- mybr0 !mybr0 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- mybr0 mybr0 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 42290 packets, 55M bytes) target prot opt in out source destination Chain DOCKER (2 references) target prot opt in out source destination Chain DOCKER-ISOLATION-STAGE-1 (1 references) target prot opt in out source destination DOCKER-ISOLATION-STAGE-2 all -- mybr0 !mybr0 0.0.0.0/0 0.0.0.0/0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-ISOLATION-STAGE-2 (2 references) target prot opt in out source destination DROP all -- * mybr0 0.0.0.0/0 0.0.0.0/0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-USER (1 references) target prot opt in out source destination RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 FORWARD -\u003e DOCKER-ISOLATION-STAGE-1 -\u003e DOCKER-ISOLATION-STAGE-2 表明允许包从 mybr0 进入并转发（即容器可以向外正常发包）； FORWARD 中对 mybr0 进入的包设置了 conntrack，使得能够收到连接建立后的正常的回包； (2) 删除网络 通过 docker network remove 删除网络时，会发现对应的 bridge 网卡与 iptables 规则都被删除。 $ docker network remove 5a17670afb6f 5a17670afb6f ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:1","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.2 启动容器后的网络 下面看下容器启停后，带来的网络变化。先启动最简单的一个容器，指定使用网络为上面创建的 mybridge0。 $ docker run -dt --rm --network=mybridge0 --name br0_container ubuntu 676f7f9eab12a20fb3a975fa99cc2c92433a9581b5774ea58e63d447d86aa5ad $ docker inspect 676f7f9eab12 … \"Networks\": { \"mybridge0\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": [ \"882cac3e472f\" ], \"NetworkID\": \"af1dbf619ac62be1ad8a6b63696d3e6edff77cceab6cd0ee78de4b51e0d33683\", \"EndpointID\": \"56cd85c5121d7d14146fcacc75599f6c56034e758e81f405a51437276ac6ac9f\", \"Gateway\": \"192.168.100.1\", \"IPAddress\": \"192.168.100.2\", \"IPPrefixLen\": 24, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:c0:a8:64:02\", \"DriverOpts\": null } } … --network=mybridge0 表明以 mybridge0 网络启动容器； 观察容器具体参数，可以看到，容器被随机分配 mybridge0 设置的 ip-range 一个 ip，并且 gateway 就是 mybridge0 网络的网关地址； 观察网络设备，可以看到一个 veth-pair 设备 出现在宿主机上，并且连接到了 mybr0 网卡： $ ifconfig … vethef6b174: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet6 fe80::e8ad:86ff:fefe:14ca prefixlen 64 scopeid 0x20\u003clink\u003e ether ea:ad:86:fe:14:ca txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 23 bytes 1882 (1.8 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 … $ brctl show bridge name bridge id STP enabled interfaces mybr0 8000.0242efdb0984 no vethef6b174 veth-pair 都是成对出现的，可以简单被看做一个通道，一端发入的包会从另一端发出，并进入内核协议栈。不过，在 bridge 网络环境下，veth5b480f8 连接到 mybr0，所以所有从 veth5b480f8 发出的包都会被 mybr0 接手转发（相当于就是一根网线插入了交换机）。 可以进入容器 namespace，看一下容器内的 veth 设备。 $ docker exec -it br0_container bash # 以下在容器 namesapce 环境执行 root@676f7f9eab12:/# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.100.2 netmask 255.255.255.0 broadcast 192.168.100.255 ether 02:42:c0:a8:64:02 txqueuelen 0 (Ethernet) RX packets 1928 bytes 21609413 (21.6 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1817 bytes 102779 (102.7 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@676f7f9eab12:/sys/class/net/eth0# ethtool -i eth0 driver: veth version: 1.0 firmware-version: expansion-rom-version: bus-info: supports-statistics: yes supports-test: no supports-eeprom-access: no supports-register-dump: no supports-priv-flags: no root@676f7f9eab12:~# cat /sys/class/net/eth0/iflink 15 在容器内的仅仅有一个 eth0 网卡，ip 设置为了容器的 ip。但是其实这个网卡就是 veth 设备改了名字； 通过 ethtool -i eth0 命令看到，其对应 driver 是 veth，并且 /sys/class/net/eth0/iflink 文件表明了对端的 veth 网卡编号为 15（即宿主机看到的 veth 网卡设备）； 现在，我们试着启动容器并添加一个 tcp 端口映射。 $ docker run -dt --rm \\ --network=mybridge0 --publish 12211:8080 \\ --name br0_container ubuntu 2502f7397a37e2ab482f8a9152d1ed968dd2e2825c71eb2a6737e4900f7236c1 而这个端口映射，就是将宿主机的 12211 端口映射给容器的 8080，所以所有发往宿主机的 12211 端口的包，都会被修改端口并转发到容器内部。这也是通过 iptables 实现的： iptables -t nat -L -nv Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL Chain INPUT (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 10 617 MASQUERADE all -- * !mybr0 192.168.100.0/24 0.0.0.0/0 0 0 MASQUERADE tcp -- * * 192.168.100.2 192.168.100.2 tcp dpt:8080 Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- mybr0 * 0.0.0.0/0 0.0.0.0/0 0 0 DNAT tcp -- !mybr0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:12211 to:192.168.100.2:8080 PREROUTING -\u003e DOCKER 链中，所有不是从 mybr0 进入的包，并且发往 tcp 12211 端口的包，都会被 DNAT 为发往 192.168.100.2:8080。这样就实现了端口映射的功能。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:2","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.3 bridge 网络总结 中心思想：bridge 网络使用 bridge 网卡创建了一个本地的内网，而 bridge 网卡 + iptables 规则成为了这个内网的 ‘路由器'。其中： bridge 网卡作为二层的交换机，bridge 网卡 ip 作为路由器的网关 ip。 iptables 规则实现了 brdige 网卡与物理网络的连接 宿主机内核栈实现了这个 ‘路由器’ 的路由功能。 下图展示了整个 bridge 网络的模型（图片来自网络）： 其中比较关键的点： veth pair 设备将容器 net namespace 连接到 bridge 网卡（可以看做将 veth pair 作为网线插到了 bridge 这个 ‘路由器’ 上）。 iptables 实现了 bridge 网卡与物理网络的 ‘连接’。 bridge 网卡收到的包，经过 iptables 的 MASQUERADE 将包进行地址转换，并经过内核协议栈的路由通过物理网卡发送到物理网络。而回包通过 conntrack 机制正常接收与逆地址转换。 容器与宿主机的端口映射，也是通过 iptables 的 DNAT 实现的。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:3","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"4 Host 网络 Host 网络没啥好说的，启动容器不创建新的 namespace，依旧在宿主机的 net namespace 下。 $ docker run -dt --rm --network=host --name host_container ubuntu da1c426a7c7501b329258b12cb475ff42669837ca686d6e946511632461cc946 观察 mount，可以看到对应还是有 net namespace 的文件挂载，文件名为 default： $ mount … nsfs on /run/docker/netns/default type nsfs (rw) 文件 inode 对比当前宿主机 net namespace inode，是一致的： $ ls -lh /proc/self/ns/net lrwxrwxrwx 1 root root 0 Nov 7 14:47 /proc/self/ns/net -\u003e 'net:[4026531992]' $ ls -lhi /run/docker/netns/default 4026531992 -r--r--r-- 1 root root 0 Oct 30 16:50 /run/docker/netns/default ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:4:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5 macvlan 网络 macvlan 网络使用 macvlan 虚拟网络设备，将容器 net namespace 网络暴露在与当前宿主机同级的局域网内，相当于容器就是当前网络内的一台 “主机”。 macvlan 网络设备也包括多种模式：bridge mode、802.1q trunk bridge mode。下面示例都是基于普通的 brdige mode。 因为 macvlan 网络在虚拟机网络下不太好验证，所以下面例子来自于一台物理机上。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5.1 创建/删除 macvlan 网络 通过 docker network create 创建 macvlan 网络。 $ docker network create -d macvlan \\ --subnet=192.168.67.130/24 --gateway=192.168.67.1 \\ -o parent=eth0 mymacvlan0 633aae3d4f430352e5439e2650c02fe9c2092b99b5b8252f8141fa5d62ec7e70 -d macvlan，指定 macvlan 网络 -subnet=192.168.67.130/24，因为 macvlan 网络下的容器会直接连入物理网络，所以子网也是要在当前子网内； --gateway=192.168.67.1，同样，gateway 就是宿主机的网关地址； -o parent=eth0，指定 macvlan 设备链接的物理网卡，一定要是一个真正可联网的物理网卡； 不过与 bridge 网络不同的是，创建一个 macvlan 网络仅仅是记录其对应的配置，不会创建对应的 macvlan 网卡或者 iptables 规则。因为 macvlan 网卡是与 net namespace 绑定的，所以当创建 net namespace 时才会出现对应网络设备。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:1","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5.2 启停容器后的网络 $ docker run --net=mymacvlan0 \\ -dt --rm --name macvlan_container \\ --ip=192.168.67.139 --privileged \\ centos_ctr bash 2eff4835733734b6819c7f97ae41585985d95c1ea4c66a6a478a43e71b60b6d6 启动容器，如果不指定 IP，Docker 会在配置的网段里分配一个。 tip 为了能够方便排查网络问题，使用的容器镜像 centos_ctr 是由 centos 镜像以 host 网络启动，预装一些命令后，才由容器导出的镜像。 但是发现进入容器后，发现静态配置 IP 无法 ping 通网关（宿主机是正常无法 ping 通，因为内核会丢弃 macvlan 网卡的包）。研究后不清楚具体原因，但是这台宿主机接的是交换机，不知道是不是不是路由器导致的。 $ docker exec -it macvlan_container bash # 以下是容器中命令 [root@2eff48357337 /]# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.67.139 netmask 255.255.255.0 broadcast 192.168.67.255 ether 02:42:c0:a8:43:8b txqueuelen 0 (Ethernet) RX packets 433282 bytes 27463954 (26.1 MiB) RX errors 0 dropped 26809 overruns 0 frame 0 TX packets 91342 bytes 6649728 (6.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@2eff48357337 /]# ping 192.168.67.1 PING 192.168.67.1 (192.168.67.1) 56(84) bytes of data. From 192.168.67.139 icmp_seq=1 Destination Host Unreachable From 192.168.67.139 icmp_seq=2 Destination Host Unreachable 因此，换个思路，静态 IP 不行，就通过 DHCP 获取一个 IP 尝试是否能够连通网络。 在删除静态 IP 之后，调用 dhclient 从上层路由器获取一个 IP。 # 删除 eth0 网卡 IP [root@2eff48357337 /]# ip address del 192.168.67.139 dev eth0 Warning: Executing wildcard deletion to stay compatible with old scripts. Explicitly specify the prefix length (192.168.67.139/32) to avoid this warning. This special behaviour is likely to disappear in further releases, fix your scripts! [root@2eff48357337 /]# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether 02:42:c0:a8:43:8b txqueuelen 0 (Ethernet) RX packets 435540 bytes 27608133 (26.3 MiB) RX errors 0 dropped 26983 overruns 0 frame 0 TX packets 91763 bytes 6678670 (6.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 调用 dhclient 获取新的 IP [root@2eff48357337 /]# dhclient -r \u0026\u0026 dhclient -v Removed stale PID file Internet Systems Consortium DHCP Client 4.3.6 Copyright 2004-2017 Internet Systems Consortium. All rights reserved. For info, please visit https://www.isc.org/software/dhcp/ Listening on LPF/eth0/02:42:c0:a8:43:8b Sending on LPF/eth0/02:42:c0:a8:43:8b Sending on Socket/fallback DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 3 (xid=0xdf4e0e25) DHCPREQUEST on eth0 to 255.255.255.255 port 67 (xid=0xdf4e0e25) DHCPOFFER from 192.168.9.253 DHCPACK from 192.168.9.253 (xid=0xdf4e0e25) System has not been booted with systemd as init system (PID 1). Can't operate. Failed to create bus connection: Host is down bound to 192.168.9.235 -- renewal in 38783 seconds. [root@2eff48357337 /]# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.9.235 netmask 255.255.255.0 broadcast 192.168.9.255 ether 02:42:c0:a8:43:8b txqueuelen 0 (Ethernet) RX packets 435635 bytes 27614880 (26.3 MiB) RX errors 0 dropped 27001 overruns 0 frame 0 TX packets 91767 bytes 6679438 (6.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@2eff48357337 /]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.9.253 0.0.0.0 UG 0 0 0 eth0 192.168.9.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 可以看到，DHCP 获得的 IP 与宿主机都不是同一个网段的，并且网关地址也不是同一个，因此上层连着交换机有多个网段（这块不太理解了）。 但是，测试后是可以 ping 通网关，并且可以访问外网的： [root@2eff48357337 /]# ping 192.168.9.253 PING 192.168.9.253 (192.168.9.253) 56(84) bytes of data. 64 bytes from 192.168.9.253: icmp_seq=1 ttl=64 time=0.637 ms 64 bytes from 192.168.9.253: icmp_seq=2 ttl=64 time=0.250 ms ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:2","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5.3 总结 中心思想：将 macvlan 网络启动容器看做一个与宿主机同级的网络，其获取 IP 方式都与正常的机器相同。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:3","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"参考 Docker 容器网络官方文档 Linux 虚拟网络设备之 bridge（桥） Linux interfaces for virtual networking ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:6:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["VM"],"content":"制作虚拟机镜像","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"中心思想：通过 libvirt 运行一个虚拟机（domain），并保存其对应的 domain 的镜像文件与配置文件，然后就可以在其他机器通过 virsh define + start 或者 virt-install 启动。 说明：下面环境都是在 centos 上制作基于 KVM 的虚拟机镜像。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:0:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"1 从 ISO 镜像安装 最基本的安装方式，通过安装并运行一个新的虚拟机，然后得到对应的配置文件与镜像文件。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:1:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"1.1 手动安装 最基本的安装方式，通过 ISO 文件进行安装。 下载 ISO 镜像文件，镜像文件在各个镜像站中就可以找到。（因为不使用图形界面，所以下载的是非图形化的 ISO） 通过 virt-install 命令安装镜像（因为使用的是非图形化的安装，所以参数有一些不一样） virt-install --name guest1_fromiso --memory 2048 --vcpus 2 \\ --disk size=8 --location CentOS-7-x86_64-DVD-2003.iso \\ --os-type Linux --os-variant=centos7.0 --virt-type kvm \\ --boot menu=on --graphics none --console pty --extra-args 'console=ttyS0' 其中要注意几个参数： 因为我们是安装非图形化，所以需要 --location 参数指定 iso，并指定 --boot menu=on 打开安装菜单，最后还需要指定安装信息的输出 --console pty --extra-args 'console=ttyS0' 这样安装菜单才能正常展示出来 --graphics none 指定非图形化； --network bridge=virbr0，指定网络模式，这里指定 libvirt 默认创建的 bridge 网卡，可以认为这是一个 libvirt 维护内网，安装时选择 dhcp 就可以获得一个可用的内网地址； 具体 libvirt 的网络模型，后面在单独研究下。 -- disk size=8，disk 参数用于指定系统盘，这里指定自动创建一个 8G 的 qcow2 文件，作为系统盘（默认镜像文件保存在 /var/lib/libvirt/images/）目录下； 这时就会进入虚拟机的安装步骤，具体安装步骤就不赘述了。 安装成功后，可以看到 domain 就被创建了，这就可以得到它的配置文件与镜像文件了。 $ virsh list Id Name State ---------------------------------------------------- 18 guest1_fromiso running ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:1:1","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"1.2 自动安装 可以看到，手动安装需要人为在菜单中选择、配置，这不适用于多个虚拟机的安装。而 RedHat 创建了 kickstart 安装方法，使得整个虚拟机安装流程变得自动化。 这块不了解，具体见红帽官方文档：KICKSTART INSTALLATIONS ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:1:2","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"2 使用 Cloud Image 当然，上面制作过程中耗时都在安装系统上了，而各个云厂商的虚拟机数量那么多，肯定不会一台台去安装操作系统了。所以，目前最常见的都是直接下载已经安装过系统的虚拟机镜像文件。 但是这样的虚拟机是没有特殊配置的，例如密码、hostname 都是一致的，所以 cloud-init 出现，用于在第一次启动虚拟机时进行系统的配置。 所以，最快速的制作方法就是：虚拟机镜像文件 + cloud-init 配置。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:2:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"2.1 cloud-init 下面内容都来自于文档，这里仅为自己做个记录。 首先明确 cloud init 的功能：系统第一次启动时，cloud init 相关的进程会根据配置信息去进行系统的配置，包括：设置 hostname、ssh key、password 等； (1) 基本概念 metadata：包含服务器信息，用于 cloud-init 配置； userdata：包含 cloud-init 系统配置信息，可以是 文件，脚本，yaml 文件等； datasource：cloud-init 读取配置数据的来源，包含大部分云厂商，当然，也可以来自本地的文件 (NoCloud datasource)； (2) 运行过程 cloud init 设置包括五个阶段： Generator 机器启动阶段，systemd 的一个 generator 将会决定是否将 cloud-init.target（target 可以简单认为特定事件下触发的一组 unit）包含在启动过程中。这就表示启动 cloud-init。 默认情况下，generator 会启动 cloud-init，除非以下情况： /etc/cloud/cloud-init.disabled 文件存在； 内核启动命令行 /proc/cmdline 中包含 “cloud-init=disabled”。如果是容器中运行，会读取环境变量 KERNEL_CMDLINE； 而下面的步骤，就是由 target 包含的各个 unit 执行的。 Local 由 cloud-init-local.service 执行，主要目的：找到 “local” 的 datasource，根据配置网络。 配置网络有三种情况： 首先，根据传入配置 “metadata” 配置网络； 当上面情况失败，直接配置 “dhcp on eth0”； 如果 /etc/cloud/cloud.cfg 配置文件中禁用了网络：network: {config: disabled}，那么就不进行网络配置； Init、Config、Final 阶段 对应 service 为 cloud-init.service、cloud-config.service、 cloud-final.service。 通过 local 阶段，网络已经配置好了，并且已经得到了 metadata。而 /etc/cloud/cloud.cfg 配置定义了剩下三个阶段对应的任务，也就是 module。 cloud init 通过一些缓存信息来判断机器是否经过初始化，通过 cloud-init clean 也可以手动清理缓存信息。 /var/log/cloud-init.log 记录了 cloud-init 运行的完整过程。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:2:1","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"2.2 制作镜像 下面就开始进行镜像制作： 下载 cloud image，这里使用中科大提供的： $ wget https://mirrors.ustc.edu.cn/centos-cloud/centos/7/images/CentOS-7-x86_64-GenericCloud-2003.qcow2 修改密码 当然，该镜像其实就可以直接进行 virt-install 来启动（因为我们没有配置文件，所以通过 virt-install 来启动并生成配置文件），但是不知道密码，也搜不到，无法登陆进入。不过你也可以使用下面命令来设置密码后进行登陆： $ virt-customize -a CentOS-7-x86_64-GenericCloud-2003.qcow2 --root-password password:yourpassword 因为 cloud-init 需要一个 datasource，而我们没有使用云厂商，所以使用 NoCloud 形式，按照官方的 s 示例创建一个 disk 文件。 # 创建 user-data 与 meta-date 配置文件 $ cat meta-data instance-id: guest1 local-hostname: guest1 $ cat user-data #cloud-config chpasswd: expire: false list: | root: password1 ssh_pwauth: True # 生成 disk 文件，包含 userdata 与 metadata 配置数据 $ genisoimage -output seed.iso -volid cidata -joliet -rock user-data meta-data 创建并运行虚拟机。 $ virt-install --memory 2048 --vcpus 2 --name guest2 \\ --disk CentOS-7-x86_64-GenericCloud-2003.qcow2 --disk seed.iso \\ --os-type Linux --os-variant centos7.0 --virt-type kvm \\ --graphics none --network default \\ --import 几个比较重要的参数： --disk CentOS-7-x86_64-GenericCloud-2003.qcow2：制定系统盘； --import：跳过安装过程，因为已经安装好操作系统，不需要进行安装过程； --disk seed.iso：传递 cloud-init datasource 信息； 虚拟机启动过程中，可以看到 cloud-init 配置信息的一些打印： 最后根据配置的密码成功进入： 而 CentOS-7-x86_64-GenericCloud-2003.qcow2 就是虚拟机经过配置的镜像文件，而 libvirt 启动所需的配置文件就是 /etc/libvirt/qemu/guest1.xml。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:2:2","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"参考 CREATING GUESTS WITH VIRT-INSTALL KICKSTART INSTALLATIONS cloud-init Documentation ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:3:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["Kubernetes 实践"],"content":"使用虚拟机搭建 Kubernetes 集群","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":["Kubernetes 实践"],"content":" Kubernetes 实践 主要记录一些自己的一些部署实践。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/:0:0","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":["Kubernetes 实践"],"content":"1 虚拟机集群搭建 目标：创建 3 个虚拟机，用作一个 Master Node，两个 Work Node；当然，三个节点处于同一个网段。 具体步骤如下: 构建节点 构建三个虚拟机，基于 centos 7、内存 2 GB，并通过虚拟机复制功能（其实就是 copy 系统盘），完全复制出 Node 1，Node 2，Node 3。 搭建网络 三个节点需要互相访问，所以将其位于 VirtualBox 创建的 Nat网络下，给予每个 Node 静态的 IP（10.0.2.10 - 10.0.2.12），为了方便访问，并设置 ssh 的 DNAT。 设置每个虚拟机网卡加入其创建的 “NodeNatNetwork”。例如： 启动每个虚拟机，设置其 hostname，与网卡静态 IP。例如： 至此，三个虚拟机位于同一个网段，并且能够相互访问；对外，则通过 VirtualBox 的 Nat 网络能够访问。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/:1:0","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":["Kubernetes 实践"],"content":"2 部署 K8s 目标：通过 kubeadm 部署整个 k8s，用 Node-1 为 Master 节点，其他为工作节点。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/:2:0","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":["Kubernetes 实践"],"content":"2.1 安装 kubeadm、kubelet、kubectl 安装 kubeadm、kubelet、kubectl。这个官方文档写的很详细，见 Installing kubeadm 。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/:2:1","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":["Kubernetes 实践"],"content":"2.2 kubeadm init 初始化 Master 节点 Node-1 节点执行 kubeadm init，将其作为 Master 节点初始化。执行成功后，kubeadm 生成了 Kubernetes 组件的各个配置，以及提供服务的各类证书，位于 /etc/kubernetes 目录下: 并且已经以 static pod 的形式启动了：apiserver、controller-manager、etcd、scheduler。 还有最重要的，kubeadm 为集群生成一个【bootstrap token】，需要加入集群的节点都需要通过这个 token 加入。 * 问题 kubeadm 检查 swap 打开着，kubeadm 推荐不使用 swap，通过 swapoff -a 关闭交换区。 kubectl 默认通过 8080 端口访问，无法执行。 设置 kubectl 的配置文件为 kubeadm 生成的 /etc/kubernetes/admin.conf。（其实就是配置公钥，或者将 admin.conf 移动到 ~/.kube/config 文件，作为默认配置。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/:2:2","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":["Kubernetes 实践"],"content":"2.3 kubeadm join [token] 设置工作节点 通过 kubead init 最后返回的提示信息，执行对应 kubeadm join 将 Node-1 Node-2 加入到集群中，作为工作节点。 [root@Node-2 kubeadm join 10.0.2.10:6443 --token mahrou.d3uodof21i3d6yxk --discovery-token-ca-cert-hash sha256:21dfe4ef6b3bbd89f803bf44ff6eda587874336d103d0e4a3b --v 5 可以看到，kubelet 启动后就通过 pod 方式启动了本节点上 kube-proxy 容器： * 问题 无法访问到 Node-1 节点，nc ip 失败，但是可以 ping 通。通过在 Node-1 tcpdump 可以抓取到来自 Node-3 的包，因此应该是防火墙的问题，通过 iptables 对 Node-2 Node-3 IP 开放。 kubectl 无法访问问题，与上述问题一致。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/:2:3","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":["Kubernetes 实践"],"content":"2.4 结果 目前为止，就完成了集群的搭建，但是 通过 kubectl get nodes，可以看到所有节点都是 NotReady： kubectl describe node node-1 可以看到，原因是因为没有设置正确的 Network Plugin： ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/:2:4","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":["Kubernetes 实践"],"content":"3 部署网络插件 目的：部署网络插件，使各个节点为 Ready 状态，并其内部 Pod 能够相互通信。 以 Weave 部署为例，部署网络插件： kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" 其描述文件中定义所有 Weave 需要的 BAAC 权限组件，以及最重要的网络插件 Pod 对应的 DaemonSet: 应用成功后，可以看到对应的 DaemonSet 就运行起来，并开始给三个 Node 部署 Pod: 在节点上，可以看到 weave-net 对应的 pod ，包括两个容器： ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/:3:0","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":["Kubernetes 实践"],"content":"4 部署容器存储插件 目的：为了能够让容器使用网络存储，使得容器数据持久化，需要部署存储插件。 以 Rook 项目为例，部署存储插件： $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/exampleskubernetes/ceph/common.yaml $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/exampleskubernetes/ceph/operator.yaml $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml 安装成功后，可以看到，rook 有着自己的 namespace，并且已经部署了 DaemonSet： 可以看到，Pod 也部署成功了： ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/create-cluster/:4:0","tags":["k8s","云计算"],"title":"K8s 实践 - 集群搭建","uri":"/posts/cloud_computing/k8s_practice/create-cluster/"},{"categories":null,"content":"写博客是为了总结自己学到的东西，如果存在错误，欢迎联系我改正 :) 博客使用 Hugo 框架进行搭建, 主题使用 LoveIt. Logo 由 gopherize.me 生成, 并使用 realfavicongenerator.net 转化. ","date":"2020-10-15","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"}]