[{"categories":["Docker 原理总结"],"content":"容器启动背后的执行过程","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"目的：描述容器启停背后的步骤，但是不涉及源码。 示例的基于 ubuntu 20.04.1 LTS 虚拟机运行，docker 版本如下： $ docker version Client: Version: 19.03.8 API version: 1.40 Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 19:43:43 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.8 API version: 1.40 (minimum version 1.12) Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 16:41:21 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.3-0ubuntu2 GitCommit: runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit: ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:0:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1 启动 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:1:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1.1 Create 通过 docker create 命令可以创建一个容器，但是容器并不会真正的运行，其对应进程不会存在。 create 容器经过的具体流程为: 容器参数检查与调整； 容器对应的读写层（RWLayer）的创建； 容器元信息的记录（主要是配置信息）； (1) 容器参数检查与调整 检查容器运行参数是否合法，并调整一些参数的值，这一步不具体描述。 (2) 读写层的创建 容器的 “层” 可以分为： 读写层 ：保存容器对 rootfs 写、修改结果的层，并在容器退出后被 docker 删除； 例如，容器中对系统盘 root 目录中某个文件的修改，这个修改后的文件会被复制一份在系统盘上。 init 层 ：用于处理一些与镜像不绑定，但是与运行容器相关的文件修改，主要是 /etc/resolve.conf /etc/hosts 等； 为什么要有 init 层？我的理解是：镜像层（只读层）提供的是一个静态的环境，即所有容器看到的环境都是一样 而有些东西并不是想让所有容器看到的相同，例如 hostname、nameserver，这些 docker 都提供了参数配置，所docker 单独抽出了一个 “机器维度” 的只读层，init 层。 置于为什么不是在读写层修改，个人觉得是因为读写层是可以被 “导出” 的（docker save），而这些 /etresolve.conf 的内容又是不应该被导出的，所以放在了 init 层。 只读层 ：镜像包含的所有层，仅仅只读。对只读层任何修改都会以 COW 形式放在读写层。 具体读写层的概念这里不展开，可以阅读官方文档：storagedriver。 这里读写层的创建仅仅指的是创建了 init 层与读写层的目录，并没有做 union mount，毕竟容器没有运行嘛，没必要。 找个例子看一下： $ docker create --rm -t ubuntu 361c520da78f848d639d65f042fcf5d448c13cbc4ce8c251dcba2250162b48fe inspect 可以看到对应的读写层的目录： $ docker inspect 361c520da78f … \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff:/var/lib/docker/overlay2/0336c489d40e65588748265a95f18328ddb1f5bcb9ebf10909fbf3f5f35b9496/diff:/var/lib/docker/overlay2/77d3ac91877751678bfec0576dab39ccd4b73666f8040aef387ef47ff30b4cf1/diff:/var/lib/docker/overlay2/ec8326178c990b52970a65371fd375737fdf256db597aa821a2b0f7d79bcc6f3/diff:/var/lib/docker/overlay2/385038374d3d369e98724926d0e1c240dcb74e31b1663ec1cb434c43ca2826f1/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/work\" }, \"Name\": \"overlay2\" }, … $ ls /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/ diff link lower work 所有的层（包含容器、镜像）都位于 /var/lib/docker/[driver] 目录下，不过不同的 driver 有着不同的目录结构； 每个层的目录结构也和对应的 driver 有关，overlay2 中就会包含 diff、work 等子目录，而真正容器运行后看到的就是 diff 目录被挂载后的内容; 在 /var/lib/docker/overlay2/ 目录下，我们还可以看到一个同读写层类似名字的 “xxx-init” 目录，这就是 init 层目录，对应的 diff 子目录也是用于挂载的目录： $ ls /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init committed diff link lower work $ tree /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff |-- dev | |-- console | |-- pts | `-- shm `-- etc |-- hostname |-- hosts |-- mtab -\u003e /proc/mounts `-- resolv.conf 但是这两个目录仅仅是被创建，如果执行 mount 命令可以看到，这些目录是没有被挂载的。 (3) 元信息的记录 执行 docker create 后，通过 docker ps -a 可以看到对应的容器，并且 inspect 可以看到对应的配置信息，因此，create 之后是有元信息的记录的。 而这个元信息就保存在 /var/lib/docker/containers 目录下，每个子目录的名字就是对应的容器 ID： $ cd /var/lib/docker/containers \u0026\u0026 ls 361c520da78f848d639d65f042fcf5d448c13cbc4ce8c251dcba2250162b48fe $ ls 361c520da78f848d639d65f042fcf5d448c13cbc4ce8c251dcba2250162b48fe checkpoints config.v2.json hostconfig.json config.v2.json 保存的就是对应的容器配置信息； 可以看到， /var/lib/docker/containers 目录就是所有容器信息的 “数据库”，这与层的概念是解耦的。 如果，你设置了 docker daemon 退出后不停止所有容器（默认情况 docker daemon 退出前会停止并删除所有容器，通过配置可以改变这个行为），那么 docker daemon 重新启动后，就会依靠这个目录进行 container 信息的恢复。 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:1:1","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1.2 Start 通过 docker start 命令运行一个容器，其主要的步骤如下： 状态检查，只有 Create 与 Stop 状态容器才可以被 Start； 进行容器 rootfs 的构建，这里会进行读写层、init 层、镜像层的 union mount； 容器网络构建； 调用 containerd 进行容器的启动； docker run 等同于 docker create + docker run，所以不需要特别说明。 (2) rootfs 的构建 rootfs 指定是容器最后看到的根文件系统，也就是 读写层 + init 层 + 镜像层经过 union mount 后的读写层。 我感觉，union mount 主要由两个特点： 统一视图 ：将各个层 “压扁”，最后得到一个层。而上下层之间相同的文件、目录，就会被上层的覆盖。 写时复制 ：对于整个视图的操作，只会影响最顶层（读写层），不会影响其他层，并且是写时复制的。 看一下具体示例： $ docker start 361c520da78f 361c520da78f 通过 mount 命令，可以看到对应容器的 union mount 已经出现： $ mount … overlay on /var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ:/var/lib/docker/overlay2/l/UHTTQ5AJKPR23Y3V7J4ZLOIFDR:/var/lib/docker/overlay2/l/VWIFLRAQOPMH7LBAQQ5DDGIYVM:/var/lib/docker/overlay2/l/LQBRTVETGGWVU2OHWC42443K7X:/var/lib/docker/overlay2/l/5PDNI5HSOH6UMUDNWF4VMR46TS,upperdir=/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/diff,workdir=/var/lib/docker/overlay2/d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c/work,xino=off) nsfs on /run/docker/netns/4500ea4f0025 type nsfs (rw) $ ls -lh /var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ lrwxrwxrwx 1 root root 77 Nov 14 15:51 /var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ -\u003e ../d063d1d9c81d0c72d7384ea999dbd77b33d04b942ef94a5aabc6fb6cf984194c-init/diff 第一个挂载信息看出，将 init 层与镜像层的目录挂载到了读写的 merged 目录； 后面执行的命令看待，使用的 /var/lib/docker/overlay2/l/O2TO66S3K4MTADSAAX6VXGTWSJ 这样的目录是软链，指向前面说的对应的各个层的目录，因为整个层的名字太长，所以使用软链别名让挂载属性短一些； (3) 容器网络构建 首先明确，先有网络环境，再有的容器运行。也就是说，是容器中的进程加入一个特定的 net namespace。所以，网络环境的构建的目标就是：得到一个配好网络的 net namespace。 所以网络构建可以分为三个大概的步骤： 创建一个新的 net namespace； 在这个 net namespace 里操作，构建好网络； 保留这个 net namesapce； 如何 “配好网络的 net namespace” 就和具体的网络模式有关了，具体见：容器网络总结。 默认下，当一个 namespace 中没有任何进程时，该 namespace 就自动被内核销毁了（垃圾回收），而要将一个 namespace 持久化，就需要将其挂载到一个具体文件，这样该 net namespace 就会保留。 因此，docker 会通过这种方式先保留 net namespace，并让容器运行后的进程可以加入。当容器停止是，docker 将其手动删除。 通过 mount 命令，你可以看到对应容器的 net namespace 的挂载会随着容器运行出现。 $ mount … nsfs on /run/docker/netns/4500ea4f0025 type nsfs (rw) … 当然，上面说的 “构建网络”，“挂载文件”，包括 “namespace 文件如何映射到对应的容器” 这些行为与信息，都是由 libnetwork 库中负责的。 Tip 看 libnetwork 源码时发现一点，当某个代码需要进入 namespace，一定需要将当前 goroutine 与 thread 绑定（runtime.LockOSThread()）接口。 Why ？举个例子，一段代码由 G1 groutine 绑定到了 T1 线程执行，创建了 N1 namespace，并且期望在 N1 namespace 下执行。但是代码运行中，可能由于 Go 的调度，变成了 G1 groutine 绑定到 T2 线程执行。这时，就切换了 namespace 了，代码也就不是在期望的 namespace 下执行，这可能带来很大的问题。 (4) 调用 containerd 启动容器 docker daemon 在设计到容器进程的运行时，都是交给 containerd，然后 containerd 调用 shim，shim 调用 runc 库执行。 真正容器内进程怎么运行还包括很多内容，特别是 runc 如何调整进程 namespace，如何启动进程这些内容，这里不再深入说。 如果有空，等后面出个文章详述（挖坑。 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:1:2","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2 停止 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:2:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.1 Stop stop 的步骤就是对 run 操作的回滚，包括： 停止容器的运行； 销毁容器网络； 解除容器读写层的挂载； 其中第 2、3 步就是反着来操作，没啥好说的。 (1) 停止容器运行 停止容器运行，其实就是停止容器内所有进程的运行。 有趣的是停止容器运行的过程里，并没有使用 runc 库，而是在 shim 这一层中，对 shim 进程发送信号。这一块也是后面细说。 大致的停止流程就是下面两步： 发送 配置/默认(SIGTERM) 的停止信号； 上一步停止失败/超时，那么就发送 SIGKILL 信号； ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:2:1","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.2 Remove 老样子，操作的逆向，这也没啥好说的。 删除对应读写层目录与 init 层目录； 删除对应的容器元信息； ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:2:2","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3 暂停与恢复 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:3:0","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.1 Pause 与 Resume Pause 操作的原理很简单，通过 cgroup.freezer 冻结进程的运行，也就是不让进程被内核调度运行。 在容器运行状态，读取对应 cgroup 的 freezer.state 文件可以看到是 THAWED 状态。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f4b92a61851a ubuntu \"/bin/bash\" 2 seconds ago Up 1 second host_container $ cat /sys/fs/cgroup/freezer/docker/f4b92a61851a7f5f85569318c830722c275796b36d34a506eae05b547b31cb7c/freezer.state THAWED 执行 docker pause 后，看到对应的 freezer.state 变为 FROZEN，表明被冻结了。 $ docker pause host_container host_container $ cat /sys/fs/cgroup/freezer/docker/f4b92a61851a7f5f85569318c830722c275796b36d34a506eae05b547b31cb7c/freezer.state FROZEN 执行 docker unpause 恢复运行后，对应状态又变为了 THAWED： $ docker unpause host_container $ cat /sys/fs/cgroup/freezer/docker/f4b92a61851a7f5f85569318c830722c275796b36d34a506eae05b547b31cb7c/freezer.state THAWED 而这个状态的变化，其实就是通过 echo \"\u003cstate\u003e\" \u003e freezer.state 实现的。 ","date":"2020-11-13","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/:3:1","tags":["docker","container"],"title":"容器启停原理总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E5%90%AF%E5%81%9C%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"描述 docker 下容器网络模型与实现","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"1 概览 docker 容器网络目前包含 5 中模式，包括： bridge：默认的网络模式，使用 bridge 虚拟网卡 + iptables 实现一个内地内网，所有容器都处于该内网内，并且可以相互访问； host：与宿主机处于同一个 net namespace，使用宿主机网络环境； overlay：在多个 docker daemon 之间建立 overlay 网络，使得不同 docker daemon 的容器之间可以相互通信； macvlan：使用 macvlan 虚拟网卡，将容器物理地址暴露在宿主机局域网中，你可以认为就是一台同局域网的物理机； none：不进行任何网络配置，通常与自定义网络 driver 配合使用； 除了上述模式之外，每个容器也可以加入其它容器的网络中（通过加入对应的 net namespace）。 docker 还支持使用自定义的网络插件，这块不了解，具体见官方文档。 下面所有示例都在虚拟机 ubuntu 20.04 与内核 5.4.0-52-generic 中完成，docker 版本如下： $docker version Client: Version: 19.03.8 API version: 1.40 Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 19:43:43 2020 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 19.03.8 API version: 1.40 (minimum version 1.12) Go version: go1.13.8 Git commit: afacb8b7f0 Built: Wed Oct 14 16:41:21 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.3-0ubuntu2 GitCommit: runc: Version: spec: 1.0.1-dev GitCommit: docker-init: Version: 0.18.0 GitCommit: ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:1:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2 背景知识 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.1 cgroup 与 namespace 这部分网上知识很多，这里就不复制别人的了。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:1","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.2 docker 如何使用 net namespace namespace 用于各个进程间的环境的隔离，而容器运行（非 host 与 container 模式）的就是处于一个独立的 net namespace。 当处于一个 net namespace 时，可以认为，内核协议栈、iptables(net_filter)、网络设备等与其他 net namespace 都是隔离的。（这里只是说可以这么认为，但是真正还是只有一个内核，内核为 namespace 做了逻辑上的隔离） 在容器运行之间，docker 就会创建容器对应的 net namespace，并构建好对应的网络，然后将其 ‘持久化’（因为默认 namespace 是随着进程消失而消失的，如果想进程消失而 namespace 存在，那么需要将其 mount 到一个文件上）。 例如，当我们创建了一个容器后，可以看到这么一个挂载： $ mount … nsfs on /run/docker/netns/9779108cb6b0 type nsfs (rw) 该文件就是对应 net namespace 的挂载，通过对比容器进程的 netns inode 与 文件 inode 可以证明： $ docker top br0_container UID PID PPID C STIME TTY TIME CMD root 92658 92640 0 Nov06 pts/0 00:00:00 /bin/bash $ ls -lhi /proc/92658/ns/net 474863 lrwxrwxrwx 1 root root 0 Nov 7 12:42 /proc/92658/ns/net -\u003e 'net:[4026532287]' # 文件 inode 与进程 net 指向 inode 相同 $ ls -lhi /run/docker/netns/9779108cb6b0 4026532287 -r--r--r-- 1 root root 0 Nov 6 19:47 /run/docker/netns/9779108cb6b0 在容器被删除后，对应 net namespace 就会被销毁。 而各个网络模式最大的不同，就是在于 namespace 创建后，对应的 “构建网络” 的操作了。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:2","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.3 bridge 虚拟网络设备 bridge 网络设备 相当于一个 “交换机”，让任何其他网络设备链接上 bridge 时，所有包的都会无条件经过 bridge 转发，而链接的网络设备就变成了一根 “网线”。 不过与真实的交换机不同，brdige 网卡可以被赋值 IP，当 bridge 拥有 IP 后，它就与内核协议栈连接了，因此接收到的包可以到达内核协议栈的 IP 层处理，也就会经过 net_filter 处理。 推荐阅读 bridge 网卡推荐阅读：Linux虚拟网络设备之bridge(桥) ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:3","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.4 veth-pair 虚拟网络设备 veth-pair 设备 总是成对的出现，当数据包进入一端 veth 设备时，会从另一端 veth 设备出。veth-pair 两个设备可以处于不同的 net namespace，也就可以实现不同 net namespace 间数据传输。 默认下，veth 设备链接的两端是内核协议栈。不过 veth 设备链接上 bridge，这样另一端发送的数据都会由 bridge 处理。 推荐阅读 veth-pair 设备了解推荐文章：Linux虚拟网络设备之veth ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:4","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"2.5 macvlan 虚拟网络设备 macvlan 网络设备 可以有 mac 地址与 ip 地址，用于将 net namespace 连接到宿主机的物理网络中，相当于，容器直接连接着物理网络。 macvlan 网络设备有着多种的模式，包括：bridge、private 等，这影响着各个 macvlan 网络设备之间的通信。 更过 macvlan 网络设备推荐文章：Linux interfaces for virtual networking ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:2:5","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3 Bridge 网络 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.1 创建/删除 Bridge 网络 (1) 创建网络 先试着创建一个自定义的 bridge 网络，观察 docker 会对应创建哪些东西。 $ docker network create --driver=bridge \\ --subnet=192.168.100.0/24 \\ --ip-range=192.168.100.0/26 \\ --gateway=192.168.100.1 \\ --opt com.docker.network.bridge.name=mybr0 \\ mybridge0 2e61a7dc333c1bc61d9cb86503ce4cd5a7435977ea2f9b7cc97fc71ae0e2bb93 --driver=bridge 指定创建的网络 driver； --subnet=192.168.100.0/24 指定对应 bridge 网络的网段； --ip-range=192.168.100.0/26 指定运行分配给容器的 ip 范围，当然，这个是要在指定的网段内的； --gateway=192.168.100.1 指定该内网的网关 IP； --opt com.docker.network.bridge.name=mybr0 指定创建虚拟 bridge 网卡的命名； mybridge0 为创建的 docker network 的命名； 通过 ifconfig 可以看到，bridge 网络创建会对应创建一个 bridge 网络设备，作为整个内网的 ‘交换机’。其 IP 就是指定的 gateway IP。 $ ifconfig … mybr0: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 192.168.100.1 netmask 255.255.255.0 broadcast 192.168.100.255 ether 02:42:46:8a:cf:34 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 … $ brctl show bridge name bridge id STP enabled interfaces mybr0 8000.0242efdb0984 no 但是与虚拟机网络中的 bridge 网卡不同，该 bridge 不会连接任何的物理网卡，仅仅是作为内网的 ‘交换机’ 使用。但是，毕竟内网是虚拟的，没有实际与物理网络连接，如何访问外网呢？ 答案是，通过内核 iptables 进行 NAT，然后将包从实际的物理网卡上发送与接受。因此还有一部分的改变在于 iptables，主要会建立的是 nat 与 filter 表的规则。 先看 nat 表的相关规则（下面输出中省略了不相关规则）： $ iptables -t nat -L -nv Chain PREROUTING (policy ACCEPT 2 packets, 88 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL Chain INPUT (policy ACCEPT 2 packets, 88 bytes) target prot opt in out source destination Chain OUTPUT (policy ACCEPT 124 packets, 8797 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT 124 packets, 8797 bytes) target prot opt in out source destination MASQUERADE all -- * !mybr0 192.168.100.0/24 0.0.0.0/0 Chain DOCKER (2 references) target prot opt in out source destination RETURN all -- mybr0 * 0.0.0.0/0 0.0.0.0/0 PREROUTING 与 OUTPUT 链中规则，使得所有入和出的包都会经过 DOCKER 链； POSTROUTING 链中，将 mybridge0 网络（192.168.100.0/24）的内网 ip 通过 MASQUERADE 行为进行伪装（可以简单认为内网 ip 会变为当前网卡的 ip）； 当然，如果包发往的是 mybr0 网卡，说明是 mybridge0 网络内部通信，就不需要进行 MASQUERADE 伪装（!mybr0）； 当容器发包时，会通过 mybr0 网卡转发进入内核栈，因此在 filter 表中，相关的规则都是针对于 “in=mybr0”。看一下 filter 表的规则： $ iptables -t filter -L -nv Chain INPUT (policy ACCEPT 61774 packets, 79M bytes) Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- * mybr0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED DOCKER all -- * mybr0 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- mybr0 !mybr0 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- mybr0 mybr0 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 42290 packets, 55M bytes) target prot opt in out source destination Chain DOCKER (2 references) target prot opt in out source destination Chain DOCKER-ISOLATION-STAGE-1 (1 references) target prot opt in out source destination DOCKER-ISOLATION-STAGE-2 all -- mybr0 !mybr0 0.0.0.0/0 0.0.0.0/0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-ISOLATION-STAGE-2 (2 references) target prot opt in out source destination DROP all -- * mybr0 0.0.0.0/0 0.0.0.0/0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-USER (1 references) target prot opt in out source destination RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 FORWARD -\u003e DOCKER-ISOLATION-STAGE-1 -\u003e DOCKER-ISOLATION-STAGE-2 表明允许包从 mybr0 进入并转发（即容器可以向外正常发包）； FORWARD 中对 mybr0 进入的包设置了 conntrack，使得能够收到连接建立后的正常的回包； (2) 删除网络 通过 docker network remove 删除网络时，会发现对应的 bridge 网卡与 iptables 规则都被删除。 $ docker network remove 5a17670afb6f 5a17670afb6f ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:1","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.2 启动容器后的网络 下面看下容器启停后，带来的网络变化。先启动最简单的一个容器，指定使用网络为上面创建的 mybridge0。 $ docker run -dt --rm --network=mybridge0 --name br0_container ubuntu 676f7f9eab12a20fb3a975fa99cc2c92433a9581b5774ea58e63d447d86aa5ad $ docker inspect 676f7f9eab12 … \"Networks\": { \"mybridge0\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": [ \"882cac3e472f\" ], \"NetworkID\": \"af1dbf619ac62be1ad8a6b63696d3e6edff77cceab6cd0ee78de4b51e0d33683\", \"EndpointID\": \"56cd85c5121d7d14146fcacc75599f6c56034e758e81f405a51437276ac6ac9f\", \"Gateway\": \"192.168.100.1\", \"IPAddress\": \"192.168.100.2\", \"IPPrefixLen\": 24, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:c0:a8:64:02\", \"DriverOpts\": null } } … --network=mybridge0 表明以 mybridge0 网络启动容器； 观察容器具体参数，可以看到，容器被随机分配 mybridge0 设置的 ip-range 一个 ip，并且 gateway 就是 mybridge0 网络的网关地址； 观察网络设备，可以看到一个 veth-pair 设备 出现在宿主机上，并且连接到了 mybr0 网卡： $ ifconfig … vethef6b174: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet6 fe80::e8ad:86ff:fefe:14ca prefixlen 64 scopeid 0x20\u003clink\u003e ether ea:ad:86:fe:14:ca txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 23 bytes 1882 (1.8 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 … $ brctl show bridge name bridge id STP enabled interfaces mybr0 8000.0242efdb0984 no vethef6b174 veth-pair 都是成对出现的，可以简单被看做一个通道，一端发入的包会从另一端发出，并进入内核协议栈。不过，在 bridge 网络环境下，veth5b480f8 连接到 mybr0，所以所有从 veth5b480f8 发出的包都会被 mybr0 接手转发（相当于就是一根网线插入了交换机）。 可以进入容器 namespace，看一下容器内的 veth 设备。 $ docker exec -it br0_container bash # 以下在容器 namesapce 环境执行 root@676f7f9eab12:/# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.100.2 netmask 255.255.255.0 broadcast 192.168.100.255 ether 02:42:c0:a8:64:02 txqueuelen 0 (Ethernet) RX packets 1928 bytes 21609413 (21.6 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1817 bytes 102779 (102.7 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@676f7f9eab12:/sys/class/net/eth0# ethtool -i eth0 driver: veth version: 1.0 firmware-version: expansion-rom-version: bus-info: supports-statistics: yes supports-test: no supports-eeprom-access: no supports-register-dump: no supports-priv-flags: no root@676f7f9eab12:~# cat /sys/class/net/eth0/iflink 15 在容器内的仅仅有一个 eth0 网卡，ip 设置为了容器的 ip。但是其实这个网卡就是 veth 设备改了名字； 通过 ethtool -i eth0 命令看到，其对应 driver 是 veth，并且 /sys/class/net/eth0/iflink 文件表明了对端的 veth 网卡编号为 15（即宿主机看到的 veth 网卡设备）； 现在，我们试着启动容器并添加一个 tcp 端口映射。 $ docker run -dt --rm \\ --network=mybridge0 --publish 12211:8080 \\ --name br0_container ubuntu 2502f7397a37e2ab482f8a9152d1ed968dd2e2825c71eb2a6737e4900f7236c1 而这个端口映射，就是将宿主机的 12211 端口映射给容器的 8080，所以所有发往宿主机的 12211 端口的包，都会被修改端口并转发到容器内部。这也是通过 iptables 实现的： iptables -t nat -L -nv Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL Chain INPUT (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes) target prot opt in out source destination DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 10 617 MASQUERADE all -- * !mybr0 192.168.100.0/24 0.0.0.0/0 0 0 MASQUERADE tcp -- * * 192.168.100.2 192.168.100.2 tcp dpt:8080 Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- mybr0 * 0.0.0.0/0 0.0.0.0/0 0 0 DNAT tcp -- !mybr0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:12211 to:192.168.100.2:8080 PREROUTING -\u003e DOCKER 链中，所有不是从 mybr0 进入的包，并且发往 tcp 12211 端口的包，都会被 DNAT 为发往 192.168.100.2:8080。这样就实现了端口映射的功能。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:2","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"3.3 bridge 网络总结 中心思想：bridge 网络使用 bridge 网卡创建了一个本地的内网，而 bridge 网卡 + iptables 规则成为了这个内网的 ‘路由器'。其中: bridge 网卡作为二层的交换机，bridge 网卡 ip 作为路由器的网关 ip。 iptables 规则实现了 brdige 网卡与物理网络的连接 宿主机内核栈实现了这个 ‘路由器’ 的路由功能。 下图展示了整个 bridge 网络的模型（图片来自网络）： 其中比较关键的点： veth pair 设备将容器 net namespace 连接到 bridge 网卡（可以看做将 veth pair 作为网线插到了 bridge 这个 ‘路由器’ 上）。 iptables 实现了 bridge 网卡与物理网络的 ‘连接’。 bridge 网卡收到的包，经过 iptables 的 MASQUERADE 将包进行地址转换，并经过内核协议栈的路由通过物理网卡发送到物理网络。而回包通过 conntrack 机制正常接收与逆地址转换。 容器与宿主机的端口映射，也是通过 iptables 的 DNAT 实现的。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:3:3","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"4 Host 网络 Host 网络没啥好说的，启动容器不创建新的 namespace，依旧在宿主机的 net namespace 下。 $ docker run -dt --rm --network=host --name host_container ubuntu da1c426a7c7501b329258b12cb475ff42669837ca686d6e946511632461cc946 观察 mount，可以看到对应还是有 net namespace 的文件挂载，文件名为 default： $ mount … nsfs on /run/docker/netns/default type nsfs (rw) 文件 inode 对比当前宿主机 net namespace inode，是一致的： $ ls -lh /proc/self/ns/net lrwxrwxrwx 1 root root 0 Nov 7 14:47 /proc/self/ns/net -\u003e 'net:[4026531992]' $ ls -lhi /run/docker/netns/default 4026531992 -r--r--r-- 1 root root 0 Oct 30 16:50 /run/docker/netns/default ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:4:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5 macvlan 网络 macvlan 网络使用 macvlan 虚拟网络设备，将容器 net namespace 网络暴露在与当前宿主机同级的局域网内，相当于容器就是当前网络内的一台 “主机”。 macvlan 网络设备也包括多种模式：bridge mode、802.1q trunk bridge mode。下面示例都是基于普通的 brdige mode。 因为 macvlan 网络在虚拟机网络下不太好验证，所以下面例子来自于一台物理机上。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5.1 创建/删除 macvlan 网络 通过 docker network create 创建 macvlan 网络。 $ docker network create -d macvlan \\ --subnet=192.168.67.130/24 --gateway=192.168.67.1 \\ -o parent=eth0 mymacvlan0 633aae3d4f430352e5439e2650c02fe9c2092b99b5b8252f8141fa5d62ec7e70 -d macvlan，指定 macvlan 网络 -subnet=192.168.67.130/24，因为 macvlan 网络下的容器会直接连入物理网络，所以子网也是要在当前子网内； --gateway=192.168.67.1，同样，gateway 就是宿主机的网关地址； -o parent=eth0，指定 macvlan 设备链接的物理网卡，一定要是一个真正可联网的物理网卡； 不过与 bridge 网络不同的是，创建一个 macvlan 网络仅仅是记录其对应的配置，不会创建对应的 macvlan 网卡或者 iptables 规则。因为 macvlan 网卡是与 net namespace 绑定的，所以当创建 net namespace 时才会出现对应网络设备。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:1","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5.2 启停容器后的网络 $ docker run --net=mymacvlan0 \\ -dt --rm --name macvlan_container \\ --ip=192.168.67.139 --privileged \\ centos_ctr bash 2eff4835733734b6819c7f97ae41585985d95c1ea4c66a6a478a43e71b60b6d6 启动容器，如果不指定 IP，Docker 会在配置的网段里分配一个。 tip 为了能够方便排查网络问题，使用的容器镜像 centos_ctr 是由 centos 镜像以 host 网络启动，预装一些命令后，才由容器导出的镜像。 但是发现进入容器后，发现静态配置 IP 无法 ping 通网关（宿主机是正常无法 ping 通，因为内核会丢弃 macvlan 网卡的包）。研究后不清楚具体原因，但是这台宿主机接的是交换机，不知道是不是不是路由器导致的。 $ docker exec -it macvlan_container bash # 以下是容器中命令 [root@2eff48357337 /]# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.67.139 netmask 255.255.255.0 broadcast 192.168.67.255 ether 02:42:c0:a8:43:8b txqueuelen 0 (Ethernet) RX packets 433282 bytes 27463954 (26.1 MiB) RX errors 0 dropped 26809 overruns 0 frame 0 TX packets 91342 bytes 6649728 (6.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@2eff48357337 /]# ping 192.168.67.1 PING 192.168.67.1 (192.168.67.1) 56(84) bytes of data. From 192.168.67.139 icmp_seq=1 Destination Host Unreachable From 192.168.67.139 icmp_seq=2 Destination Host Unreachable 因此，换个思路，静态 IP 不行，就通过 DHCP 获取一个 IP 尝试是否能够连通网络。 在删除静态 IP 之后，调用 dhclient 从上层路由器获取一个 IP。 # 删除 eth0 网卡 IP [root@2eff48357337 /]# ip address del 192.168.67.139 dev eth0 Warning: Executing wildcard deletion to stay compatible with old scripts. Explicitly specify the prefix length (192.168.67.139/32) to avoid this warning. This special behaviour is likely to disappear in further releases, fix your scripts! [root@2eff48357337 /]# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether 02:42:c0:a8:43:8b txqueuelen 0 (Ethernet) RX packets 435540 bytes 27608133 (26.3 MiB) RX errors 0 dropped 26983 overruns 0 frame 0 TX packets 91763 bytes 6678670 (6.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 调用 dhclient 获取新的 IP [root@2eff48357337 /]# dhclient -r \u0026\u0026 dhclient -v Removed stale PID file Internet Systems Consortium DHCP Client 4.3.6 Copyright 2004-2017 Internet Systems Consortium. All rights reserved. For info, please visit https://www.isc.org/software/dhcp/ Listening on LPF/eth0/02:42:c0:a8:43:8b Sending on LPF/eth0/02:42:c0:a8:43:8b Sending on Socket/fallback DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 3 (xid=0xdf4e0e25) DHCPREQUEST on eth0 to 255.255.255.255 port 67 (xid=0xdf4e0e25) DHCPOFFER from 192.168.9.253 DHCPACK from 192.168.9.253 (xid=0xdf4e0e25) System has not been booted with systemd as init system (PID 1). Can't operate. Failed to create bus connection: Host is down bound to 192.168.9.235 -- renewal in 38783 seconds. [root@2eff48357337 /]# ifconfig eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.9.235 netmask 255.255.255.0 broadcast 192.168.9.255 ether 02:42:c0:a8:43:8b txqueuelen 0 (Ethernet) RX packets 435635 bytes 27614880 (26.3 MiB) RX errors 0 dropped 27001 overruns 0 frame 0 TX packets 91767 bytes 6679438 (6.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@2eff48357337 /]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.9.253 0.0.0.0 UG 0 0 0 eth0 192.168.9.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 可以看到，DHCP 获得的 IP 与宿主机都不是同一个网段的，并且网关地址也不是同一个，因此上层连着交换机有多个网段（这块不太理解了）。 但是，测试后是可以 ping 通网关，并且可以访问外网的： [root@2eff48357337 /]# ping 192.168.9.253 PING 192.168.9.253 (192.168.9.253) 56(84) bytes of data. 64 bytes from 192.168.9.253: icmp_seq=1 ttl=64 time=0.637 ms 64 bytes from 192.168.9.253: icmp_seq=2 ttl=64 time=0.250 ms ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:2","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"5.3 总结 中心思想：将 macvlan 网络启动容器看做一个与宿主机同级的网络，其获取 IP 方式都与正常的机器相同。 ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:5:3","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["Docker 原理总结"],"content":"参考 Docker 容器网络官方文档 Linux虚拟网络设备之bridge(桥) Linux interfaces for virtual networking ","date":"2020-11-06","objectID":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/:6:0","tags":["docker","container"],"title":"容器网络总结","uri":"/posts/cloud_computing/how_docker_work/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"categories":["VM"],"content":"制作虚拟机镜像","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"中心思想：通过 libvirt 运行一个虚拟机（domain），并保存其对应的 domain 的镜像文件与配置文件，然后就可以在其他机器通过 virsh define + start 或者 virt-install 启动。 说明：下面环境都是在 centos 上制作基于 KVM 的虚拟机镜像。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:0:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"1 从 ISO 镜像安装 最基本的安装方式，通过安装并运行一个新的虚拟机，然后得到对应的配置文件与镜像文件。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:1:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"1.1 手动安装 最基本的安装方式，通过 ISO 文件进行安装。 下载 ISO 镜像文件，镜像文件在各个镜像站中就可以找到。（因为不使用图形界面，所以下载的是非图形化的 ISO） 通过 virt-install 命令安装镜像（因为使用的是非图形化的安装，所以参数有一些不一样） virt-install --name guest1_fromiso --memory 2048 --vcpus 2 \\ --disk size=8 --location CentOS-7-x86_64-DVD-2003.iso \\ --os-type Linux --os-variant=centos7.0 --virt-type kvm \\ --boot menu=on --graphics none --console pty --extra-args 'console=ttyS0' 其中要注意几个参数： 因为我们是安装非图形化，所以需要 --location 参数指定 iso，并指定 --boot menu=on 打开安装菜单，最后还需要指定安装信息的输出 --console pty --extra-args 'console=ttyS0' 这样安装菜单才能正常展示出来 --graphics none 指定非图形化； --network bridge=virbr0，指定网络模式，这里指定 libvirt 默认创建的 bridge 网卡，可以认为这是一个 libvirt 维护内网，安装时选择 dhcp 就可以获得一个可用的内网地址； 具体 libvirt 的网络模型，后面在单独研究下。 -- disk size=8，disk 参数用于指定系统盘，这里指定自动创建一个 8G 的 qcow2 文件，作为系统盘（默认镜像文件保存在 /var/lib/libvirt/images/）目录下； 这时就会进入虚拟机的安装步骤，具体安装步骤就不赘述了。 安装成功后，可以看到 domain 就被创建了，这就可以得到它的配置文件与镜像文件了。 $ virsh list Id Name State ---------------------------------------------------- 18 guest1_fromiso running ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:1:1","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"1.2 自动安装 可以看到，手动安装需要人为在菜单中选择、配置，这不适用于多个虚拟机的安装。而 RedHat 创建了 kickstart 安装方法，使得整个虚拟机安装流程变得自动化。 这块不了解，具体见红帽官方文档：KICKSTART INSTALLATIONS ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:1:2","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"2 使用 Cloud Image 当然，上面制作过程中耗时都在安装系统上了，而各个云厂商的虚拟机数量那么多，肯定不会一台台去安装操作系统了。所以，目前最常见的都是直接下载已经安装过系统的虚拟机镜像文件。 但是这样的虚拟机是没有特殊配置的，例如密码、hostname 都是一致的，所以 cloud-init 出现，用于在第一次启动虚拟机时进行系统的配置。 所以，最快速的制作方法就是：虚拟机镜像文件 + cloud-init 配置。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:2:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"2.1 cloud-init 下面内容都来自于文档，这里仅为自己做个记录。 首先明确 cloud init 的功能：系统第一次启动时，cloud init 相关的进程会根据配置信息去进行系统的配置，包括：设置 hostname、ssh key、password 等； (1) 基本概念 metadata：包含服务器信息，用于 cloud-init 配置； userdata：包含 cloud-init 系统配置信息，可以是 文件，脚本，yaml 文件等； datasource：cloud-init 读取配置数据的来源，包含大部分云厂商，当然，也可以来自本地的文件(NoCloud datasource)； (2) 运行过程 cloud init 设置包括五个阶段： Generator 机器启动阶段，systemd 的一个 generator 将会决定是否将 cloud-init.target（target 可以简单认为特定事件下触发的一组 unit）包含在启动过程中。这就表示启动 cloud-init。 默认情况下，generator 会启动 cloud-init，除非以下情况： /etc/cloud/cloud-init.disabled 文件存在； 内核启动命令行 /proc/cmdline 中包含 “cloud-init=disabled”。如果是容器中运行，会读取环境变量 KERNEL_CMDLINE； 而下面的步骤，就是由 target 包含的各个 unit 执行的。 Local 由 cloud-init-local.service 执行，主要目的：找到 “local” 的 datasource，根据配置网络。 配置网络有三种情况： 首先，根据传入配置 “metadata” 配置网络； 当上面情况失败，直接配置 “dhcp on eth0”； 如果 /etc/cloud/cloud.cfg 配置文件中禁用了网络：network: {config: disabled}，那么就不进行网络配置； Init、Config、Final 阶段 对应 service 为 cloud-init.service、cloud-config.service、 cloud-final.service。 通过 local 阶段，网络已经配置好了，并且已经得到了 metadata。而 /etc/cloud/cloud.cfg 配置定义了剩下三个阶段对应的任务，也就是 module。 cloud init 通过一些缓存信息来判断机器是否经过初始化，通过 cloud-init clean 也可以手动清理缓存信息。 /var/log/cloud-init.log 记录了 cloud-init 运行的完整过程。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:2:1","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"2.2 制作镜像 下面就开始进行镜像制作： 下载 cloud image，这里使用中科大提供的： $ wget https://mirrors.ustc.edu.cn/centos-cloud/centos/7/images/CentOS-7-x86_64-GenericCloud-2003.qcow2 修改密码 当然，该镜像其实就可以直接进行 virt-install 来启动（因为我们没有配置文件，所以通过 virt-install 来启动并生成配置文件），但是不知道密码，也搜不到，无法登陆进入。不过你也可以使用下面命令来设置密码后进行登陆： $ virt-customize -a CentOS-7-x86_64-GenericCloud-2003.qcow2 --root-password password:yourpassword 2. 因为 cloud-init 需要一个 datasource，而我们没有使用云厂商，所以使用 NoCloud 形式，按照官方的s示例创建一个 disk 文件。 # 创建 user-data 与 meta-date 配置文件 $ cat meta-data instance-id: guest1 local-hostname: guest1 $ cat user-data #cloud-config chpasswd: expire: false list: | root: password1 ssh_pwauth: True # 生成 disk 文件，包含 userdata 与 metadata 配置数据 $ genisoimage -output seed.iso -volid cidata -joliet -rock user-data meta-data 创建并运行虚拟机。 $ virt-install --memory 2048 --vcpus 2 --name guest2 \\ --disk CentOS-7-x86_64-GenericCloud-2003.qcow2 --disk seed.iso \\ --os-type Linux --os-variant centos7.0 --virt-type kvm \\ --graphics none --network default \\ --import 几个比较重要的参数： --disk CentOS-7-x86_64-GenericCloud-2003.qcow2：制定系统盘； --import：跳过安装过程，因为已经安装好操作系统，不需要进行安装过程； --disk seed.iso：传递 cloud-init datasource 信息； 虚拟机启动过程中，可以看到 cloud-init 配置信息的一些打印： 最后根据配置的密码成功进入： 而 CentOS-7-x86_64-GenericCloud-2003.qcow2 就是虚拟机经过配置的镜像文件，而 libvirt 启动所需的配置文件就是 /etc/libvirt/qemu/guest1.xml。 ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:2:2","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["VM"],"content":"参考 CREATING GUESTS WITH VIRT-INSTALL KICKSTART INSTALLATIONS cloud-init Documentation ","date":"2020-10-31","objectID":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/:3:0","tags":["虚拟机","KVM","云计算"],"title":"制作虚拟机镜像","uri":"/posts/cloud_computing/vm/%E5%88%B6%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F/"},{"categories":["k8s 实践"],"content":"使用 PV 与 PVC 为 Pod 提供存储","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/pv-pvc-%E4%B8%8E-storageclass/","tags":["k8s","云计算"],"title":"PV PVC 与 StorageClass","uri":"/posts/cloud_computing/k8s_practice/pv-pvc-%E4%B8%8E-storageclass/"},{"categories":["k8s 实践"],"content":"1 PV 与 PVC 目的：使用 NFS 做 PV，创建 Pod 使用该 PV Node-1 构建 nfs 服务，位于 “/nfs” 目录。 创建 PV，使用 nfs 类型。 声明、创建 PVC，使用上述指定的 StorageClass，形成指定的绑定关系。 pvc 状态为 Bound，表明已经成功绑定到了 pvc。查询 pv，可以看到 pv 也是被绑定了。(注意：一个 PV 只能绑定一个 PV) 创建 Pod Deployment，在 Pod 的 Volume 使用 PVC。 其中两个 Pod 分别调度到了 Node-2 Node-3，在 Node-2 Node-3 中可以看到对应的 nfs mount： 对应容器配置也可以看到指定的挂载： 在 Node-3 节点的容器环境内写入挂载路径文件，可以看到同步到了主节点的 /nfs 目录上，因此，存储配置成功。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/pv-pvc-%E4%B8%8E-storageclass/:1:0","tags":["k8s","云计算"],"title":"PV PVC 与 StorageClass","uri":"/posts/cloud_computing/k8s_practice/pv-pvc-%E4%B8%8E-storageclass/"},{"categories":["k8s 实践"],"content":"使用 RBAC 进行访问控制与授权","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"RBAC 实践","uri":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"Kubernetes 中，通过 RBAC 机制来实现集群 Pod 中对 APIServer 的访问权限控制与授权。 RBAC 机制有三个最基本的概念： Role：一组规则，定义了对 API 对象的操作权限； Subject：被作用者，集群内部常常使用的是 ServiceAccount； RoleBinding：绑定 Role 与 Subject； ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"RBAC 实践","uri":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 Role 与 ServiceAccount 实践 目的：通过 Role 与 RoleBinding 限制一类 ServiceAccount，并在 Pod 中使用该 ServiceAccount 观察权限控制。 创建需要使用的自定义 namespace [mynamespace] 创建需要限制访问权限的 ServiceAccount [example-sa] 可以看到，每个 namespace 有个默认的 ServiceAccount default，提供完整的 APIServer 访问权限。 每个 ServiceAccount 在容器维度看到就是 Secret 对象，包含证书内容。 创建 Role，定义允许的权限规则。 可以看到，rules 指定了该 Role 为：允许对 mynamespace 下的 pod 进行 get、watch、list 操作。 创建 RoleBinding，关联刚刚创建的 Role 与 ServiceAccount。 创建 Pod，指定使用的 ServiceAccount，在 Pod 内观察权限是否被限制了 进入容器中，可以看到，k8s 将 ServiceAccount 对应的 Serects 对象挂载到了 /run/secrets/kubernetes.io/serviceaccount 目录下，包含 client 需要使用的【证书 ca.crt】、【token】: ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"RBAC 实践","uri":"/posts/cloud_computing/k8s_practice/rbac-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 CRD 自定义资源，通过 Kubernetes 编排","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"CRD 是 Kubernetes 可扩展性的第一个体现，因为 Kubernetes 提供的是一个编排的框架，因此不止可以对 Pod 进行编排，也支持通过 CRD 对你自定义的类型的编排。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 CRD 构建 目的：通过 CRD 构建一个自定义资源。 编写 CRD manifest，完成自定义资源的定义。 可以看到，CRD 中只有类型的简单定义，没有该 CR 的元属性的定义，因为这些需要通过代码中定义。 通过 kubectl apply 创建 CR 对应 CRD 对象，让 Kubernetes “认识” 这个自定义资源。 调用 kubectl apply 创建自定义资源： 这里其实 Kubernetes 不知道该资源具体的代码类型，它只是知道有 CRNetwork 这个资源，并支持创建删除，将其保存下来了。 编写 CR 相关定义代码。其实这里编写代码是为生成 kubectl 的 Client，使其在编写 Controller 时候能够正确的解析你的 CR 对象。 整个的目录结构如下： 具体代码见仓库：k8s_practice 通过 k8s 生成代码，最终生成的目录结构如下： ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2 编写自定义控制器 因为 Kubernetes 中是基于声明式 API 的业务实现，所以需要控制器来“监控”对象变化，执行对应的操作。 编写自动以控制器主要有三个过程：编写 main 函数、编写自定义控制器定义，编写控制器业务逻辑。 整个实践代码见：k8s_practice Controller 代码编写。主要逻辑：处理 Informer 通知的 Event，执行 CR Sync 操作。 Controller 主要包含三个部分： Informer：包含从 APIServer 同步的 CR 对象的 Cache，并且处理 CR Event，调用 Event Handler。 Event Handler：Informer 的各个 Event 调用的事件回调，一般都是放入 workQueue，延后处理。 Workers：根据各个 Event 进行真正的业务处理，例如真实资源的创建、删除、更新等。 main 函数代码编写。主要逻辑：创建 Informer、Controller，执行 Controller 的启动。 编译后运行 Controller。 第一同步后，所有的 CR 对象都可以被任务是“新添加的”，因此会一个个调用 HandleAdd 接口。上图可以看到，因为集群中已经有了一个 CR 对象，因此 Controller 会进行该对象的 Sync。 创建一个新的 CR 对象 example-crnetwork-2，观察 Controller。 可以看到 Controller 处理完成。 删除刚创建的 example-crnetwork-2，观察 Controller。 可以看到 Controller 正确的执行删除的逻辑。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/:2:0","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"3 将自定义资源控制封装为 Pod TODO ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/:3:0","tags":["k8s","云计算"],"title":"CRD 实践","uri":"/posts/cloud_computing/k8s_practice/crd-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 Job 或者 CronJob 部署一次性任务","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"Job CronJob 实践","uri":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"Job 用于运行一次性的任务，即“离线任务”。CronJob 在 Job 之上提供了周期性任务支持。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"Job CronJob 实践","uri":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 Job 目的：部署 Job 任务，使用并行运行（Batch）的功能。 创建部署 Job 任务。 可以看到，并行的两个 Pod 正在运行。 经过一段时间可以看到，因为设置 deadline 为 100，所以 pod 异常被退出。而 restartPolicy: Never 使得不会再次运行。 去除 deadline 设置，重新部署，可以看到，本次 4 次成功完成。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"Job CronJob 实践","uri":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2 CronJob 目的：体验 CronJob 的周期性任务功能，明确 CronJob 是基于 Job 管理实现的。 创建部署 CronJob 。可以看到，CronJob 中需要指定 JobTemplate，因此 CronJob 完全是基于 Job 管理的。 部署后可以看到，CronJob 创建了一个 Job： 经过一分钟，新 Job 被创建，并运行成功： ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/:2:0","tags":["k8s","云计算"],"title":"Job CronJob 实践","uri":"/posts/cloud_computing/k8s_practice/job-cronjob-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 DaemonSet 部署常驻容器","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/daemonset-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"DaemonSet 实践","uri":"/posts/cloud_computing/k8s_practice/daemonset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"DaemonSet 会为匹配的 Node 运行一个 Daemon Pod，与 Deployment 类最大不同，DaemonSet 没有副本的概念。 目的：部署 DaemonSet，试用 toleration 与 nodeAffinity，观察滚动升级流程。 部署 DaemonSet。其中，是用 nodeAffinity 指定选择调度的节点，使用 toleration 容器 Node 的 taint: 创建后，可以看到，只选择调度到了 node-1 node-2 节点，和配置的 Affinity 匹配： 改变 DaemonSet 使用的镜像版本，观察滚动升级流程: 通过 kubectl rollout status 可以看到，滚动升级流程与 Deployment 过程一致： 观察 Event，可以看到也是按照 delete -\u003e create 的流程进行升级的。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/daemonset-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"DaemonSet 实践","uri":"/posts/cloud_computing/k8s_practice/daemonset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 StatefulSet 进行副本控制","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"Deployment 对于“无状态”的任务，已经能够做到副本控制，滚动升级功能了。但是 Deployment 无法适用于“有状态”的任务，因为其中 Pod 都是相同的，没有任何的对应关系。 而 StatefulSet 通过“固定命名、域名的 Pod，以及固定的创建顺序”作为基础，加上与命名对应的网络、存储，搭建一个“有状态”Pod 管理。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:0:0","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 HeadlessService StatefulSet 会使用 HeadlessService 的概念，首先来部署 HeadlessService。 创建描述文件 headless_service 可以看到，Headless 与普通 Service 最大区别是 clusterIP 为 None。 通过 kubectl create 部署 HeadlessService，成功后可以看到： 查看 Endpoint，可对应的 Endpoint 包含了 Node-1 Node-2 的 Pod 的地址了。并且 Endpoint 命名和Service 名字一样。 你按照这样的方式创建了一个 Headless Service 之后，它所代理的所有 Pod 的 IP 地址，都会被绑定一个这样格式的 DNS 记录： \u003cpod-name\u003e.\u003csvc-name\u003e.\u003cnamespace\u003e.svc.cluster.local 但是好像单独使用 Headless Service 是无法访问这些域名的。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2 StatfulSet ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:2:0","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.1 StaefulSet 的固定域名 下面开始部署 StatefulSet： 先要创建对应的 Headless Service，如前面一样。 创建对应的 StatefulSet 描述文件： 可以看到，StatefulSet 与 Deployment 最大的区别就是指定了 serviceName 字段，指定了使用的 HeadlessService。 创建 StatefulSet 对象，类似于 Deployment，开始创建 Pod 了 但是 StatefulSet 并没有创建任何的 ReplicaSet，所以实现上与 Deployment 不一样： 观察创建出的 Pod，可以看到，其 Pod 命名不是加上随机字符串了，而是有序的数字： 并且，其创建顺序也是有序的，先创建 web-0 ，web-0 运行后并 Ready 后，创建 web-1。 运行一个 busybox 测试容器，执行 nslookup 访问 HeadlessService 为其绑定的域名，可以看到正常返回了。 删除 web-0 Pod，可以看到 StatefulSet 会重新立刻创建同名的 Pod。所以，Pod 名字是固定的 再次通过 web-0.nginx.default.svc.cluster.local 进行域名解析，还是能够正确的解析： 注意：虽然域名可以正确解析，但是其域名对应的 IP 不是保证固定的，所以不能保存 Pod 的 IP ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:2:1","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.2 StatefulSet 的固定存储 目的：使用 StatefulSet 中的 Template PVC 自动构建持久化存储，并观察其 PVC 是否与 PodName 绑定。 构建 nfs 的 2 个 PV，给 2 个 Pod 准备。（构建过程见\u003cPV、PVC 与 StorageClass\u003e一节） 创建 StatefulSet，其指定 PVC 模板，使得能够为每个 Pod 自动创建其对应的 PVC。 创建后可以看到，StatefulSet 为 2 个 Pod 创建了对应的 PVC： 可以看到，PVC 是和名字对应的，格式为 [volume name]-[pod name]，因此，PVC 与 Pod 名字的映射关系是固定的。 所以这就实现了，当旧 Pod 删除，新 Pod 被创建后，因为其 Pod Name 没有变，所以就找到了旧 Pod 使用 PVC。 ","date":"2020-10-30","objectID":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/:2:2","tags":["k8s","云计算"],"title":"StatefulSet 实践","uri":"/posts/cloud_computing/k8s_practice/statefulset-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"使用 Deployment 进行部署","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1 ReplicaSet Deploment 管理的是 ReplicaSet，所以先运行 ReplicaSet 观察。 ReplicaSet 只包含副本控制功能，没有滚动升级等高级的功能。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:1:0","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1.1 部署 ReplicaSet 创建 manifest 文件。 调用 kubectl create 创建资源。 观察下 ReplicaSet 的事件，可以看到各个 Pod 的创建流程。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:1:1","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1.2 副本保证 目前，3 个 Pod 都运行在了 Node-3 上： 下线 Node-3 ，可以看到 node-3 变为 NotReady: 过了一段时间后，原来三个 Pod 变为 Terminating 状态，而新的 Pod 被创建被调度。 可以看到，新创建 3 个 Pod 被调度到了 Node-2 上: 恢复 Node-3 上线，kubelet 会同步任务，因此不会再次运行旧的三个 Pod。Pod 的状态也从 Terminating 变为被删除。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:1:2","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"1.3 水平缩扩 通过 kubectl scale 进行副本扩展: 可以看到，新的 Pod 在 Node-3 被运行： 依旧 kubectl scale 进行副本缩容，可以看到，两个 Pod 被停止: 可以看到，ReplicaSet 启动和停止任务都是由 Scheduler 选择的，而不能认为的控制选择指定的 Pod，也就是说，所有的 Pod 应该被认为是“无状态的”，随时可能被停止。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:1:3","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2 Deployment Deployment 操作与管理的是 ReplicaSet，而不是 Pod。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:0","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.1 部署 构建 manfiest 文件: 通过 kubectl create 创建 Deployment。 可以看到，Event 中打印的是对应的ReplicaSet 的自动被创建，所以Deployment 是创建 ReplicaSet 的。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:1","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.2 水平扩展 Deployment 水平扩展方式与 ReplicaSet 一致，并且就是操作 ReplicaSet 的 replica 的值来实现，跳过。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:2","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.3 滚动升级 (1) 正常流程 Deployment 在 ReplicaSet 基础上添加了“滚动升级”的功能。 依旧创建 Deployment，并直接扩展到副本数为 3。 修改 Deployment 的配置文件，将 image 版本升级。 可以看到，Deployment 新建了一个 ReplicaSet （部署新版本 Pod），而旧的 ReplicaSet 副本变为了 0。 通过 Deployment 的 Event 可以看到，旧版 ReplicaSet 的副本数逐渐减少，而新版本 ReplicaSet 副本数逐渐增加。这样使得集群中 Pod 会维持在一个最低数量（示例中为 3） (2) 错误流程 观察下当升级出现错误时，Deployment 会处于怎样的状态。 通过 kubectl set image 将 Deployment 使用镜像变为一个不存在的镜像。 通过 Event 看到，滚动升级停止在了最新版本的 Replicaset 的第一个副本部署。 因为新旧版本是交替部署的，所以当第一个副本部署失败时，也就不会继续进行旧版本 Pod 的停止了。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:3","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"2.4 回滚 在错误流程看到，当发布错误的版本后，Deployment 会停止新版本的发布，而这时，就需要通过 kubectl rollout 进行 Deployment 的回滚。 执行 kubectl rollout history，查看每次 Deployment 变更对应的版本。（因为 -record，所有的 kubectl 命令都会被记录）。 可以看到，两次的版本变更都被记录了下来。 通过 –revision 参数，查看对应的命令细节。 通过 kubectl rollout undo 进行版本的回退，默认为上一次版本，通过 –to-revision 可以执行回退的版本。 事实上，回退也是一次“升级”，通过 history 可以看到一个新的部署记录： 这个 4，就是最新的一次回滚执行的命令了。 ","date":"2020-10-16","objectID":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/:2:4","tags":["k8s","云计算"],"title":"Deployment 实践","uri":"/posts/cloud_computing/k8s_practice/deployment-%E5%AE%9E%E8%B7%B5/"},{"categories":["k8s 实践"],"content":"单机使用虚拟机搭建 k8s 集群","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"1 虚拟机集群搭建 目标：创建 3 个虚拟机，用作一个 Master Node，两个 Work Node；当然，三个节点处于同一个网段。 具体步骤如下: 构建节点 构建三个虚拟机，基于 centos 7、内存 2 GB，并通过虚拟机复制功能（其实就是 copy 系统盘），完全复制出 Node 1，Node 2，Node 3。 搭建网络 三个节点需要互相访问，所以将其位于 VirtualBox 创建的 Nat网络下，给予每个 Node 静态的 IP（10.0.2.10 - 10.0.2.12），为了方便访问，并设置 ssh 的 DNAT。 设置每个虚拟机网卡加入其创建的 “NodeNatNetwork”。例如： 启动每个虚拟机，设置其 hostname，与网卡静态 IP。例如： 至此，三个虚拟机位于同一个网段，并且能够相互访问；对外，则通过 VirtualBox 的 Nat 网络能够访问。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:1:0","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2 部署 K8s 目标：通过 kubeadm 部署整个 k8s，用 Node-1 为 Master 节点，其他为工作节点。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:0","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2.1 安装 kubeadm、kubelet、kubectl 安装 kubeadm、kubelet、kubectl。这个官方文档写的很详细，见 Installing kubeadm 。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:1","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2.2 kubeadm init 初始化 Master 节点 Node-1 节点执行 kubeadm init，将其作为 Master 节点初始化。执行成功后，kubeadm 生成了 Kubernetes 组件的各个配置，以及提供服务的各类证书，位于 /etc/kubernetes 目录下: 并且已经以 static pod 的形式启动了：apiserver、controller-manager、etcd、scheduler。 还有最重要的，kubeadm 为集群生成一个【bootstrap token】，需要加入集群的节点都需要通过这个 token 加入。 * 问题 kubeadm 检查 swap 打开着，kubeadm 推荐不使用 swap，通过 swapoff -a 关闭交换区。 kubectl 默认通过 8080 端口访问，无法执行。 设置 kubectl 的配置文件为 kubeadm 生成的 /etc/kubernetes/admin.conf。（其实就是配置公钥，或者将 admin.conf 移动到 ~/.kube/config 文件，作为默认配置。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:2","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2.3 kubeadm join [token] 设置工作节点 通过 kubead init 最后返回的提示信息，执行对应 kubeadm join 将 Node-1 Node-2 加入到集群中，作为工作节点。 [root@Node-2 kubeadm join 10.0.2.10:6443 --token mahrou.d3uodof21i3d6yxk --discovery-token-ca-cert-hash sha256:21dfe4ef6b3bbd89f803bf44ff6eda587874336d103d0e4a3b --v 5 可以看到，kubelet 启动后就通过 pod 方式启动了本节点上 kube-proxy 容器： * 问题 无法访问到 Node-1 节点，nc ip 失败，但是可以 ping 通。通过在 Node-1 tcpdump 可以抓取到来自 Node-3 的包，因此应该是防火墙的问题，通过 iptables 对 Node-2 Node-3 IP 开放。 kubectl 无法访问问题，与上述问题一致。 ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:3","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"2.4 结果 目前为止，就完成了集群的搭建，但是 通过 kubectl get nodes，可以看到所有节点都是 NotReady： kubectl describe node node-1 可以看到，原因是因为没有设置正确的 Network Plugin： ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:2:4","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"3 部署网络插件 目的：部署网络插件，使各个节点为 Ready 状态，并其内部 Pod 能够相互通信。 以 Weave 部署为例，部署网络插件： kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" 其描述文件中定义所有 Weave 需要的 BAAC 权限组件，以及最重要的网络插件 Pod 对应的 DaemonSet: 应用成功后，可以看到对应的 DaemonSet 就运行起来，并开始给三个 Node 部署 Pod: 在节点上，可以看到 weave-net 对应的 pod ，包括两个容器： ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:3:0","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":["k8s 实践"],"content":"4 部署容器存储插件 目的：为了能够让容器使用网络存储，使得容器数据持久化，需要部署存储插件。 以 Rook 项目为例，部署存储插件： $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/exampleskubernetes/ceph/common.yaml $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/exampleskubernetes/ceph/operator.yaml $ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml 安装成功后，可以看到，rook 有着自己的 namespace，并且已经部署了 DaemonSet： 可以看到，Pod 也部署成功了： ","date":"2020-10-15","objectID":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/:4:0","tags":["k8s","云计算"],"title":"虚拟机 k8s 集群搭建","uri":"/posts/cloud_computing/k8s_practice/%E8%99%9A%E6%8B%9F%E6%9C%BA-k8s-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"categories":null,"content":"博客使用 Hugo 框架进行搭建, 主题使用 LoveIt. Logo 由 gopherize.me 生成, 并使用 realfavicongenerator.net 转化. ","date":"2020-10-15","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"}]